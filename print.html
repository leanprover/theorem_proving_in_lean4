<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Theorem Proving in Lean 4</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme && theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar && sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="title_page.html">Theorem Proving in Lean 4</a></li><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="dependent_type_theory.html"><strong aria-hidden="true">2.</strong> Dependent Type Theory</a></li><li class="chapter-item expanded "><a href="propositions_and_proofs.html"><strong aria-hidden="true">3.</strong> Propositions and Proofs</a></li><li class="chapter-item expanded "><a href="quantifiers_and_equality.html"><strong aria-hidden="true">4.</strong> Quantifiers and Equality</a></li><li class="chapter-item expanded "><a href="tactics.html"><strong aria-hidden="true">5.</strong> Tactics</a></li><li class="chapter-item expanded "><a href="interacting_with_lean.html"><strong aria-hidden="true">6.</strong> Interacting with Lean</a></li><li class="chapter-item expanded "><a href="inductive_types.html"><strong aria-hidden="true">7.</strong> Inductive Types</a></li><li class="chapter-item expanded "><a href="induction_and_recursion.html"><strong aria-hidden="true">8.</strong> Induction and Recursion</a></li><li class="chapter-item expanded "><a href="structures_and_records.html"><strong aria-hidden="true">9.</strong> Structures and Records</a></li><li class="chapter-item expanded "><a href="type_classes.html"><strong aria-hidden="true">10.</strong> Type Classes</a></li><li class="chapter-item expanded "><a href="conv.html"><strong aria-hidden="true">11.</strong> The Conversion Tactic Mode</a></li><li class="chapter-item expanded "><a href="axioms_and_computation.html"><strong aria-hidden="true">12.</strong> Axioms and Computation</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Theorem Proving in Lean 4</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        <a href="https://github.com/leanprover/theorem_proving_in_lean4" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#theorem-proving-in-lean-4" id="theorem-proving-in-lean-4">Theorem Proving in Lean 4</a></h1>
<p><em>by Jeremy Avigad, Leonardo de Moura, Soonho Kong and Sebastian Ullrich, with contributions from the Lean Community</em></p>
<p>This version of the text assumes you’re using Lean 4. See the
<a href="https://lean-lang.org/lean4/doc/quickstart.html">Quickstart section</a> of
the <a href="https://lean-lang.org/lean4/doc/">Lean 4 Manual</a> to install Lean. The first version of this book was
written for Lean 2, and the Lean 3 version is available
<a href="https://leanprover.github.io/theorem_proving_in_lean/">here</a>.</p>
<h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<h2><a class="header" href="#computers-and-theorem-proving" id="computers-and-theorem-proving">Computers and Theorem Proving</a></h2>
<p><em>Formal verification</em> involves the use of logical and computational methods to establish claims that are expressed in
precise mathematical terms. These can include ordinary mathematical theorems, as well as claims that pieces of hardware
or software, network protocols, and mechanical and hybrid systems meet their specifications. In practice, there is not a
sharp distinction between verifying a piece of mathematics and verifying the correctness of a system: formal
verification requires describing hardware and software systems in mathematical terms, at which point establishing claims
as to their correctness becomes a form of theorem proving. Conversely, the proof of a mathematical theorem may require a
lengthy computation, in which case verifying the truth of the theorem requires verifying that the computation does what
it is supposed to do.</p>
<p>The gold standard for supporting a mathematical claim is to provide a proof, and twentieth-century developments in logic
show most if not all conventional proof methods can be reduced to a small set of axioms and rules in any of a number of
foundational systems. With this reduction, there are two ways that a computer can help establish a claim: it can help
find a proof in the first place, and it can help verify that a purported proof is correct.</p>
<p><em>Automated theorem proving</em> focuses on the &quot;finding&quot; aspect. Resolution theorem provers, tableau theorem provers, fast
satisfiability solvers, and so on provide means of establishing the validity of formulas in propositional and
first-order logic. Other systems provide search procedures and decision procedures for specific languages and domains,
such as linear or nonlinear expressions over the integers or the real numbers. Architectures like SMT (&quot;satisfiability
modulo theories&quot;) combine domain-general search methods with domain-specific procedures. Computer algebra systems and
specialized mathematical software packages provide means of carrying out mathematical computations, establishing
mathematical bounds, or finding mathematical objects. A calculation can be viewed as a proof as well, and these systems,
too, help establish mathematical claims.</p>
<p>Automated reasoning systems strive for power and efficiency, often at the expense of guaranteed soundness. Such systems
can have bugs, and it can be difficult to ensure that the results they deliver are correct. In contrast, <em>interactive
theorem proving</em> focuses on the &quot;verification&quot; aspect of theorem proving, requiring that every claim is supported by a
proof in a suitable axiomatic foundation. This sets a very high standard: every rule of inference and every step of a
calculation has to be justified by appealing to prior definitions and theorems, all the way down to basic axioms and
rules. In fact, most such systems provide fully elaborated &quot;proof objects&quot; that can be communicated to other systems and
checked independently. Constructing such proofs typically requires much more input and interaction from users, but it
allows you to obtain deeper and more complex proofs.</p>
<p>The <em>Lean Theorem Prover</em> aims to bridge the gap between interactive and automated theorem proving, by situating
automated tools and methods in a framework that supports user interaction and the construction of fully specified
axiomatic proofs. The goal is to support both mathematical reasoning and reasoning about complex systems, and to verify
claims in both domains.</p>
<p>Lean's underlying logic has a computational interpretation, and Lean can be viewed equally well as a programming
language. More to the point, it can be viewed as a system for writing programs with a precise semantics, as well as
reasoning about the functions that the programs compute. Lean also has mechanisms to serve as its own <em>metaprogramming
language</em>, which means that you can implement automation and extend the functionality of Lean using Lean itself. These
aspects of Lean are described in the free online book, <a href="https://lean-lang.org/functional_programming_in_lean/">Functional Programming in Lean</a>, though computational
aspects of the system will make an appearance here.</p>
<h2><a class="header" href="#about-lean" id="about-lean">About Lean</a></h2>
<p>The <em>Lean</em> project was launched by Leonardo de Moura at Microsoft Research Redmond in 2013. It is an ongoing, long-term
effort, and much of the potential for automation will be realized only gradually over time. Lean is released under the
<a href="LICENSE">Apache 2.0 license</a>, a permissive open source license that permits others to use and extend the code and
mathematical libraries freely.</p>
<p>To install Lean in your computer consider using the <a href="https://github.com/leanprover/lean4/blob/master/doc/quickstart.md">Quickstart</a> instructions. The Lean source code, and instructions for building Lean, are available at
<a href="https://github.com/leanprover/lean4/">https://github.com/leanprover/lean4/</a>.</p>
<p>This tutorial describes the current version of Lean, known as Lean 4.</p>
<h2><a class="header" href="#about-this-book" id="about-this-book">About this Book</a></h2>
<p>This book is designed to teach you to develop and verify proofs in Lean. Much of the background information you will
need in order to do this is not specific to Lean at all. To start with, you will learn the logical system that Lean is
based on, a version of <em>dependent type theory</em> that is powerful enough to prove almost any conventional mathematical
theorem, and expressive enough to do it in a natural way. More specifically, Lean is based on a version of a system
known as the Calculus of Constructions with inductive types. Lean can not only define mathematical objects and express
mathematical assertions in dependent type theory, but it also can be used as a language for writing proofs.</p>
<p>Because fully detailed axiomatic proofs are so complicated, the challenge of theorem proving is to have the computer
fill in as many of the details as possible. You will learn various methods to support this in <a href="dependent_type_theory.html">dependent type
theory</a>. For example, term rewriting, and Lean's automated methods for simplifying terms and
expressions automatically. Similarly, methods of <em>elaboration</em> and <em>type inference</em>, which can be used to support
flexible forms of algebraic reasoning.</p>
<p>Finally, you will learn about features that are specific to Lean, including the language you use to communicate
with the system, and the mechanisms Lean offers for managing complex theories and data.</p>
<p>Throughout the text you will find examples of Lean code like the one below:</p>
<pre><code class="language-lean">theorem and_commutative (p q : Prop) : p ∧ q → q ∧ p :=
  fun hpq : p ∧ q =&gt;
  have hp : p := And.left hpq
  have hq : q := And.right hpq
  show q ∧ p from And.intro hq hp
</code></pre>
<p>If you are reading the book inside of <a href="https://code.visualstudio.com/">VS Code</a>, you will see a button that reads &quot;try it!&quot; Pressing the button copies the example to your editor with enough surrounding context to make the code compile correctly. You can type
things into the editor and modify the examples, and Lean will check the results and provide feedback continuously as you
type. We recommend running the examples and experimenting with the code on your own as you work through the chapters
that follow. You can open this book on VS Code by using the command &quot;Lean 4: Open Documentation View&quot;.</p>
<h2><a class="header" href="#acknowledgments" id="acknowledgments">Acknowledgments</a></h2>
<p>This tutorial is an open access project maintained on Github. Many people have contributed to the effort, providing
corrections, suggestions, examples, and text. We are grateful to Ulrik Buchholz, Kevin Buzzard, Mario Carneiro, Nathan
Carter, Eduardo Cavazos, Amine Chaieb, Joe Corneli, William DeMeo, Marcus Klaas de Vries, Ben Dyer, Gabriel Ebner,
Anthony Hart, Simon Hudon, Sean Leather, Assia Mahboubi, Gihan Marasingha, Patrick Massot, Christopher John Mazey,
Sebastian Ullrich, Floris van Doorn, Daniel Velleman, Théo Zimmerman, Paul Chisholm, Chris Lovett, and Siddhartha Gadgil for their contributions.  Please see <a href="https://github.com/leanprover/">lean prover</a> and <a href="https://github.com/leanprover-community/">lean community</a> for an up to date list
of our amazing contributors.</p>
<h1><a class="header" href="#dependent-type-theory" id="dependent-type-theory">Dependent Type Theory</a></h1>
<p>Dependent type theory is a powerful and expressive language, allowing
you to express complex mathematical assertions, write complex hardware
and software specifications, and reason about both of these in a
natural and uniform way. Lean is based on a version of dependent type
theory known as the <em>Calculus of Constructions</em>, with a countable
hierarchy of non-cumulative universes and inductive types. By the end
of this chapter, you will understand much of what this means.</p>
<h2><a class="header" href="#simple-type-theory" id="simple-type-theory">Simple Type Theory</a></h2>
<p>&quot;Type theory&quot; gets its name from the fact that every expression has an
associated <em>type</em>. For example, in a given context, <code>x + 0</code> may
denote a natural number and <code>f</code> may denote a function on the natural
numbers. For those who like precise definitions, a Lean natural number
is an arbitrary-precision unsigned integer.</p>
<p>Here are some examples of how you can declare objects in Lean and
check their types.</p>
<pre><code class="language-lean">/- Define some constants. -/

def m : Nat := 1       -- m is a natural number
def n : Nat := 0
def b1 : Bool := true  -- b1 is a Boolean
def b2 : Bool := false

/- Check their types. -/

#check m            -- output: Nat
#check n
#check n + 0        -- Nat
#check m * (n + 0)  -- Nat
#check b1           -- Bool
#check b1 &amp;&amp; b2     -- &quot;&amp;&amp;&quot; is the Boolean and
#check b1 || b2     -- Boolean or
#check true         -- Boolean &quot;true&quot;

/- Evaluate -/

#eval 5 * 4         -- 20
#eval m + 2         -- 3
#eval b1 &amp;&amp; b2      -- false
</code></pre>
<p>Any text between <code>/-</code> and <code>-/</code> constitutes a comment block that is
ignored by Lean. Similarly, two dashes <code>--</code> indicate that the rest of
the line contains a comment that is also ignored. Comment blocks can
be nested, making it possible to &quot;comment out&quot; chunks of code, just as
in many programming languages.</p>
<p>The <code>def</code> keyword declares new constant symbols into the
working environment. In the example above, <code>def m : Nat := 1</code>
defines a new constant <code>m</code> of type <code>Nat</code> whose value is <code>1</code>.
The <code>#check</code> command asks Lean to report their
types; in Lean, auxiliary commands that query the system for
information typically begin with the hash (#) symbol.
The <code>#eval</code> command asks Lean to evaluate the given expression.
You should try
declaring some constants and type checking some expressions on your
own. Declaring new objects in this manner is a good way to experiment
with the system.</p>
<p>What makes simple type theory powerful is that you can build new types
out of others. For example, if <code>a</code> and <code>b</code> are types, <code>a -&gt; b</code>
denotes the type of functions from <code>a</code> to <code>b</code>, and <code>a × b</code>
denotes the type of pairs consisting of an element of <code>a</code> paired
with an element of <code>b</code>, also known as the <em>Cartesian product</em>. Note
that <code>×</code> is a Unicode symbol. The judicious use of Unicode improves
legibility, and all modern editors have great support for it. In the
Lean standard library, you often see Greek letters to denote types,
and the Unicode symbol <code>→</code> as a more compact version of <code>-&gt;</code>.</p>
<pre><code class="language-lean">#check Nat → Nat      -- type the arrow as &quot;\to&quot; or &quot;\r&quot;
#check Nat -&gt; Nat     -- alternative ASCII notation

#check Nat × Nat      -- type the product as &quot;\times&quot;
#check Prod Nat Nat   -- alternative notation

#check Nat → Nat → Nat
#check Nat → (Nat → Nat)  --  same type as above

#check Nat × Nat → Nat
#check (Nat → Nat) → Nat -- a &quot;functional&quot;

#check Nat.succ     -- Nat → Nat
#check (0, 1)       -- Nat × Nat
#check Nat.add      -- Nat → Nat → Nat

#check Nat.succ 2   -- Nat
#check Nat.add 3    -- Nat → Nat
#check Nat.add 5 2  -- Nat
#check (5, 9).1     -- Nat
#check (5, 9).2     -- Nat

#eval Nat.succ 2   -- 3
#eval Nat.add 5 2  -- 7
#eval (5, 9).1     -- 5
#eval (5, 9).2     -- 9
</code></pre>
<p>Once again, you should try some examples on your own.</p>
<p>Let's take a look at some basic syntax. You can enter the unicode
arrow <code>→</code> by typing <code>\to</code> or <code>\r</code> or <code>\-&gt;</code>. You can also use the
ASCII alternative <code>-&gt;</code>, so the expressions <code>Nat -&gt; Nat</code> and <code>Nat → Nat</code> mean the same thing. Both expressions denote the type of
functions that take a natural number as input and return a natural
number as output. The unicode symbol <code>×</code> for the Cartesian product
is entered as <code>\times</code>. You will generally use lower-case Greek
letters like <code>α</code>, <code>β</code>, and <code>γ</code> to range over types. You can
enter these particular ones with <code>\a</code>, <code>\b</code>, and <code>\g</code>.</p>
<p>There are a few more things to notice here. First, the application of
a function <code>f</code> to a value <code>x</code> is denoted <code>f x</code> (e.g., <code>Nat.succ 2</code>).
Second, when writing type expressions, arrows associate to the <em>right</em>; for
example, the type of <code>Nat.add</code> is <code>Nat → Nat → Nat</code> which is equivalent
to <code>Nat → (Nat → Nat)</code>. Thus you can
view <code>Nat.add</code> as a function that takes a natural number and returns
another function that takes a natural number and returns a natural
number. In type theory, this is generally more convenient than
writing <code>Nat.add</code> as a function that takes a pair of natural numbers as
input and returns a natural number as output. For example, it allows
you to &quot;partially apply&quot; the function <code>Nat.add</code>.  The example above shows
that <code>Nat.add 3</code> has type <code>Nat → Nat</code>, that is, <code>Nat.add 3</code> returns a
function that &quot;waits&quot; for a second argument, <code>n</code>, which is then
equivalent to writing <code>Nat.add 3 n</code>.</p>
<!-- Taking a function ``h`` of type ``Nat
× Nat → Nat`` and "redefining" it to look like ``g`` is a process
known as *currying*. -->
<p>You have seen that if you have <code>m : Nat</code> and <code>n : Nat</code>, then
<code>(m, n)</code> denotes the ordered pair of <code>m</code> and <code>n</code> which is of
type <code>Nat × Nat</code>. This gives you a way of creating pairs of natural
numbers. Conversely, if you have <code>p : Nat × Nat</code>, then you can write
<code>p.1 : Nat</code> and <code>p.2 : Nat</code>. This gives you a way of extracting
its two components.</p>
<h2><a class="header" href="#types-as-objects" id="types-as-objects">Types as objects</a></h2>
<p>One way in which Lean's dependent type theory extends simple type
theory is that types themselves --- entities like <code>Nat</code> and <code>Bool</code>
--- are first-class citizens, which is to say that they themselves are
objects. For that to be the case, each of them also has to have a
type.</p>
<pre><code class="language-lean">#check Nat               -- Type
#check Bool              -- Type
#check Nat → Bool        -- Type
#check Nat × Bool        -- Type
#check Nat → Nat         -- ...
#check Nat × Nat → Nat
#check Nat → Nat → Nat
#check Nat → (Nat → Nat)
#check Nat → Nat → Bool
#check (Nat → Nat) → Nat
</code></pre>
<p>You can see that each one of the expressions above is an object of
type <code>Type</code>. You can also declare new constants for types:</p>
<pre><code class="language-lean">def α : Type := Nat
def β : Type := Bool
def F : Type → Type := List
def G : Type → Type → Type := Prod

#check α        -- Type
#check F α      -- Type
#check F Nat    -- Type
#check G α      -- Type → Type
#check G α β    -- Type
#check G α Nat  -- Type
</code></pre>
<p>As the example above suggests, you have already seen an example of a function of type
<code>Type → Type → Type</code>, namely, the Cartesian product <code>Prod</code>:</p>
<pre><code class="language-lean">def α : Type := Nat
def β : Type := Bool

#check Prod α β       -- Type
#check α × β          -- Type

#check Prod Nat Nat   -- Type
#check Nat × Nat      -- Type
</code></pre>
<p>Here is another example: given any type <code>α</code>, the type <code>List α</code>
denotes the type of lists of elements of type <code>α</code>.</p>
<pre><code class="language-lean">def α : Type := Nat

#check List α    -- Type
#check List Nat  -- Type
</code></pre>
<p>Given that every expression in Lean has a type, it is natural to ask:
what type does <code>Type</code> itself have?</p>
<pre><code class="language-lean">#check Type      -- Type 1
</code></pre>
<p>You have actually come up against one of the most subtle aspects of
Lean's typing system. Lean's underlying foundation has an infinite
hierarchy of types:</p>
<pre><code class="language-lean">#check Type     -- Type 1
#check Type 1   -- Type 2
#check Type 2   -- Type 3
#check Type 3   -- Type 4
#check Type 4   -- Type 5
</code></pre>
<p>Think of <code>Type 0</code> as a universe of &quot;small&quot; or &quot;ordinary&quot; types.
<code>Type 1</code> is then a larger universe of types, which contains <code>Type 0</code> as an element, and <code>Type 2</code> is an even larger universe of types,
which contains <code>Type 1</code> as an element. The list is indefinite, so
that there is a <code>Type n</code> for every natural number <code>n</code>. <code>Type</code> is
an abbreviation for <code>Type 0</code>:</p>
<pre><code class="language-lean">#check Type
#check Type 0
</code></pre>
<p>The following table may help concretize the relationships being discussed.
Movement along the x-axis represents a change in the universe, while movement
along the y-axis represents a change in what is sometimes referred to as
&quot;degree&quot;.</p>
<table><thead><tr><th align="center"></th><th align="center"></th><th align="center"></th><th align="center"></th><th align="center"></th><th align="center"></th></tr></thead><tbody>
<tr><td align="center">sort</td><td align="center">Prop (Sort 0)</td><td align="center">Type (Sort 1)</td><td align="center">Type 1 (Sort 2)</td><td align="center">Type 2 (Sort 3)</td><td align="center">...</td></tr>
<tr><td align="center">type</td><td align="center">True</td><td align="center">Bool</td><td align="center">Nat -&gt; Type</td><td align="center">Type -&gt; Type 1</td><td align="center">...</td></tr>
<tr><td align="center">term</td><td align="center">trivial</td><td align="center">true</td><td align="center">fun n =&gt; Fin n</td><td align="center">fun (_ : Type) =&gt; Type</td><td align="center">...</td></tr>
</tbody></table>
<p>Some operations, however, need to be <em>polymorphic</em> over type
universes. For example, <code>List α</code> should make sense for any type
<code>α</code>, no matter which type universe <code>α</code> lives in. This explains the
type signature of the function <code>List</code>:</p>
<pre><code class="language-lean">#check List    -- List.{u} (α : Type u) : Type u
</code></pre>
<p>Here <code>u</code> is a variable ranging over type levels. The output of the
<code>#check</code> command means that whenever <code>α</code> has type <code>Type n</code>,
<code>List α</code> also has type <code>Type n</code>. The function <code>Prod</code> is
similarly polymorphic:</p>
<pre><code class="language-lean">#check Prod    -- Prod.{u, v} (α : Type u) (β : Type v) : Type (max u v)
</code></pre>
<p>To define polymorphic constants, Lean allows you to
declare universe variables explicitly using the <code>universe</code> command:</p>
<pre><code class="language-lean">universe u

def F (α : Type u) : Type u := Prod α α

#check F    -- Type u → Type u
</code></pre>
<p>You can avoid the universe command by providing the universe parameters when defining F.</p>
<pre><code class="language-lean">def F.{u} (α : Type u) : Type u := Prod α α

#check F    -- Type u → Type u
</code></pre>
<h2><a class="header" href="#function-abstraction-and-evaluation" id="function-abstraction-and-evaluation">Function Abstraction and Evaluation</a></h2>
<p>Lean provides a <code>fun</code> (or <code>λ</code>) keyword to create a function
from an expression as follows:</p>
<pre><code class="language-lean">#check fun (x : Nat) =&gt; x + 5   -- Nat → Nat
#check λ (x : Nat) =&gt; x + 5     -- λ and fun mean the same thing
#check fun x =&gt; x + 5     -- Nat inferred
#check λ x =&gt; x + 5       -- Nat inferred
</code></pre>
<p>You can evaluate a lambda function by passing the required parameters:</p>
<pre><code class="language-lean">#eval (λ x : Nat =&gt; x + 5) 10    -- 15
</code></pre>
<p>Creating a function from another expression is a process known as
<em>lambda abstraction</em>. Suppose you have the variable <code>x : α</code> and you can
construct an expression <code>t : β</code>, then the expression <code>fun (x : α) =&gt; t</code>, or, equivalently, <code>λ (x : α) =&gt; t</code>, is an object of type <code>α → β</code>. Think of this as the function from <code>α</code> to <code>β</code> which maps
any value <code>x</code> to the value <code>t</code>.</p>
<p>Here are some more examples</p>
<pre><code class="language-lean">#check fun x : Nat =&gt; fun y : Bool =&gt; if not y then x + 1 else x + 2
#check fun (x : Nat) (y : Bool) =&gt; if not y then x + 1 else x + 2
#check fun x y =&gt; if not y then x + 1 else x + 2   -- Nat → Bool → Nat
</code></pre>
<p>Lean interprets the final three examples as the same expression; in
the last expression, Lean infers the type of <code>x</code> and <code>y</code> from the
expression <code>if not y then x + 1 else x + 2</code>.</p>
<p>Some mathematically common examples of operations of functions can be
described in terms of lambda abstraction:</p>
<pre><code class="language-lean">def f (n : Nat) : String := toString n
def g (s : String) : Bool := s.length &gt; 0

#check fun x : Nat =&gt; x        -- Nat → Nat
#check fun x : Nat =&gt; true     -- Nat → Bool
#check fun x : Nat =&gt; g (f x)  -- Nat → Bool
#check fun x =&gt; g (f x)        -- Nat → Bool
</code></pre>
<p>Think about what these expressions mean. The expression
<code>fun x : Nat =&gt; x</code> denotes the identity function on <code>Nat</code>, the
expression <code>fun x : Nat =&gt; true</code> denotes the constant function that
always returns <code>true</code>, and <code>fun x : Nat =&gt; g (f x)</code> denotes the
composition of <code>f</code> and <code>g</code>.  You can, in general, leave off the
type annotation and let Lean infer it for you.  So, for example, you
can write <code>fun x =&gt; g (f x)</code> instead of <code>fun x : Nat =&gt; g (f x)</code>.</p>
<p>You can pass functions as parameters and by giving them names <code>f</code>
and <code>g</code> you can then use those functions in the implementation:</p>
<pre><code class="language-lean">#check fun (g : String → Bool) (f : Nat → String) (x : Nat) =&gt; g (f x)
-- (String → Bool) → (Nat → String) → Nat → Bool
</code></pre>
<p>You can also pass types as parameters:</p>
<pre><code class="language-lean">#check fun (α β γ : Type) (g : β → γ) (f : α → β) (x : α) =&gt; g (f x)
</code></pre>
<p>The last expression, for example, denotes the function that takes
three types, <code>α</code>, <code>β</code>, and <code>γ</code>, and two functions, <code>g : β → γ</code>
and <code>f : α → β</code>, and returns the composition of <code>g</code> and <code>f</code>.
(Making sense of the type of this function requires an understanding
of dependent products, which will be explained below.)</p>
<p>The general form of a lambda expression is <code>fun x : α =&gt; t</code>, where
the variable <code>x</code> is a &quot;bound variable&quot;: it is really a placeholder,
whose &quot;scope&quot; does not extend beyond the expression <code>t</code>.  For
example, the variable <code>b</code> in the expression <code>fun (b : β) (x : α) =&gt; b</code>
has nothing to do with the constant <code>b</code> declared earlier.  In fact,
the expression denotes the same function as <code>fun (u : β) (z : α) =&gt; u</code>.</p>
<p>Formally, expressions that are the same up to a renaming of bound
variables are called <em>alpha equivalent</em>, and are considered &quot;the
same.&quot; Lean recognizes this equivalence.</p>
<p>Notice that applying a term <code>t : α → β</code> to a term <code>s : α</code> yields
an expression <code>t s : β</code>. Returning to the previous example and
renaming bound variables for clarity, notice the types of the
following expressions:</p>
<pre><code class="language-lean">#check (fun x : Nat =&gt; x) 1     -- Nat
#check (fun x : Nat =&gt; true) 1  -- Bool

def f (n : Nat) : String := toString n
def g (s : String) : Bool := s.length &gt; 0

#check
  (fun (α β γ : Type) (u : β → γ) (v : α → β) (x : α) =&gt; u (v x)) Nat String Bool g f 0
  -- Bool
</code></pre>
<p>As expected, the expression <code>(fun x : Nat =&gt;  x) 1</code> has type <code>Nat</code>.
In fact, more should be true: applying the expression <code>(fun x : Nat =&gt; x)</code> to <code>1</code> should &quot;return&quot; the value <code>1</code>. And, indeed, it does:</p>
<pre><code class="language-lean">#eval (fun x : Nat =&gt; x) 1     -- 1
#eval (fun x : Nat =&gt; true) 1  -- true
</code></pre>
<p>You will see later how these terms are evaluated. For now, notice that
this is an important feature of dependent type theory: every term has
a computational behavior, and supports a notion of <em>normalization</em>. In
principle, two terms that reduce to the same value are called
<em>definitionally equal</em>. They are considered &quot;the same&quot; by Lean's type
checker, and Lean does its best to recognize and support these
identifications.</p>
<p>Lean is a complete programming language. It has a compiler that
generates a binary executable and an interactive interpreter. You can
use the command <code>#eval</code> to execute expressions, and it is the
preferred way of testing your functions.</p>
<!--
Note that `#eval` and
`#reduce` are *not* equivalent. The command `#eval` first compiles
Lean expressions into an intermediate representation (IR) and then
uses an interpreter to execute the generated IR. Some builtin types
(e.g., `Nat`, `String`, `Array`) have a more efficient representation
in the IR. The IR has support for using foreign functions that are
opaque to Lean.

In contrast, the ``#reduce`` command relies on a reduction engine
similar to the one used in Lean's trusted kernel, the part of Lean
that is responsible for checking and verifying the correctness of
expressions and proofs. It is less efficient than ``#eval``, and
treats all foreign functions as opaque constants. You will learn later
that there are some other differences between the two commands.
-->
<h2><a class="header" href="#definitions" id="definitions">Definitions</a></h2>
<p>Recall that the <code>def</code> keyword provides one important way of declaring new named
objects.</p>
<pre><code class="language-lean">def double (x : Nat) : Nat :=
  x + x
</code></pre>
<p>This might look more familiar to you if you know how functions work in
other programming languages. The name <code>double</code> is defined as a
function that takes an input parameter <code>x</code> of type <code>Nat</code>, where the
result of the call is <code>x + x</code>, so it is returning type <code>Nat</code>. You
can then invoke this function using:</p>
<pre><code class="language-lean"><span class="boring">def double (x : Nat) : Nat :=
</span><span class="boring"> x + x
</span>#eval double 3    -- 6
</code></pre>
<p>In this case you can think of <code>def</code> as a kind of named <code>lambda</code>.
The following yields the same result:</p>
<pre><code class="language-lean">def double : Nat → Nat :=
  fun x =&gt; x + x

#eval double 3    -- 6
</code></pre>
<p>You can omit the type declarations when Lean has enough information to
infer it.  Type inference is an important part of Lean:</p>
<pre><code class="language-lean">def double :=
  fun (x : Nat) =&gt; x + x
</code></pre>
<p>The general form of a definition is <code>def foo : α := bar</code> where
<code>α</code> is the type returned from the expression <code>bar</code>.  Lean can
usually infer the type <code>α</code>, but it is often a good idea to write it
explicitly.  This clarifies your intention, and Lean will flag an
error if the right-hand side of the definition does not have a matching
type.</p>
<p>The right hand side <code>bar</code> can be any expression, not just a lambda.
So <code>def</code> can also be used to simply name a value like this:</p>
<pre><code class="language-lean">def pi := 3.141592654
</code></pre>
<p><code>def</code> can take multiple input parameters.  Let's create one
that adds two natural numbers:</p>
<pre><code class="language-lean">def add (x y : Nat) :=
  x + y

#eval add 3 2               -- 5
</code></pre>
<p>The parameter list can be separated like this:</p>
<pre><code class="language-lean"><span class="boring">def double (x : Nat) : Nat :=
</span><span class="boring"> x + x
</span>def add (x : Nat) (y : Nat) :=
  x + y

#eval add (double 3) (7 + 9)  -- 22
</code></pre>
<p>Notice here we called the <code>double</code> function to create the first
parameter to <code>add</code>.</p>
<p>You can use other more interesting expressions inside a <code>def</code>:</p>
<pre><code class="language-lean">def greater (x y : Nat) :=
  if x &gt; y then x
  else y
</code></pre>
<p>You can probably guess what this one will do.</p>
<p>You can also define a function that takes another function as input.
The following calls a given function twice passing the output of the
first invocation to the second:</p>
<pre><code class="language-lean"><span class="boring">def double (x : Nat) : Nat :=
</span><span class="boring"> x + x
</span>def doTwice (f : Nat → Nat) (x : Nat) : Nat :=
  f (f x)

#eval doTwice double 2   -- 8
</code></pre>
<p>Now to get a bit more abstract, you can also specify arguments that
are like type parameters:</p>
<pre><code class="language-lean">def compose (α β γ : Type) (g : β → γ) (f : α → β) (x : α) : γ :=
  g (f x)
</code></pre>
<p>This means <code>compose</code> is a function that takes any two functions as input
arguments, so long as those functions each take only one input.
The type algebra <code>β → γ</code> and <code>α → β</code> means it is a requirement
that the type of the output of the second function must match the
type of the input to the first function - which makes sense, otherwise
the two functions would not be composable.</p>
<p><code>compose</code> also takes a 3rd argument of type <code>α</code> which
it uses to invoke the second function (locally named <code>f</code>) and it
passes the result of that function (which is type <code>β</code>) as input to the
first function (locally named <code>g</code>).  The first function returns a type
<code>γ</code> so that is also the return type of the <code>compose</code> function.</p>
<p><code>compose</code> is also very general in that it works over any type
<code>α β γ</code>.  This means <code>compose</code> can compose just about any 2 functions
so long as they each take one parameter, and so long as the type of
output of the second matches the input of the first.  For example:</p>
<pre><code class="language-lean"><span class="boring">def compose (α β γ : Type) (g : β → γ) (f : α → β) (x : α) : γ :=
</span><span class="boring"> g (f x)
</span><span class="boring">def double (x : Nat) : Nat :=
</span><span class="boring"> x + x
</span>def square (x : Nat) : Nat :=
  x * x

#eval compose Nat Nat Nat double square 3  -- 18
</code></pre>
<h2><a class="header" href="#local-definitions" id="local-definitions">Local Definitions</a></h2>
<p>Lean also allows you to introduce &quot;local&quot; definitions using the
<code>let</code> keyword. The expression <code>let a := t1; t2</code> is
definitionally equal to the result of replacing every occurrence of
<code>a</code> in <code>t2</code> by <code>t1</code>.</p>
<pre><code class="language-lean">#check let y := 2 + 2; y * y   -- Nat
#eval  let y := 2 + 2; y * y   -- 16

def twice_double (x : Nat) : Nat :=
  let y := x + x; y * y

#eval twice_double 2   -- 16
</code></pre>
<p>Here, <code>twice_double x</code> is definitionally equal to the term <code>(x + x) * (x + x)</code>.</p>
<p>You can combine multiple assignments by chaining <code>let</code> statements:</p>
<pre><code class="language-lean">#check let y := 2 + 2; let z := y + y; z * z   -- Nat
#eval  let y := 2 + 2; let z := y + y; z * z   -- 64
</code></pre>
<p>The <code>;</code> can be omitted when a line break is used.</p>
<pre><code class="language-lean">def t (x : Nat) : Nat :=
  let y := x + x
  y * y
</code></pre>
<p>Notice that the meaning of the expression <code>let a := t1; t2</code> is very
similar to the meaning of <code>(fun a =&gt; t2) t1</code>, but the two are not
the same. In the first expression, you should think of every instance
of <code>a</code> in <code>t2</code> as a syntactic abbreviation for <code>t1</code>. In the
second expression, <code>a</code> is a variable, and the expression
<code>fun a =&gt; t2</code> has to make sense independently of the value of <code>a</code>.
The <code>let</code> construct is a stronger means of abbreviation, and there
are expressions of the form <code>let a := t1; t2</code> that cannot be
expressed as <code>(fun a =&gt; t2) t1</code>. As an exercise, try to understand
why the definition of <code>foo</code> below type checks, but the definition of
<code>bar</code> does not.</p>
<pre><code class="language-lean">def foo := let a := Nat; fun x : a =&gt; x + 2
/-
  def bar := (fun a =&gt; fun x : a =&gt; x + 2) Nat
-/
</code></pre>
<h1><a class="header" href="#variables-and-sections" id="variables-and-sections">Variables and Sections</a></h1>
<p>Consider the following three function definitions:</p>
<pre><code class="language-lean">def compose (α β γ : Type) (g : β → γ) (f : α → β) (x : α) : γ :=
  g (f x)

def doTwice (α : Type) (h : α → α) (x : α) : α :=
  h (h x)

def doThrice (α : Type) (h : α → α) (x : α) : α :=
  h (h (h x))
</code></pre>
<p>Lean provides you with the <code>variable</code> command to make such
declarations look more compact:</p>
<pre><code class="language-lean">variable (α β γ : Type)

def compose (g : β → γ) (f : α → β) (x : α) : γ :=
  g (f x)

def doTwice (h : α → α) (x : α) : α :=
  h (h x)

def doThrice (h : α → α) (x : α) : α :=
  h (h (h x))
</code></pre>
<p>You can declare variables of any type, not just <code>Type</code> itself:</p>
<pre><code class="language-lean">variable (α β γ : Type)
variable (g : β → γ) (f : α → β) (h : α → α)
variable (x : α)

def compose := g (f x)
def doTwice := h (h x)
def doThrice := h (h (h x))

#print compose
#print doTwice
#print doThrice
</code></pre>
<p>Printing them out shows that all three groups of definitions have
exactly the same effect.</p>
<p>The <code>variable</code> command instructs Lean to insert the declared
variables as bound variables in definitions that refer to them by
name. Lean is smart enough to figure out which variables are used
explicitly or implicitly in a definition. You can therefore proceed as
though <code>α</code>, <code>β</code>, <code>γ</code>, <code>g</code>, <code>f</code>, <code>h</code>, and <code>x</code> are fixed
objects when you write your definitions, and let Lean abstract the
definitions for you automatically.</p>
<p>When declared in this way, a variable stays in scope until the end of
the file you are working on. Sometimes, however, it is useful to limit
the scope of a variable. For that purpose, Lean provides the notion of
a <code>section</code>:</p>
<pre><code class="language-lean">section useful
  variable (α β γ : Type)
  variable (g : β → γ) (f : α → β) (h : α → α)
  variable (x : α)

  def compose := g (f x)
  def doTwice := h (h x)
  def doThrice := h (h (h x))
end useful
</code></pre>
<p>When the section is closed, the variables go out of scope, and cannot
be referenced any more.</p>
<p>You do not have to indent the lines within a section. Nor do you have
to name a section, which is to say, you can use an anonymous
<code>section</code> / <code>end</code> pair. If you do name a section, however, you
have to close it using the same name. Sections can also be nested,
which allows you to declare new variables incrementally.</p>
<h1><a class="header" href="#namespaces" id="namespaces">Namespaces</a></h1>
<p>Lean provides you with the ability to group definitions into nested,
hierarchical <em>namespaces</em>:</p>
<pre><code class="language-lean">namespace Foo
  def a : Nat := 5
  def f (x : Nat) : Nat := x + 7

  def fa : Nat := f a
  def ffa : Nat := f (f a)

  #check a
  #check f
  #check fa
  #check ffa
  #check Foo.fa
end Foo

-- #check a  -- error
-- #check f  -- error
#check Foo.a
#check Foo.f
#check Foo.fa
#check Foo.ffa

open Foo

#check a
#check f
#check fa
#check Foo.fa
</code></pre>
<p>When you declare that you are working in the namespace <code>Foo</code>, every
identifier you declare has a full name with prefix &quot;<code>Foo.</code>&quot;. Within
the namespace, you can refer to identifiers by their shorter names,
but once you end the namespace, you have to use the longer names.
Unlike <code>section</code>, namespaces require a name. There is only one
anonymous namespace at the root level.</p>
<p>The <code>open</code> command brings the shorter names into the current
context. Often, when you import a module, you will want to open one or
more of the namespaces it contains, to have access to the short
identifiers. But sometimes you will want to leave this information
protected by a fully qualified name, for example, when they conflict
with identifiers in another namespace you want to use. Thus namespaces
give you a way to manage names in your working environment.</p>
<p>For example, Lean groups definitions and theorems involving lists into
a namespace <code>List</code>.</p>
<pre><code class="language-lean">#check List.nil
#check List.cons
#check List.map
</code></pre>
<p>The command <code>open List</code> allows you to use the shorter names:</p>
<pre><code class="language-lean">open List

#check nil
#check cons
#check map
</code></pre>
<p>Like sections, namespaces can be nested:</p>
<pre><code class="language-lean">namespace Foo
  def a : Nat := 5
  def f (x : Nat) : Nat := x + 7

  def fa : Nat := f a

  namespace Bar
    def ffa : Nat := f (f a)

    #check fa
    #check ffa
  end Bar

  #check fa
  #check Bar.ffa
end Foo

#check Foo.fa
#check Foo.Bar.ffa

open Foo

#check fa
#check Bar.ffa
</code></pre>
<p>Namespaces that have been closed can later be reopened, even in another file:</p>
<pre><code class="language-lean">namespace Foo
  def a : Nat := 5
  def f (x : Nat) : Nat := x + 7

  def fa : Nat := f a
end Foo

#check Foo.a
#check Foo.f

namespace Foo
  def ffa : Nat := f (f a)
end Foo
</code></pre>
<p>Like sections, nested namespaces have to be closed in the order they
are opened. Namespaces and sections serve different purposes:
namespaces organize data and sections declare variables for insertion
in definitions. Sections are also useful for delimiting the scope of
commands such as <code>set_option</code> and <code>open</code>.</p>
<p>In many respects, however, a <code>namespace ... end</code> block behaves the
same as a <code>section ... end</code> block. In particular, if you use the
<code>variable</code> command within a namespace, its scope is limited to the
namespace. Similarly, if you use an <code>open</code> command within a
namespace, its effects disappear when the namespace is closed.</p>
<h2><a class="header" href="#what-makes-dependent-type-theory-dependent" id="what-makes-dependent-type-theory-dependent">What makes dependent type theory dependent?</a></h2>
<p>The short explanation is that types can depend on parameters. You
have already seen a nice example of this: the type <code>List α</code> depends
on the argument <code>α</code>, and this dependence is what distinguishes
<code>List Nat</code> and <code>List Bool</code>. For another example, consider the
type <code>Vector α n</code>, the type of vectors of elements of <code>α</code> of
length <code>n</code>.  This type depends on <em>two</em> parameters: the type of the
elements in the vector (<code>α : Type</code>) and the length of the vector
<code>n : Nat</code>.</p>
<p>Suppose you wish to write a function <code>cons</code> which inserts a new
element at the head of a list. What type should <code>cons</code> have? Such a
function is <em>polymorphic</em>: you expect the <code>cons</code> function for
<code>Nat</code>, <code>Bool</code>, or an arbitrary type <code>α</code> to behave the same way.
So it makes sense to take the type to be the first argument to
<code>cons</code>, so that for any type, <code>α</code>, <code>cons α</code> is the insertion
function for lists of type <code>α</code>. In other words, for every <code>α</code>,
<code>cons α</code> is the function that takes an element <code>a : α</code> and a list
<code>as : List α</code>, and returns a new list, so you have <code>cons α a as : List α</code>.</p>
<p>It is clear that <code>cons α</code> should have type <code>α → List α → List α</code>.
But what type should <code>cons</code> have?  A first guess might be
<code>Type → α → List α → List α</code>, but, on reflection, this does not make
sense: the <code>α</code> in this expression does not refer to anything,
whereas it should refer to the argument of type <code>Type</code>.  In other
words, <em>assuming</em> <code>α : Type</code> is the first argument to the function,
the type of the next two elements are <code>α</code> and <code>List α</code>. These
types vary depending on the first argument, <code>α</code>.</p>
<pre><code class="language-lean">def cons (α : Type) (a : α) (as : List α) : List α :=
  List.cons a as

#check cons Nat        -- Nat → List Nat → List Nat
#check cons Bool       -- Bool → List Bool → List Bool
#check cons            -- (α : Type) → α → List α → List α
</code></pre>
<p>This is an instance of a <em>dependent function type</em>, or <em>dependent
arrow type</em>. Given <code>α : Type</code> and <code>β : α → Type</code>, think of <code>β</code>
as a family of types over <code>α</code>, that is, a type <code>β a</code> for each
<code>a : α</code>. In that case, the type <code>(a : α) → β a</code> denotes the type
of functions <code>f</code> with the property that, for each <code>a : α</code>, <code>f a</code>
is an element of <code>β a</code>. In other words, the type of the value
returned by <code>f</code> depends on its input.</p>
<p>Notice that <code>(a : α) → β</code> makes sense for any expression <code>β : Type</code>. When the value of <code>β</code> depends on <code>a</code> (as does, for
example, the expression <code>β a</code> in the previous paragraph),
<code>(a : α) → β</code> denotes a dependent function type. When <code>β</code> doesn't
depend on <code>a</code>, <code>(a : α) → β</code> is no different from the type
<code>α → β</code>.  Indeed, in dependent type theory (and in Lean), <code>α → β</code>
is just notation for <code>(a : α) → β</code> when <code>β</code> does not depend on <code>a</code>.</p>
<p>Returning to the example of lists, you can use the command <code>#check</code> to
inspect the type of the following <code>List</code> functions.  The <code>@</code> symbol
and the difference between the round and curly braces will be
explained momentarily.</p>
<pre><code class="language-lean">#check @List.cons    -- {α : Type u_1} → α → List α → List α
#check @List.nil     -- {α : Type u_1} → List α
#check @List.length  -- {α : Type u_1} → List α → Nat
#check @List.append  -- {α : Type u_1} → List α → List α → List α
</code></pre>
<p>Just as dependent function types <code>(a : α) → β a</code> generalize the
notion of a function type <code>α → β</code> by allowing <code>β</code> to depend on
<code>α</code>, dependent Cartesian product types <code>(a : α) × β a</code> generalize
the Cartesian product <code>α × β</code> in the same way. Dependent products
are also called <em>sigma</em> types, and you can also write them as
<code>Σ a : α, β a</code>. You can use <code>⟨a, b⟩</code> or <code>Sigma.mk a b</code> to create a
dependent pair.  The <code>⟨</code> and <code>⟩</code> characters may be typed with
<code>\langle</code> and <code>\rangle</code> or <code>\&lt;</code> and <code>\&gt;</code>, respectively.</p>
<pre><code class="language-lean">universe u v

def f (α : Type u) (β : α → Type v) (a : α) (b : β a) : (a : α) × β a :=
  ⟨a, b⟩

def g (α : Type u) (β : α → Type v) (a : α) (b : β a) : Σ a : α, β a :=
  Sigma.mk a b

def h1 (x : Nat) : Nat :=
  (f Type (fun α =&gt; α) Nat x).2

#eval h1 5 -- 5

def h2 (x : Nat) : Nat :=
  (g Type (fun α =&gt; α) Nat x).2

#eval h2 5 -- 5
</code></pre>
<p>The functions <code>f</code> and <code>g</code> above denote the same function.</p>
<h2><a class="header" href="#implicit-arguments" id="implicit-arguments">Implicit Arguments</a></h2>
<p>Suppose we have an implementation of lists as:</p>
<pre><code class="language-lean"><span class="boring">universe u
</span><span class="boring">def Lst (α : Type u) : Type u := List α
</span><span class="boring">def Lst.cons (α : Type u) (a : α) (as : Lst α) : Lst α := List.cons a as
</span><span class="boring">def Lst.nil (α : Type u) : Lst α := List.nil
</span><span class="boring">def Lst.append (α : Type u) (as bs : Lst α) : Lst α := List.append as bs
</span>#check Lst          -- Lst.{u} (α : Type u) : Type u
#check Lst.cons     -- Lst.cons.{u} (α : Type u) (a : α) (as : Lst α) : Lst α
#check Lst.nil      -- Lst.nil.{u} (α : Type u) : Lst α
#check Lst.append   -- Lst.append.{u} (α : Type u) (as bs : Lst α) : Lst α
</code></pre>
<p>Then, you can construct lists of <code>Nat</code> as follows.</p>
<pre><code class="language-lean"><span class="boring">universe u
</span><span class="boring">def Lst (α : Type u) : Type u := List α
</span><span class="boring">def Lst.cons (α : Type u) (a : α) (as : Lst α) : Lst α := List.cons a as
</span><span class="boring">def Lst.nil (α : Type u) : Lst α := List.nil
</span><span class="boring">def Lst.append (α : Type u) (as bs : Lst α) : Lst α := List.append as bs
</span><span class="boring">#check Lst          -- Type u_1 → Type u_1
</span><span class="boring">#check Lst.cons     -- (α : Type u_1) → α → Lst α → Lst α
</span><span class="boring">#check Lst.nil      -- (α : Type u_1) → Lst α
</span><span class="boring">#check Lst.append   -- (α : Type u_1) → Lst α → Lst α → Lst α
</span>#check Lst.cons Nat 0 (Lst.nil Nat)

def as : Lst Nat := Lst.nil Nat
def bs : Lst Nat := Lst.cons Nat 5 (Lst.nil Nat)

#check Lst.append Nat as bs
</code></pre>
<p>Because the constructors are polymorphic over types, we have to insert
the type <code>Nat</code> as an argument repeatedly. But this information is
redundant: one can infer the argument <code>α</code> in
<code>Lst.cons Nat 5 (Lst.nil Nat)</code> from the fact that the second argument, <code>5</code>, has
type <code>Nat</code>. One can similarly infer the argument in <code>Lst.nil Nat</code>, not
from anything else in that expression, but from the fact that it is
sent as an argument to the function <code>Lst.cons</code>, which expects an element
of type <code>Lst α</code> in that position.</p>
<p>This is a central feature of dependent type theory: terms carry a lot
of information, and often some of that information can be inferred
from the context. In Lean, one uses an underscore, <code>_</code>, to specify
that the system should fill in the information automatically. This is
known as an &quot;implicit argument.&quot;</p>
<pre><code class="language-lean"><span class="boring">universe u
</span><span class="boring">def Lst (α : Type u) : Type u := List α
</span><span class="boring">def Lst.cons (α : Type u) (a : α) (as : Lst α) : Lst α := List.cons a as
</span><span class="boring">def Lst.nil (α : Type u) : Lst α := List.nil
</span><span class="boring">def Lst.append (α : Type u) (as bs : Lst α) : Lst α := List.append as bs
</span><span class="boring">#check Lst          -- Type u_1 → Type u_1
</span><span class="boring">#check Lst.cons     -- (α : Type u_1) → α → Lst α → Lst α
</span><span class="boring">#check Lst.nil      -- (α : Type u_1) → Lst α
</span><span class="boring">#check Lst.append   -- (α : Type u_1) → Lst α → Lst α → Lst α
</span>#check Lst.cons _ 0 (Lst.nil _)

def as : Lst Nat := Lst.nil _
def bs : Lst Nat := Lst.cons _ 5 (Lst.nil _)

#check Lst.append _ as bs
</code></pre>
<p>It is still tedious, however, to type all these underscores. When a
function takes an argument that can generally be inferred from
context, Lean allows you to specify that this argument should, by
default, be left implicit. This is done by putting the arguments in
curly braces, as follows:</p>
<pre><code class="language-lean">universe u
def Lst (α : Type u) : Type u := List α

def Lst.cons {α : Type u} (a : α) (as : Lst α) : Lst α := List.cons a as
def Lst.nil {α : Type u} : Lst α := List.nil
def Lst.append {α : Type u} (as bs : Lst α) : Lst α := List.append as bs

#check Lst.cons 0 Lst.nil

def as : Lst Nat := Lst.nil
def bs : Lst Nat := Lst.cons 5 Lst.nil

#check Lst.append as bs
</code></pre>
<p>All that has changed are the braces around <code>α : Type u</code> in the
declaration of the variables. We can also use this device in function
definitions:</p>
<pre><code class="language-lean">universe u
def ident {α : Type u} (x : α) := x

#check ident         -- ?m → ?m
#check ident 1       -- Nat
#check ident &quot;hello&quot; -- String
#check @ident        -- {α : Type u_1} → α → α
</code></pre>
<p>This makes the first argument to <code>ident</code> implicit. Notationally,
this hides the specification of the type, making it look as though
<code>ident</code> simply takes an argument of any type. In fact, the function
<code>id</code> is defined in the standard library in exactly this way. We have
chosen a nontraditional name here only to avoid a clash of names.</p>
<p>Variables can also be specified as implicit when they are declared with
the <code>variable</code> command:</p>
<pre><code class="language-lean">universe u

section
  variable {α : Type u}
  variable (x : α)
  def ident := x
end

#check ident
#check ident 4
#check ident &quot;hello&quot;
</code></pre>
<p>This definition of <code>ident</code> here has the same effect as the one
above.</p>
<p>Lean has very complex mechanisms for instantiating implicit arguments,
and we will see that they can be used to infer function types,
predicates, and even proofs. The process of instantiating these
&quot;holes,&quot; or &quot;placeholders,&quot; in a term is often known as
<em>elaboration</em>. The presence of implicit arguments means that at times
there may be insufficient information to fix the meaning of an
expression precisely. An expression like <code>id</code> or <code>List.nil</code> is
said to be <em>polymorphic</em>, because it can take on different meanings in
different contexts.</p>
<p>One can always specify the type <code>T</code> of an expression <code>e</code> by
writing <code>(e : T)</code>. This instructs Lean's elaborator to use the value
<code>T</code> as the type of <code>e</code> when trying to resolve implicit
arguments. In the second pair of examples below, this mechanism is
used to specify the desired types of the expressions <code>id</code> and
<code>List.nil</code>:</p>
<pre><code class="language-lean">#check List.nil               -- List ?m
#check id                     -- ?m → ?m

#check (List.nil : List Nat)  -- List Nat
#check (id : Nat → Nat)       -- Nat → Nat
</code></pre>
<p>Numerals are overloaded in Lean, but when the type of a numeral cannot
be inferred, Lean assumes, by default, that it is a natural number. So
the expressions in the first two <code>#check</code> commands below are
elaborated in the same way, whereas the third <code>#check</code> command
interprets <code>2</code> as an integer.</p>
<pre><code class="language-lean">#check 2            -- Nat
#check (2 : Nat)    -- Nat
#check (2 : Int)    -- Int
</code></pre>
<p>Sometimes, however, we may find ourselves in a situation where we have
declared an argument to a function to be implicit, but now want to
provide the argument explicitly. If <code>foo</code> is such a function, the
notation <code>@foo</code> denotes the same function with all the arguments
made explicit.</p>
<pre><code class="language-lean">#check @id        -- {α : Sort u_1} → α → α
#check @id Nat    -- Nat → Nat
#check @id Bool   -- Bool → Bool

#check @id Nat 1     -- Nat
#check @id Bool true -- Bool
</code></pre>
<p>Notice that now the first <code>#check</code> command gives the type of the
identifier, <code>id</code>, without inserting any placeholders. Moreover, the
output indicates that the first argument is implicit.</p>
<h1><a class="header" href="#propositions-and-proofs" id="propositions-and-proofs">Propositions and Proofs</a></h1>
<p>By now, you have seen some ways of defining objects and functions in
Lean. In this chapter, we will begin to explain how to write
mathematical assertions and proofs in the language of dependent type
theory as well.</p>
<h2><a class="header" href="#propositions-as-types" id="propositions-as-types">Propositions as Types</a></h2>
<p>One strategy for proving assertions about objects defined in the
language of dependent type theory is to layer an assertion language
and a proof language on top of the definition language. But there is
no reason to multiply languages in this way: dependent type theory is
flexible and expressive, and there is no reason we cannot represent
assertions and proofs in the same general framework.</p>
<p>For example, we could introduce a new type, <code>Prop</code>, to represent
propositions, and introduce constructors to build new propositions
from others.</p>
<pre><code class="language-lean"><span class="boring">def Implies (p q : Prop) : Prop := p → q
</span>#check And     -- Prop → Prop → Prop
#check Or      -- Prop → Prop → Prop
#check Not     -- Prop → Prop
#check Implies -- Prop → Prop → Prop

variable (p q r : Prop)
#check And p q                      -- Prop
#check Or (And p q) r               -- Prop
#check Implies (And p q) (And q p)  -- Prop
</code></pre>
<p>We could then introduce, for each element <code>p : Prop</code>, another type
<code>Proof p</code>, for the type of proofs of <code>p</code>.  An &quot;axiom&quot; would be a
constant of such a type.</p>
<pre><code class="language-lean"><span class="boring">def Implies (p q : Prop) : Prop := p → q
</span><span class="boring">structure Proof (p : Prop) : Type where
</span><span class="boring">  proof : p
</span>#check Proof   -- Proof : Prop → Type

axiom and_comm (p q : Prop) : Proof (Implies (And p q) (And q p))

variable (p q : Prop)
#check and_comm p q     -- Proof (Implies (And p q) (And q p))
</code></pre>
<p>In addition to axioms, however, we would also need rules to build new
proofs from old ones. For example, in many proof systems for
propositional logic, we have the rule of modus ponens:</p>
<blockquote>
<p>From a proof of <code>Implies p q</code> and a proof of <code>p</code>, we obtain a proof of <code>q</code>.</p>
</blockquote>
<p>We could represent this as follows:</p>
<pre><code class="language-lean"><span class="boring">def Implies (p q : Prop) : Prop := p → q
</span><span class="boring">structure Proof (p : Prop) : Type where
</span><span class="boring">  proof : p
</span>axiom modus_ponens : (p q : Prop) → Proof (Implies p q) → Proof p → Proof q
</code></pre>
<p>Systems of natural deduction for propositional logic also typically rely on the following rule:</p>
<blockquote>
<p>Suppose that, assuming <code>p</code> as a hypothesis, we have a proof of <code>q</code>. Then we can &quot;cancel&quot; the hypothesis and obtain a proof of <code>Implies p q</code>.</p>
</blockquote>
<p>We could render this as follows:</p>
<pre><code class="language-lean"><span class="boring">def Implies (p q : Prop) : Prop := p → q
</span><span class="boring">structure Proof (p : Prop) : Type where
</span><span class="boring">  proof : p
</span>axiom implies_intro : (p q : Prop) → (Proof p → Proof q) → Proof (Implies p q)
</code></pre>
<p>This approach would provide us with a reasonable way of building assertions and proofs.
Determining that an expression <code>t</code> is a correct proof of assertion <code>p</code> would then
simply be a matter of checking that <code>t</code> has type <code>Proof p</code>.</p>
<p>Some simplifications are possible, however. To start with, we can
avoid writing the term <code>Proof</code> repeatedly by conflating <code>Proof p</code>
with <code>p</code> itself. In other words, whenever we have <code>p : Prop</code>, we
can interpret <code>p</code> as a type, namely, the type of its proofs. We can
then read <code>t : p</code> as the assertion that <code>t</code> is a proof of <code>p</code>.</p>
<p>Moreover, once we make this identification, the rules for implication
show that we can pass back and forth between <code>Implies p q</code> and
<code>p → q</code>. In other words, implication between propositions <code>p</code> and <code>q</code>
corresponds to having a function that takes any element of <code>p</code> to an
element of <code>q</code>. As a result, the introduction of the connective
<code>Implies</code> is entirely redundant: we can use the usual function space
constructor <code>p → q</code> from dependent type theory as our notion of
implication.</p>
<p>This is the approach followed in the Calculus of Constructions, and
hence in Lean as well. The fact that the rules for implication in a
proof system for natural deduction correspond exactly to the rules
governing abstraction and application for functions is an instance of
the <em>Curry-Howard isomorphism</em>, sometimes known as the
<em>propositions-as-types</em> paradigm. In fact, the type <code>Prop</code> is
syntactic sugar for <code>Sort 0</code>, the very bottom of the type hierarchy
described in the last chapter. Moreover, <code>Type u</code> is also just
syntactic sugar for <code>Sort (u+1)</code>. <code>Prop</code> has some special
features, but like the other type universes, it is closed under the
arrow constructor: if we have <code>p q : Prop</code>, then <code>p → q : Prop</code>.</p>
<p>There are at least two ways of thinking about propositions as
types. To some who take a constructive view of logic and mathematics,
this is a faithful rendering of what it means to be a proposition: a
proposition <code>p</code> represents a sort of data type, namely, a
specification of the type of data that constitutes a proof. A proof of
<code>p</code> is then simply an object <code>t : p</code> of the right type.</p>
<p>Those not inclined to this ideology can view it, rather, as a simple
coding trick. To each proposition <code>p</code> we associate a type that is
empty if <code>p</code> is false and has a single element, say <code>*</code>, if <code>p</code>
is true. In the latter case, let us say that (the type associated
with) <code>p</code> is <em>inhabited</em>. It just so happens that the rules for
function application and abstraction can conveniently help us keep
track of which elements of <code>Prop</code> are inhabited. So constructing an
element <code>t : p</code> tells us that <code>p</code> is indeed true. You can think of
the inhabitant of <code>p</code> as being the &quot;fact that <code>p</code> is true.&quot; A
proof of <code>p → q</code> uses &quot;the fact that <code>p</code> is true&quot; to obtain &quot;the
fact that <code>q</code> is true.&quot;</p>
<p>Indeed, if <code>p : Prop</code> is any proposition, Lean's kernel treats any
two elements <code>t1 t2 : p</code> as being definitionally equal, much the
same way as it treats <code>(fun x =&gt; t) s</code> and <code>t[s/x]</code> as
definitionally equal. This is known as <em>proof irrelevance,</em> and is
consistent with the interpretation in the last paragraph. It means
that even though we can treat proofs <code>t : p</code> as ordinary objects in
the language of dependent type theory, they carry no information
beyond the fact that <code>p</code> is true.</p>
<p>The two ways we have suggested thinking about the
propositions-as-types paradigm differ in a fundamental way. From the
constructive point of view, proofs are abstract mathematical objects
that are <em>denoted</em> by suitable expressions in dependent type
theory. In contrast, if we think in terms of the coding trick
described above, then the expressions themselves do not denote
anything interesting. Rather, it is the fact that we can write them
down and check that they are well-typed that ensures that the
proposition in question is true. In other words, the expressions
<em>themselves</em> are the proofs.</p>
<p>In the exposition below, we will slip back and forth between these two
ways of talking, at times saying that an expression &quot;constructs&quot; or
&quot;produces&quot; or &quot;returns&quot; a proof of a proposition, and at other times
simply saying that it &quot;is&quot; such a proof. This is similar to the way
that computer scientists occasionally blur the distinction between
syntax and semantics by saying, at times, that a program &quot;computes&quot; a
certain function, and at other times speaking as though the program
&quot;is&quot; the function in question.</p>
<p>In any case, all that really matters is the bottom line. To formally
express a mathematical assertion in the language of dependent type
theory, we need to exhibit a term <code>p : Prop</code>. To <em>prove</em> that
assertion, we need to exhibit a term <code>t : p</code>. Lean's task, as a
proof assistant, is to help us to construct such a term, <code>t</code>, and to
verify that it is well-formed and has the correct type.</p>
<h2><a class="header" href="#working-with-propositions-as-types" id="working-with-propositions-as-types">Working with Propositions as Types</a></h2>
<p>In the propositions-as-types paradigm, theorems involving only <code>→</code>
can be proved using lambda abstraction and application. In Lean, the
<code>theorem</code> command introduces a new theorem:</p>
<pre><code class="language-lean">variable {p : Prop}
variable {q : Prop}

theorem t1 : p → q → p := fun hp : p =&gt; fun hq : q =&gt; hp
</code></pre>
<p>Compare this proof to the expression <code>fun x : α =&gt; fun y : β =&gt; x</code>
of type <code>α → β → α</code>, where <code>α</code> and <code>β</code> are data types.
This describes the function that takes arguments <code>x</code> and <code>y</code>
of type <code>α</code> and <code>β</code>, respectively, and returns <code>x</code>.
The proof of <code>t1</code> has the same form, the only difference being that
<code>p</code> and <code>q</code> are elements of <code>Prop</code> rather than <code>Type</code>.
Intuitively, our proof of
<code>p → q → p</code> assumes <code>p</code> and <code>q</code> are true, and uses the first
hypothesis (trivially) to establish that the conclusion, <code>p</code>, is
true.</p>
<p>Note that the <code>theorem</code> command is really a version of the
<code>def</code> command: under the propositions and types
correspondence, proving the theorem <code>p → q → p</code> is really the same
as defining an element of the associated type. To the kernel type
checker, there is no difference between the two.</p>
<p>There are a few pragmatic differences between definitions and
theorems, however. In normal circumstances, it is never necessary to
unfold the &quot;definition&quot; of a theorem; by proof irrelevance, any two
proofs of that theorem are definitionally equal. Once the proof of a
theorem is complete, typically we only need to know that the proof
exists; it doesn't matter what the proof is. In light of that fact,
Lean tags proofs as <em>irreducible</em>, which serves as a hint to the
parser (more precisely, the <em>elaborator</em>) that there is generally no
need to unfold it when processing a file. In fact, Lean is generally
able to process and check proofs in parallel, since assessing the
correctness of one proof does not require knowing the details of
another.</p>
<p>As with definitions, the <code>#print</code> command will show you the proof of
a theorem.</p>
<pre><code class="language-lean"><span class="boring">variable {p : Prop}
</span><span class="boring">variable {q : Prop}
</span>theorem t1 : p → q → p := fun hp : p =&gt; fun hq : q =&gt; hp

#print t1
</code></pre>
<p>Notice that the lambda abstractions <code>hp : p</code> and <code>hq : q</code> can be
viewed as temporary assumptions in the proof of <code>t1</code>.  Lean also
allows us to specify the type of the final term <code>hp</code>, explicitly,
with a <code>show</code> statement.</p>
<pre><code class="language-lean"><span class="boring">variable {p : Prop}
</span><span class="boring">variable {q : Prop}
</span>theorem t1 : p → q → p :=
  fun hp : p =&gt;
  fun hq : q =&gt;
  show p from hp
</code></pre>
<p>Adding such extra information can improve the clarity of a proof and
help detect errors when writing a proof. The <code>show</code> command does
nothing more than annotate the type, and, internally, all the
presentations of <code>t1</code> that we have seen produce the same term.</p>
<p>As with ordinary definitions, we can move the lambda-abstracted
variables to the left of the colon:</p>
<pre><code class="language-lean"><span class="boring">variable {p : Prop}
</span><span class="boring">variable {q : Prop}
</span>theorem t1 (hp : p) (hq : q) : p := hp

#print t1    -- p → q → p
</code></pre>
<p>Now we can apply the theorem <code>t1</code> just as a function application.</p>
<pre><code class="language-lean"><span class="boring">variable {p : Prop}
</span><span class="boring">variable {q : Prop}
</span>theorem t1 (hp : p) (hq : q) : p := hp

axiom hp : p

theorem t2 : q → p := t1 hp
</code></pre>
<p>Here, the <code>axiom</code> declaration postulates the existence of an
element of the given type and may compromise logical consistency. For
example, we can use it to postulate the empty type <code>False</code> has an
element.</p>
<pre><code class="language-lean">axiom unsound : False
-- Everything follows from false
theorem ex : 1 = 0 :=
  False.elim unsound
</code></pre>
<p>Declaring an &quot;axiom&quot; <code>hp : p</code> is tantamount to declaring that <code>p</code>
is true, as witnessed by <code>hp</code>. Applying the theorem
<code>t1 : p → q → p</code> to the fact <code>hp : p</code> that <code>p</code> is true yields the theorem
<code>t1 hp : q → p</code>.</p>
<p>Recall that we can also write theorem <code>t1</code> as follows:</p>
<pre><code class="language-lean">theorem t1 {p q : Prop} (hp : p) (hq : q) : p := hp

#print t1
</code></pre>
<p>The type of <code>t1</code> is now <code>∀ {p q : Prop}, p → q → p</code>. We can read
this as the assertion &quot;for every pair of propositions <code>p q</code>, we have
<code>p → q → p</code>.&quot; For example, we can move all parameters to the right
of the colon:</p>
<pre><code class="language-lean">theorem t1 : ∀ {p q : Prop}, p → q → p :=
  fun {p q : Prop} (hp : p) (hq : q) =&gt; hp
</code></pre>
<p>If <code>p</code> and <code>q</code> have been declared as variables, Lean will
generalize them for us automatically:</p>
<pre><code class="language-lean">variable {p q : Prop}

theorem t1 : p → q → p := fun (hp : p) (hq : q) =&gt; hp
</code></pre>
<p>In fact, by the propositions-as-types correspondence, we can declare
the assumption <code>hp</code> that <code>p</code> holds, as another variable:</p>
<pre><code class="language-lean">variable {p q : Prop}
variable (hp : p)

theorem t1 : q → p := fun (hq : q) =&gt; hp
</code></pre>
<p>Lean detects that the proof uses <code>hp</code> and automatically adds
<code>hp : p</code> as a premise. In all cases, the command <code>#print t1</code> still yields
<code>∀ p q : Prop, p → q → p</code>. Remember that this type can just as well
be written <code>∀ (p q : Prop) (hp : p) (hq : q), p</code>, since the arrow
denotes nothing more than an arrow type in which the target does not
depend on the bound variable.</p>
<p>When we generalize <code>t1</code> in such a way, we can then apply it to
different pairs of propositions, to obtain different instances of the
general theorem.</p>
<pre><code class="language-lean">theorem t1 (p q : Prop) (hp : p) (hq : q) : p := hp

variable (p q r s : Prop)

#check t1 p q                -- p → q → p
#check t1 r s                -- r → s → r
#check t1 (r → s) (s → r)    -- (r → s) → (s → r) → r → s

variable (h : r → s)
#check t1 (r → s) (s → r) h  -- (s → r) → r → s
</code></pre>
<p>Once again, using the propositions-as-types correspondence, the
variable <code>h</code> of type <code>r → s</code> can be viewed as the hypothesis, or
premise, that <code>r → s</code> holds.</p>
<p>As another example, let us consider the composition function discussed
in the last chapter, now with propositions instead of types.</p>
<pre><code class="language-lean">variable (p q r s : Prop)

theorem t2 (h₁ : q → r) (h₂ : p → q) : p → r :=
  fun h₃ : p =&gt;
  show r from h₁ (h₂ h₃)
</code></pre>
<p>As a theorem of propositional logic, what does <code>t2</code> say?</p>
<p>Note that it is often useful to use numeric unicode subscripts,
entered as <code>\0</code>, <code>\1</code>, <code>\2</code>, ..., for hypotheses, as we did in
this example.</p>
<h2><a class="header" href="#propositional-logic" id="propositional-logic">Propositional Logic</a></h2>
<p>Lean defines all the standard logical connectives and notation. The propositional connectives come with the following notation:</p>
<table><thead><tr><th>Ascii</th><th>Unicode</th><th>Editor shortcut</th><th>Definition</th></tr></thead><tbody>
<tr><td>True</td><td></td><td></td><td>True</td></tr>
<tr><td>False</td><td></td><td></td><td>False</td></tr>
<tr><td>Not</td><td>¬</td><td><code>\not</code>, <code>\neg</code></td><td>Not</td></tr>
<tr><td>/\</td><td>∧</td><td><code>\and</code></td><td>And</td></tr>
<tr><td>\/</td><td>∨</td><td><code>\or</code></td><td>Or</td></tr>
<tr><td>-&gt;</td><td>→</td><td><code>\to</code>, <code>\r</code>, <code>\imp</code></td><td></td></tr>
<tr><td>&lt;-&gt;</td><td>↔</td><td><code>\iff</code>, <code>\lr</code></td><td>Iff</td></tr>
</tbody></table>
<p>They all take values in <code>Prop</code>.</p>
<pre><code class="language-lean">variable (p q : Prop)

#check p → q → p ∧ q
#check ¬p → p ↔ False
#check p ∨ q → q ∨ p
</code></pre>
<p>The order of operations is as follows: unary negation <code>¬</code> binds most
strongly, then <code>∧</code>, then <code>∨</code>, then <code>→</code>, and finally <code>↔</code>. For
example, <code>a ∧ b → c ∨ d ∧ e</code> means <code>(a ∧ b) → (c ∨ (d ∧ e))</code>. Remember that <code>→</code> associates to the right (nothing changes
now that the arguments are elements of <code>Prop</code>, instead of some other
<code>Type</code>), as do the other binary connectives. So if we have
<code>p q r : Prop</code>, the expression <code>p → q → r</code> reads &quot;if <code>p</code>, then if <code>q</code>,
then <code>r</code>.&quot; This is just the &quot;curried&quot; form of <code>p ∧ q → r</code>.</p>
<p>In the last chapter we observed that lambda abstraction can be viewed
as an &quot;introduction rule&quot; for <code>→</code>. In the current setting, it shows
how to &quot;introduce&quot; or establish an implication. Application can be
viewed as an &quot;elimination rule,&quot; showing how to &quot;eliminate&quot; or use an
implication in a proof. The other propositional connectives are
defined in Lean's library in the file <code>Prelude.core</code> (see
<a href="./interacting_with_lean.html#importing-files">importing files</a> for more information on the library
hierarchy), and each connective comes with its canonical introduction
and elimination rules.</p>
<h3><a class="header" href="#conjunction" id="conjunction">Conjunction</a></h3>
<p>The expression <code>And.intro h1 h2</code> builds a proof of <code>p ∧ q</code> using
proofs <code>h1 : p</code> and <code>h2 : q</code>. It is common to describe
<code>And.intro</code> as the <em>and-introduction</em> rule. In the next example we
use <code>And.intro</code> to create a proof of <code>p → q → p ∧ q</code>.</p>
<pre><code class="language-lean">variable (p q : Prop)

example (hp : p) (hq : q) : p ∧ q := And.intro hp hq

#check fun (hp : p) (hq : q) =&gt; And.intro hp hq
</code></pre>
<p>The <code>example</code> command states a theorem without naming it or storing
it in the permanent context. Essentially, it just checks that the
given term has the indicated type. It is convenient for illustration,
and we will use it often.</p>
<p>The expression <code>And.left h</code> creates a proof of <code>p</code> from a proof
<code>h : p ∧ q</code>. Similarly, <code>And.right h</code> is a proof of <code>q</code>. They
are commonly known as the left and right <em>and-elimination</em> rules.</p>
<pre><code class="language-lean">variable (p q : Prop)

example (h : p ∧ q) : p := And.left h
example (h : p ∧ q) : q := And.right h
</code></pre>
<p>We can now prove <code>p ∧ q → q ∧ p</code> with the following proof term.</p>
<pre><code class="language-lean">variable (p q : Prop)

example (h : p ∧ q) : q ∧ p :=
  And.intro (And.right h) (And.left h)
</code></pre>
<p>Notice that and-introduction and and-elimination are similar to the
pairing and projection operations for the Cartesian product. The
difference is that given <code>hp : p</code> and <code>hq : q</code>, <code>And.intro hp hq</code> has type <code>p ∧ q : Prop</code>, while <code>Prod hp hq</code> has type
<code>p × q : Type</code>. The similarity between <code>∧</code> and <code>×</code> is another instance
of the Curry-Howard isomorphism, but in contrast to implication and
the function space constructor, <code>∧</code> and <code>×</code> are treated separately
in Lean. With the analogy, however, the proof we have just constructed
is similar to a function that swaps the elements of a pair.</p>
<p>We will see in <a href="./structures_and_records.html">Chapter Structures and Records</a> that certain
types in Lean are <em>structures</em>, which is to say, the type is defined
with a single canonical <em>constructor</em> which builds an element of the
type from a sequence of suitable arguments. For every <code>p q : Prop</code>,
<code>p ∧ q</code> is an example: the canonical way to construct an element is
to apply <code>And.intro</code> to suitable arguments <code>hp : p</code> and
<code>hq : q</code>. Lean allows us to use <em>anonymous constructor</em> notation
<code>⟨arg1, arg2, ...⟩</code> in situations like these, when the relevant type is an
inductive type and can be inferred from the context. In particular, we
can often write <code>⟨hp, hq⟩</code> instead of <code>And.intro hp hq</code>:</p>
<pre><code class="language-lean">variable (p q : Prop)
variable (hp : p) (hq : q)

#check (⟨hp, hq⟩ : p ∧ q)
</code></pre>
<p>These angle brackets are obtained by typing <code>\&lt;</code> and <code>\&gt;</code>, respectively.</p>
<p>Lean provides another useful syntactic gadget. Given an expression
<code>e</code> of an inductive type <code>Foo</code> (possibly applied to some
arguments), the notation <code>e.bar</code> is shorthand for <code>Foo.bar e</code>.
This provides a convenient way of accessing functions without opening
a namespace.  For example, the following two expressions mean the same
thing:</p>
<pre><code class="language-lean">variable (xs : List Nat)

#check List.length xs
#check xs.length
</code></pre>
<p>As a result, given <code>h : p ∧ q</code>, we can write <code>h.left</code> for
<code>And.left h</code> and <code>h.right</code> for <code>And.right h</code>. We can therefore
rewrite the sample proof above conveniently as follows:</p>
<pre><code class="language-lean">variable (p q : Prop)

example (h : p ∧ q) : q ∧ p :=
  ⟨h.right, h.left⟩
</code></pre>
<p>There is a fine line between brevity and obfuscation, and omitting
information in this way can sometimes make a proof harder to read. But
for straightforward constructions like the one above, when the type of
<code>h</code> and the goal of the construction are salient, the notation is
clean and effective.</p>
<p>It is common to iterate constructions like &quot;And.&quot; Lean also allows you
to flatten nested constructors that associate to the right, so that
these two proofs are equivalent:</p>
<pre><code class="language-lean">variable (p q : Prop)

example (h : p ∧ q) : q ∧ p ∧ q :=
  ⟨h.right, ⟨h.left, h.right⟩⟩

example (h : p ∧ q) : q ∧ p ∧ q :=
  ⟨h.right, h.left, h.right⟩
</code></pre>
<p>This is often useful as well.</p>
<h3><a class="header" href="#disjunction" id="disjunction">Disjunction</a></h3>
<p>The expression <code>Or.intro_left q hp</code> creates a proof of <code>p ∨ q</code>
from a proof <code>hp : p</code>. Similarly, <code>Or.intro_right p hq</code> creates a
proof for <code>p ∨ q</code> using a proof <code>hq : q</code>. These are the left and
right <em>or-introduction</em> rules.</p>
<pre><code class="language-lean">variable (p q : Prop)
example (hp : p) : p ∨ q := Or.intro_left q hp
example (hq : q) : p ∨ q := Or.intro_right p hq
</code></pre>
<p>The <em>or-elimination</em> rule is slightly more complicated. The idea is
that we can prove <code>r</code> from <code>p ∨ q</code>, by showing that <code>r</code> follows
from <code>p</code> and that <code>r</code> follows from <code>q</code>.  In other words, it is a
proof by cases. In the expression <code>Or.elim hpq hpr hqr</code>, <code>Or.elim</code>
takes three arguments, <code>hpq : p ∨ q</code>, <code>hpr : p → r</code> and
<code>hqr : q → r</code>, and produces a proof of <code>r</code>. In the following example, we use
<code>Or.elim</code> to prove <code>p ∨ q → q ∨ p</code>.</p>
<pre><code class="language-lean">variable (p q r : Prop)

example (h : p ∨ q) : q ∨ p :=
  Or.elim h
    (fun hp : p =&gt;
      show q ∨ p from Or.intro_right q hp)
    (fun hq : q =&gt;
      show q ∨ p from Or.intro_left p hq)
</code></pre>
<p>In most cases, the first argument of <code>Or.intro_right</code> and
<code>Or.intro_left</code> can be inferred automatically by Lean. Lean
therefore provides <code>Or.inr</code> and <code>Or.inl</code> which can be viewed as
shorthand for <code>Or.intro_right _</code> and <code>Or.intro_left _</code>. Thus the
proof term above could be written more concisely:</p>
<pre><code class="language-lean">variable (p q r : Prop)

example (h : p ∨ q) : q ∨ p :=
  Or.elim h (fun hp =&gt; Or.inr hp) (fun hq =&gt; Or.inl hq)
</code></pre>
<p>Notice that there is enough information in the full expression for
Lean to infer the types of <code>hp</code> and <code>hq</code> as well.  But using the
type annotations in the longer version makes the proof more readable,
and can help catch and debug errors.</p>
<p>Because <code>Or</code> has two constructors, we cannot use anonymous
constructor notation. But we can still write <code>h.elim</code> instead of
<code>Or.elim h</code>:</p>
<pre><code class="language-lean">variable (p q r : Prop)

example (h : p ∨ q) : q ∨ p :=
  h.elim (fun hp =&gt; Or.inr hp) (fun hq =&gt; Or.inl hq)
</code></pre>
<p>Once again, you should exercise judgment as to whether such
abbreviations enhance or diminish readability.</p>
<h3><a class="header" href="#negation-and-falsity" id="negation-and-falsity">Negation and Falsity</a></h3>
<p>Negation, <code>¬p</code>, is actually defined to be <code>p → False</code>, so we
obtain <code>¬p</code> by deriving a contradiction from <code>p</code>. Similarly, the
expression <code>hnp hp</code> produces a proof of <code>False</code> from <code>hp : p</code>
and <code>hnp : ¬p</code>. The next example uses both these rules to produce a
proof of <code>(p → q) → ¬q → ¬p</code>. (The symbol <code>¬</code> is produced by
typing <code>\not</code> or <code>\neg</code>.)</p>
<pre><code class="language-lean">variable (p q : Prop)

example (hpq : p → q) (hnq : ¬q) : ¬p :=
  fun hp : p =&gt;
  show False from hnq (hpq hp)
</code></pre>
<p>The connective <code>False</code> has a single elimination rule,
<code>False.elim</code>, which expresses the fact that anything follows from a
contradiction. This rule is sometimes called <em>ex falso</em> (short for <em>ex
falso sequitur quodlibet</em>), or the <em>principle of explosion</em>.</p>
<pre><code class="language-lean">variable (p q : Prop)

example (hp : p) (hnp : ¬p) : q := False.elim (hnp hp)
</code></pre>
<p>The arbitrary fact, <code>q</code>, that follows from falsity is an implicit
argument in <code>False.elim</code> and is inferred automatically. This
pattern, deriving an arbitrary fact from contradictory hypotheses, is
quite common, and is represented by <code>absurd</code>.</p>
<pre><code class="language-lean">variable (p q : Prop)

example (hp : p) (hnp : ¬p) : q := absurd hp hnp
</code></pre>
<p>Here, for example, is a proof of <code>¬p → q → (q → p) → r</code>:</p>
<pre><code class="language-lean">variable (p q r : Prop)

example (hnp : ¬p) (hq : q) (hqp : q → p) : r :=
  absurd (hqp hq) hnp
</code></pre>
<p>Incidentally, just as <code>False</code> has only an elimination rule, <code>True</code>
has only an introduction rule, <code>True.intro : true</code>.  In other words,
<code>True</code> is simply true, and has a canonical proof, <code>True.intro</code>.</p>
<h3><a class="header" href="#logical-equivalence" id="logical-equivalence">Logical Equivalence</a></h3>
<p>The expression <code>Iff.intro h1 h2</code> produces a proof of <code>p ↔ q</code> from
<code>h1 : p → q</code> and <code>h2 : q → p</code>.  The expression <code>Iff.mp h</code>
produces a proof of <code>p → q</code> from <code>h : p ↔ q</code>. Similarly,
<code>Iff.mpr h</code> produces a proof of <code>q → p</code> from <code>h : p ↔ q</code>. Here is a proof
of <code>p ∧ q ↔ q ∧ p</code>:</p>
<pre><code class="language-lean">variable (p q : Prop)

theorem and_swap : p ∧ q ↔ q ∧ p :=
  Iff.intro
    (fun h : p ∧ q =&gt;
     show q ∧ p from And.intro (And.right h) (And.left h))
    (fun h : q ∧ p =&gt;
     show p ∧ q from And.intro (And.right h) (And.left h))

#check and_swap p q    -- p ∧ q ↔ q ∧ p

variable (h : p ∧ q)
example : q ∧ p := Iff.mp (and_swap p q) h
</code></pre>
<p>We can use the anonymous constructor notation to construct a proof of
<code>p ↔ q</code> from proofs of the forward and backward directions, and we
can also use <code>.</code> notation with <code>mp</code> and <code>mpr</code>. The previous
examples can therefore be written concisely as follows:</p>
<pre><code class="language-lean">variable (p q : Prop)

theorem and_swap : p ∧ q ↔ q ∧ p :=
  ⟨ fun h =&gt; ⟨h.right, h.left⟩, fun h =&gt; ⟨h.right, h.left⟩ ⟩

example (h : p ∧ q) : q ∧ p := (and_swap p q).mp h
</code></pre>
<h2><a class="header" href="#introducing-auxiliary-subgoals" id="introducing-auxiliary-subgoals">Introducing Auxiliary Subgoals</a></h2>
<p>This is a good place to introduce another device Lean offers to help
structure long proofs, namely, the <code>have</code> construct, which
introduces an auxiliary subgoal in a proof. Here is a small example,
adapted from the last section:</p>
<pre><code class="language-lean">variable (p q : Prop)

example (h : p ∧ q) : q ∧ p :=
  have hp : p := h.left
  have hq : q := h.right
  show q ∧ p from And.intro hq hp
</code></pre>
<p>Internally, the expression <code>have h : p := s; t</code> produces the term
<code>(fun (h : p) =&gt; t) s</code>. In other words, <code>s</code> is a proof of <code>p</code>,
<code>t</code> is a proof of the desired conclusion assuming <code>h : p</code>, and the
two are combined by a lambda abstraction and application. This simple
device is extremely useful when it comes to structuring long proofs,
since we can use intermediate <code>have</code>'s as stepping stones leading to
the final goal.</p>
<p>Lean also supports a structured way of reasoning backwards from a
goal, which models the &quot;suffices to show&quot; construction in ordinary
mathematics. The next example simply permutes the last two lines in
the previous proof.</p>
<pre><code class="language-lean">variable (p q : Prop)

example (h : p ∧ q) : q ∧ p :=
  have hp : p := h.left
  suffices hq : q from And.intro hq hp
  show q from And.right h
</code></pre>
<p>Writing <code>suffices hq : q</code> leaves us with two goals. First, we have
to show that it indeed suffices to show <code>q</code>, by proving the original
goal of <code>q ∧ p</code> with the additional hypothesis <code>hq : q</code>. Finally,
we have to show <code>q</code>.</p>
<h2><a class="header" href="#classical-logic" id="classical-logic">Classical Logic</a></h2>
<p>The introduction and elimination rules we have seen so far are all
constructive, which is to say, they reflect a computational
understanding of the logical connectives based on the
propositions-as-types correspondence. Ordinary classical logic adds to
this the law of the excluded middle, <code>p ∨ ¬p</code>. To use this
principle, you have to open the classical namespace.</p>
<pre><code class="language-lean">open Classical

variable (p : Prop)
#check em p
</code></pre>
<p>Intuitively, the constructive &quot;Or&quot; is very strong: asserting <code>p ∨ q</code>
amounts to knowing which is the case. If <code>RH</code> represents the Riemann
hypothesis, a classical mathematician is willing to assert
<code>RH ∨ ¬RH</code>, even though we cannot yet assert either disjunct.</p>
<p>One consequence of the law of the excluded middle is the principle of
double-negation elimination:</p>
<pre><code class="language-lean">open Classical

theorem dne {p : Prop} (h : ¬¬p) : p :=
  Or.elim (em p)
    (fun hp : p =&gt; hp)
    (fun hnp : ¬p =&gt; absurd hnp h)
</code></pre>
<p>Double-negation elimination allows one to prove any proposition,
<code>p</code>, by assuming <code>¬p</code> and deriving <code>false</code>, because that amounts
to proving <code>¬¬p</code>. In other words, double-negation elimination allows
one to carry out a proof by contradiction, something which is not
generally possible in constructive logic. As an exercise, you might
try proving the converse, that is, showing that <code>em</code> can be proved
from <code>dne</code>.</p>
<p>The classical axioms also give you access to additional patterns of
proof that can be justified by appeal to <code>em</code>.  For example, one can
carry out a proof by cases:</p>
<pre><code class="language-lean">open Classical
variable (p : Prop)

example (h : ¬¬p) : p :=
  byCases
    (fun h1 : p =&gt; h1)
    (fun h1 : ¬p =&gt; absurd h1 h)
</code></pre>
<p>Or you can carry out a proof by contradiction:</p>
<pre><code class="language-lean">open Classical
variable (p : Prop)

example (h : ¬¬p) : p :=
  byContradiction
    (fun h1 : ¬p =&gt;
     show False from h h1)
</code></pre>
<p>If you are not used to thinking constructively, it may take some time
for you to get a sense of where classical reasoning is used.  It is
needed in the following example because, from a constructive
standpoint, knowing that <code>p</code> and <code>q</code> are not both true does not
necessarily tell you which one is false:</p>
<pre><code class="language-lean"><span class="boring">open Classical
</span><span class="boring">variable (p q : Prop)
</span>example (h : ¬(p ∧ q)) : ¬p ∨ ¬q :=
  Or.elim (em p)
    (fun hp : p =&gt;
      Or.inr
        (show ¬q from
          fun hq : q =&gt;
          h ⟨hp, hq⟩))
    (fun hp : ¬p =&gt;
      Or.inl hp)
</code></pre>
<p>We will see later that there <em>are</em> situations in constructive logic
where principles like excluded middle and double-negation elimination
are permissible, and Lean supports the use of classical reasoning in
such contexts without relying on excluded middle.</p>
<p>The full list of axioms that are used in Lean to support classical
reasoning are discussed in <a href="./axioms_and_computation.html">Axioms and Computation</a>.</p>
<h2><a class="header" href="#examples-of-propositional-validities" id="examples-of-propositional-validities">Examples of Propositional Validities</a></h2>
<p>Lean's standard library contains proofs of many valid statements of
propositional logic, all of which you are free to use in proofs of
your own. The following list includes a number of common identities.</p>
<p>Commutativity:</p>
<ol>
<li><code>p ∧ q ↔ q ∧ p</code></li>
<li><code>p ∨ q ↔ q ∨ p</code></li>
</ol>
<p>Associativity:</p>
<ol start="3">
<li><code>(p ∧ q) ∧ r ↔ p ∧ (q ∧ r)</code></li>
<li><code>(p ∨ q) ∨ r ↔ p ∨ (q ∨ r)</code></li>
</ol>
<p>Distributivity:</p>
<ol start="5">
<li><code>p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r)</code></li>
<li><code>p ∨ (q ∧ r) ↔ (p ∨ q) ∧ (p ∨ r)</code></li>
</ol>
<p>Other properties:</p>
<ol start="7">
<li><code>(p → (q → r)) ↔ (p ∧ q → r)</code></li>
<li><code>((p ∨ q) → r) ↔ (p → r) ∧ (q → r)</code></li>
<li><code>¬(p ∨ q) ↔ ¬p ∧ ¬q</code></li>
<li><code>¬p ∨ ¬q → ¬(p ∧ q)</code></li>
<li><code>¬(p ∧ ¬p)</code></li>
<li><code>p ∧ ¬q → ¬(p → q)</code></li>
<li><code>¬p → (p → q)</code></li>
<li><code>(¬p ∨ q) → (p → q)</code></li>
<li><code>p ∨ False ↔ p</code></li>
<li><code>p ∧ False ↔ False</code></li>
<li><code>¬(p ↔ ¬p)</code></li>
<li><code>(p → q) → (¬q → ¬p)</code></li>
</ol>
<p>These require classical reasoning:</p>
<ol start="19">
<li><code>(p → r ∨ s) → ((p → r) ∨ (p → s))</code></li>
<li><code>¬(p ∧ q) → ¬p ∨ ¬q</code></li>
<li><code>¬(p → q) → p ∧ ¬q</code></li>
<li><code>(p → q) → (¬p ∨ q)</code></li>
<li><code>(¬q → ¬p) → (p → q)</code></li>
<li><code>p ∨ ¬p</code></li>
<li><code>(((p → q) → p) → p)</code></li>
</ol>
<p>The <code>sorry</code> identifier magically produces a proof of anything, or
provides an object of any data type at all. Of course, it is unsound
as a proof method -- for example, you can use it to prove <code>False</code> --
and Lean produces severe warnings when files use or import theorems
which depend on it. But it is very useful for building long proofs
incrementally. Start writing the proof from the top down, using
<code>sorry</code> to fill in subproofs. Make sure Lean accepts the term with
all the <code>sorry</code>'s; if not, there are errors that you need to
correct. Then go back and replace each <code>sorry</code> with an actual proof,
until no more remain.</p>
<p>Here is another useful trick. Instead of using <code>sorry</code>, you can use
an underscore <code>_</code> as a placeholder. Recall this tells Lean that
the argument is implicit, and should be filled in automatically. If
Lean tries to do so and fails, it returns with an error message &quot;don't
know how to synthesize placeholder,&quot; followed by the type of
the term it is expecting, and all the objects and hypotheses available
in the context. In other words, for each unresolved placeholder, Lean
reports the subgoal that needs to be filled at that point. You can
then construct a proof by incrementally filling in these placeholders.</p>
<p>For reference, here are two sample proofs of validities taken from the
list above.</p>
<pre><code class="language-lean">open Classical

-- distributivity
example (p q r : Prop) : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) :=
  Iff.intro
    (fun h : p ∧ (q ∨ r) =&gt;
      have hp : p := h.left
      Or.elim (h.right)
        (fun hq : q =&gt;
          show (p ∧ q) ∨ (p ∧ r) from Or.inl ⟨hp, hq⟩)
        (fun hr : r =&gt;
          show (p ∧ q) ∨ (p ∧ r) from Or.inr ⟨hp, hr⟩))
    (fun h : (p ∧ q) ∨ (p ∧ r) =&gt;
      Or.elim h
        (fun hpq : p ∧ q =&gt;
          have hp : p := hpq.left
          have hq : q := hpq.right
          show p ∧ (q ∨ r) from ⟨hp, Or.inl hq⟩)
        (fun hpr : p ∧ r =&gt;
          have hp : p := hpr.left
          have hr : r := hpr.right
          show p ∧ (q ∨ r) from ⟨hp, Or.inr hr⟩))

-- an example that requires classical reasoning
example (p q : Prop) : ¬(p ∧ ¬q) → (p → q) :=
  fun h : ¬(p ∧ ¬q) =&gt;
  fun hp : p =&gt;
  show q from
    Or.elim (em q)
      (fun hq : q =&gt; hq)
      (fun hnq : ¬q =&gt; absurd (And.intro hp hnq) h)
</code></pre>
<h2><a class="header" href="#exercises" id="exercises">Exercises</a></h2>
<p>Prove the following identities, replacing the &quot;sorry&quot; placeholders with actual proofs.</p>
<pre><code class="language-lean">variable (p q r : Prop)

-- commutativity of ∧ and ∨
example : p ∧ q ↔ q ∧ p := sorry
example : p ∨ q ↔ q ∨ p := sorry

-- associativity of ∧ and ∨
example : (p ∧ q) ∧ r ↔ p ∧ (q ∧ r) := sorry
example : (p ∨ q) ∨ r ↔ p ∨ (q ∨ r) := sorry

-- distributivity
example : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) := sorry
example : p ∨ (q ∧ r) ↔ (p ∨ q) ∧ (p ∨ r) := sorry

-- other properties
example : (p → (q → r)) ↔ (p ∧ q → r) := sorry
example : ((p ∨ q) → r) ↔ (p → r) ∧ (q → r) := sorry
example : ¬(p ∨ q) ↔ ¬p ∧ ¬q := sorry
example : ¬p ∨ ¬q → ¬(p ∧ q) := sorry
example : ¬(p ∧ ¬p) := sorry
example : p ∧ ¬q → ¬(p → q) := sorry
example : ¬p → (p → q) := sorry
example : (¬p ∨ q) → (p → q) := sorry
example : p ∨ False ↔ p := sorry
example : p ∧ False ↔ False := sorry
example : (p → q) → (¬q → ¬p) := sorry
</code></pre>
<p>Prove the following identities, replacing the &quot;sorry&quot; placeholders
with actual proofs. These require classical reasoning.</p>
<pre><code class="language-lean">open Classical

variable (p q r : Prop)

example : (p → q ∨ r) → ((p → q) ∨ (p → r)) := sorry
example : ¬(p ∧ q) → ¬p ∨ ¬q := sorry
example : ¬(p → q) → p ∧ ¬q := sorry
example : (p → q) → (¬p ∨ q) := sorry
example : (¬q → ¬p) → (p → q) := sorry
example : p ∨ ¬p := sorry
example : (((p → q) → p) → p) := sorry
</code></pre>
<p>Prove <code>¬(p ↔ ¬p)</code> without using classical logic.</p>
<h1><a class="header" href="#quantifiers-and-equality" id="quantifiers-and-equality">Quantifiers and Equality</a></h1>
<p>The last chapter introduced you to methods that construct proofs of
statements involving the propositional connectives. In this chapter,
we extend the repertoire of logical constructions to include the
universal and existential quantifiers, and the equality relation.</p>
<h2><a class="header" href="#the-universal-quantifier" id="the-universal-quantifier">The Universal Quantifier</a></h2>
<p>Notice that if <code>α</code> is any type, we can represent a unary predicate
<code>p</code> on <code>α</code> as an object of type <code>α → Prop</code>. In that case, given
<code>x : α</code>, <code>p x</code> denotes the assertion that <code>p</code> holds of
<code>x</code>. Similarly, an object <code>r : α → α → Prop</code> denotes a binary
relation on <code>α</code>: given <code>x y : α</code>, <code>r x y</code> denotes the assertion
that <code>x</code> is related to <code>y</code>.</p>
<p>The universal quantifier, <code>∀ x : α, p x</code> is supposed to denote the
assertion that &quot;for every <code>x : α</code>, <code>p x</code>&quot; holds. As with the
propositional connectives, in systems of natural deduction, &quot;forall&quot;
is governed by an introduction and elimination rule. Informally, the
introduction rule states:</p>
<blockquote>
<p>Given a proof of <code>p x</code>, in a context where <code>x : α</code> is arbitrary, we obtain a proof <code>∀ x : α, p x</code>.</p>
</blockquote>
<p>The elimination rule states:</p>
<blockquote>
<p>Given a proof <code>∀ x : α, p x</code> and any term <code>t : α</code>, we obtain a proof of <code>p t</code>.</p>
</blockquote>
<p>As was the case for implication, the propositions-as-types
interpretation now comes into play. Remember the introduction and
elimination rules for dependent arrow types:</p>
<blockquote>
<p>Given a term <code>t</code> of type <code>β x</code>, in a context where <code>x : α</code> is arbitrary, we have <code>(fun x : α =&gt; t) : (x : α) → β x</code>.</p>
</blockquote>
<p>The elimination rule states:</p>
<blockquote>
<p>Given a term <code>s : (x : α) → β x</code> and any term <code>t : α</code>, we have <code>s t : β t</code>.</p>
</blockquote>
<p>In the case where <code>p x</code> has type <code>Prop</code>, if we replace
<code>(x : α) → β x</code> with <code>∀ x : α, p x</code>, we can read these as the correct rules
for building proofs involving the universal quantifier.</p>
<p>The Calculus of Constructions therefore identifies dependent arrow
types with forall-expressions in this way. If <code>p</code> is any expression,
<code>∀ x : α, p</code> is nothing more than alternative notation for
<code>(x : α) → p</code>, with the idea that the former is more natural than the latter
in cases where <code>p</code> is a proposition. Typically, the expression <code>p</code>
will depend on <code>x : α</code>. Recall that, in the case of ordinary
function spaces, we could interpret <code>α → β</code> as the special case of
<code>(x : α) → β</code> in which <code>β</code> does not depend on <code>x</code>. Similarly, we
can think of an implication <code>p → q</code> between propositions as the
special case of <code>∀ x : p, q</code> in which the expression <code>q</code> does not
depend on <code>x</code>.</p>
<p>Here is an example of how the propositions-as-types correspondence gets put into practice.</p>
<pre><code class="language-lean">example (α : Type) (p q : α → Prop) : (∀ x : α, p x ∧ q x) → ∀ y : α, p y :=
  fun h : ∀ x : α, p x ∧ q x =&gt;
  fun y : α =&gt;
  show p y from (h y).left
</code></pre>
<p>As a notational convention, we give the universal quantifier the
widest scope possible, so parentheses are needed to limit the
quantifier over <code>x</code> to the hypothesis in the example above. The
canonical way to prove <code>∀ y : α, p y</code> is to take an arbitrary <code>y</code>,
and prove <code>p y</code>. This is the introduction rule. Now, given that
<code>h</code> has type <code>∀ x : α, p x ∧ q x</code>, the expression <code>h y</code> has type
<code>p y ∧ q y</code>. This is the elimination rule. Taking the left conjunct
gives the desired conclusion, <code>p y</code>.</p>
<p>Remember that expressions which differ up to renaming of bound
variables are considered to be equivalent. So, for example, we could
have used the same variable, <code>x</code>, in both the hypothesis and
conclusion, and instantiated it by a different variable, <code>z</code>, in the
proof:</p>
<pre><code class="language-lean">example (α : Type) (p q : α → Prop) : (∀ x : α, p x ∧ q x) → ∀ x : α, p x :=
  fun h : ∀ x : α, p x ∧ q x =&gt;
  fun z : α =&gt;
  show p z from And.left (h z)
</code></pre>
<p>As another example, here is how we can express the fact that a relation, <code>r</code>, is transitive:</p>
<pre><code class="language-lean">variable (α : Type) (r : α → α → Prop)
variable (trans_r : ∀ x y z, r x y → r y z → r x z)

variable (a b c : α)
variable (hab : r a b) (hbc : r b c)

#check trans_r    -- ∀ (x y z : α), r x y → r y z → r x z
#check trans_r a b c -- r a b → r b c → r a c
#check trans_r a b c hab -- r b c → r a c
#check trans_r a b c hab hbc -- r a c
</code></pre>
<p>Think about what is going on here. When we instantiate <code>trans_r</code> at
the values <code>a b c</code>, we end up with a proof of <code>r a b → r b c → r a c</code>.
Applying this to the &quot;hypothesis&quot; <code>hab : r a b</code>, we get a proof
of the implication <code>r b c → r a c</code>. Finally, applying it to the
hypothesis <code>hbc</code> yields a proof of the conclusion <code>r a c</code>.</p>
<p>In situations like this, it can be tedious to supply the arguments
<code>a b c</code>, when they can be inferred from <code>hab hbc</code>. For that reason, it
is common to make these arguments implicit:</p>
<pre><code class="language-lean">variable (α : Type) (r : α → α → Prop)
variable (trans_r : ∀ {x y z}, r x y → r y z → r x z)

variable (a b c : α)
variable (hab : r a b) (hbc : r b c)

#check trans_r
#check trans_r hab
#check trans_r hab hbc
</code></pre>
<p>The advantage is that we can simply write <code>trans_r hab hbc</code> as a
proof of <code>r a c</code>. A disadvantage is that Lean does not have enough
information to infer the types of the arguments in the expressions
<code>trans_r</code> and <code>trans_r hab</code>. The output of the first <code>#check</code>
command is <code>r ?m.1 ?m.2 → r ?m.2 ?m.3 → r ?m.1 ?m.3</code>, indicating
that the implicit arguments are unspecified in this case.</p>
<p>Here is an example of how we can carry out elementary reasoning with an equivalence relation:</p>
<pre><code class="language-lean">variable (α : Type) (r : α → α → Prop)

variable (refl_r : ∀ x, r x x)
variable (symm_r : ∀ {x y}, r x y → r y x)
variable (trans_r : ∀ {x y z}, r x y → r y z → r x z)

example (a b c d : α) (hab : r a b) (hcb : r c b) (hcd : r c d) : r a d :=
  trans_r (trans_r hab (symm_r hcb)) hcd
</code></pre>
<p>To get used to using universal quantifiers, you should try some of the
exercises at the end of this section.</p>
<p>It is the typing rule for dependent arrow types, and the universal
quantifier in particular, that distinguishes <code>Prop</code> from other
types.  Suppose we have <code>α : Sort i</code> and <code>β : Sort j</code>, where the
expression <code>β</code> may depend on a variable <code>x : α</code>. Then
<code>(x : α) → β</code> is an element of <code>Sort (imax i j)</code>, where <code>imax i j</code> is the
maximum of <code>i</code> and <code>j</code> if <code>j</code> is not 0, and 0 otherwise.</p>
<p>The idea is as follows. If <code>j</code> is not <code>0</code>, then <code>(x : α) → β</code> is
an element of <code>Sort (max i j)</code>. In other words, the type of
dependent functions from <code>α</code> to <code>β</code> &quot;lives&quot; in the universe whose
index is the maximum of <code>i</code> and <code>j</code>. Suppose, however, that <code>β</code>
is of <code>Sort 0</code>, that is, an element of <code>Prop</code>. In that case,
<code>(x : α) → β</code> is an element of <code>Sort 0</code> as well, no matter which
type universe <code>α</code> lives in. In other words, if <code>β</code> is a
proposition depending on <code>α</code>, then <code>∀ x : α, β</code> is again a
proposition. This reflects the interpretation of <code>Prop</code> as the type
of propositions rather than data, and it is what makes <code>Prop</code>
<em>impredicative</em>.</p>
<p>The term &quot;predicative&quot; stems from foundational developments around the
turn of the twentieth century, when logicians such as Poincaré and
Russell blamed set-theoretic paradoxes on the &quot;vicious circles&quot; that
arise when we define a property by quantifying over a collection that
includes the very property being defined. Notice that if <code>α</code> is any
type, we can form the type <code>α → Prop</code> of all predicates on <code>α</code>
(the &quot;power type of <code>α</code>&quot;). The impredicativity of <code>Prop</code> means that we
can form propositions that quantify over <code>α → Prop</code>. In particular,
we can define predicates on <code>α</code> by quantifying over all predicates
on <code>α</code>, which is exactly the type of circularity that was once
considered problematic.</p>
<h2><a class="header" href="#equality" id="equality">Equality</a></h2>
<p>Let us now turn to one of the most fundamental relations defined in
Lean's library, namely, the equality relation. In <a href="inductive_types.html">Chapter Inductive Types</a>,
we will explain <em>how</em> equality is defined from the primitives of Lean's logical framework.
In the meanwhile, here we explain how to use it.</p>
<p>Of course, a fundamental property of equality is that it is an equivalence relation:</p>
<pre><code class="language-lean">#check Eq.refl    -- Eq.refl.{u_1} {α : Sort u_1} (a : α) : a = a
#check Eq.symm    -- Eq.symm.{u} {α : Sort u} {a b : α} (h : a = b) : b = a
#check Eq.trans   -- Eq.trans.{u} {α : Sort u} {a b c : α} (h₁ : a = b) (h₂ : b = c) : a = c
</code></pre>
<p>We can make the output easier to read by telling Lean not to insert
the implicit arguments (which are displayed here as metavariables).</p>
<pre><code class="language-lean">universe u

#check @Eq.refl.{u}   -- @Eq.refl : ∀ {α : Sort u} (a : α), a = a
#check @Eq.symm.{u}   -- @Eq.symm : ∀ {α : Sort u} {a b : α}, a = b → b = a
#check @Eq.trans.{u}  -- @Eq.trans : ∀ {α : Sort u} {a b c : α}, a = b → b = c → a = c
</code></pre>
<p>The inscription <code>.{u}</code> tells Lean to instantiate the constants at the universe <code>u</code>.</p>
<p>Thus, for example, we can specialize the example from the previous section to the equality relation:</p>
<pre><code class="language-lean">variable (α : Type) (a b c d : α)
variable (hab : a = b) (hcb : c = b) (hcd : c = d)

example : a = d :=
  Eq.trans (Eq.trans hab (Eq.symm hcb)) hcd
</code></pre>
<p>We can also use the projection notation:</p>
<pre><code class="language-lean"><span class="boring">variable (α : Type) (a b c d : α)
</span><span class="boring">variable (hab : a = b) (hcb : c = b) (hcd : c = d)
</span>example : a = d := (hab.trans hcb.symm).trans hcd
</code></pre>
<p>Reflexivity is more powerful than it looks. Recall that terms in the
Calculus of Constructions have a computational interpretation, and
that the logical framework treats terms with a common reduct as the
same. As a result, some nontrivial identities can be proved by
reflexivity:</p>
<pre><code class="language-lean">variable (α β : Type)

example (f : α → β) (a : α) : (fun x =&gt; f x) a = f a := Eq.refl _
example (a : α) (b : β) : (a, b).1 = a := Eq.refl _
example : 2 + 3 = 5 := Eq.refl _
</code></pre>
<p>This feature of the framework is so important that the library defines a notation <code>rfl</code> for <code>Eq.refl _</code>:</p>
<pre><code class="language-lean"><span class="boring">variable (α β : Type)
</span>example (f : α → β) (a : α) : (fun x =&gt; f x) a = f a := rfl
example (a : α) (b : β) : (a, b).1 = a := rfl
example : 2 + 3 = 5 := rfl
</code></pre>
<p>Equality is much more than an equivalence relation, however. It has
the important property that every assertion respects the equivalence,
in the sense that we can substitute equal expressions without changing
the truth value. That is, given <code>h1 : a = b</code> and <code>h2 : p a</code>, we
can construct a proof for <code>p b</code> using substitution:
<code>Eq.subst h1 h2</code>.</p>
<pre><code class="language-lean">example (α : Type) (a b : α) (p : α → Prop)
        (h1 : a = b) (h2 : p a) : p b :=
  Eq.subst h1 h2

example (α : Type) (a b : α) (p : α → Prop)
    (h1 : a = b) (h2 : p a) : p b :=
  h1 ▸ h2
</code></pre>
<p>The triangle in the second presentation is a macro built on top of
<code>Eq.subst</code> and <code>Eq.symm</code>, and you can enter it by typing <code>\t</code>.</p>
<p>The rule <code>Eq.subst</code> is used to define the following auxiliary rules,
which carry out more explicit substitutions. They are designed to deal
with applicative terms, that is, terms of form <code>s t</code>. Specifically,
<code>congrArg</code> can be used to replace the argument, <code>congrFun</code> can be
used to replace the term that is being applied, and <code>congr</code> can be
used to replace both at once.</p>
<pre><code class="language-lean">variable (α : Type)
variable (a b : α)
variable (f g : α → Nat)
variable (h₁ : a = b)
variable (h₂ : f = g)

example : f a = f b := congrArg f h₁
example : f a = g a := congrFun h₂ a
example : f a = g b := congr h₂ h₁
</code></pre>
<p>Lean's library contains a large number of common identities, such as these:</p>
<pre><code class="language-lean">variable (a b c : Nat)

example : a + 0 = a := Nat.add_zero a
example : 0 + a = a := Nat.zero_add a
example : a * 1 = a := Nat.mul_one a
example : 1 * a = a := Nat.one_mul a
example : a + b = b + a := Nat.add_comm a b
example : a + b + c = a + (b + c) := Nat.add_assoc a b c
example : a * b = b * a := Nat.mul_comm a b
example : a * b * c = a * (b * c) := Nat.mul_assoc a b c
example : a * (b + c) = a * b + a * c := Nat.mul_add a b c
example : a * (b + c) = a * b + a * c := Nat.left_distrib a b c
example : (a + b) * c = a * c + b * c := Nat.add_mul a b c
example : (a + b) * c = a * c + b * c := Nat.right_distrib a b c
</code></pre>
<p>Note that <code>Nat.mul_add</code> and <code>Nat.add_mul</code> are alternative names
for <code>Nat.left_distrib</code> and <code>Nat.right_distrib</code>, respectively.  The
properties above are stated for the natural numbers (type <code>Nat</code>).</p>
<p>Here is an example of a calculation in the natural numbers that uses
substitution combined with associativity and distributivity.</p>
<pre><code class="language-lean">example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  have h1 : (x + y) * (x + y) = (x + y) * x + (x + y) * y :=
    Nat.mul_add (x + y) x y
  have h2 : (x + y) * (x + y) = x * x + y * x + (x * y + y * y) :=
    (Nat.add_mul x y x) ▸ (Nat.add_mul x y y) ▸ h1
  h2.trans (Nat.add_assoc (x * x + y * x) (x * y) (y * y)).symm
</code></pre>
<p>Notice that the second implicit parameter to <code>Eq.subst</code>, which
provides the context in which the substitution is to occur, has type
<code>α → Prop</code>.  Inferring this predicate therefore requires an instance
of <em>higher-order unification</em>. In full generality, the problem of
determining whether a higher-order unifier exists is undecidable, and
Lean can at best provide imperfect and approximate solutions to the
problem. As a result, <code>Eq.subst</code> doesn't always do what you want it
to.  The macro <code>h ▸ e</code> uses more effective heuristics for computing
this implicit parameter, and often succeeds in situations where
applying <code>Eq.subst</code> fails.</p>
<p>Because equational reasoning is so common and important, Lean provides
a number of mechanisms to carry it out more effectively. The next
section offers syntax that allow you to write calculational proofs in
a more natural and perspicuous way. But, more importantly, equational
reasoning is supported by a term rewriter, a simplifier, and other
kinds of automation. The term rewriter and simplifier are described
briefly in the next section, and then in greater detail in the next
chapter.</p>
<h2><a class="header" href="#calculational-proofs" id="calculational-proofs">Calculational Proofs</a></h2>
<p>A calculational proof is just a chain of intermediate results that are
meant to be composed by basic principles such as the transitivity of
equality. In Lean, a calculational proof starts with the keyword
<code>calc</code>, and has the following syntax:</p>
<pre><code>calc
  &lt;expr&gt;_0  'op_1'  &lt;expr&gt;_1  ':='  &lt;proof&gt;_1
  '_'       'op_2'  &lt;expr&gt;_2  ':='  &lt;proof&gt;_2
  ...
  '_'       'op_n'  &lt;expr&gt;_n  ':='  &lt;proof&gt;_n
</code></pre>
<p>Note that the <code>calc</code> relations all have the same indentation. Each
<code>&lt;proof&gt;_i</code> is a proof for <code>&lt;expr&gt;_{i-1} op_i &lt;expr&gt;_i</code>.</p>
<p>We can also use <code>_</code> in the first relation (right after <code>&lt;expr&gt;_0</code>)
which is useful to align the sequence of relation/proof pairs:</p>
<pre><code>calc &lt;expr&gt;_0 
    '_' 'op_1' &lt;expr&gt;_1 ':=' &lt;proof&gt;_1
    '_' 'op_2' &lt;expr&gt;_2 ':=' &lt;proof&gt;_2
    ...
    '_' 'op_n' &lt;expr&gt;_n ':=' &lt;proof&gt;_n
</code></pre>
<p>Here is an example:</p>
<pre><code class="language-lean">variable (a b c d e : Nat)
variable (h1 : a = b)
variable (h2 : b = c + 1)
variable (h3 : c = d)
variable (h4 : e = 1 + d)

theorem T : a = e :=
  calc
    a = b      := h1
    _ = c + 1  := h2
    _ = d + 1  := congrArg Nat.succ h3
    _ = 1 + d  := Nat.add_comm d 1
    _ = e      := Eq.symm h4
</code></pre>
<p>This style of writing proofs is most effective when it is used in
conjunction with the <code>simp</code> and <code>rewrite</code> tactics, which are
discussed in greater detail in the next chapter. For example, using
the abbreviation <code>rw</code> for rewrite, the proof above could be written
as follows:</p>
<pre><code class="language-lean"><span class="boring">variable (a b c d e : Nat)
</span><span class="boring">variable (h1 : a = b)
</span><span class="boring">variable (h2 : b = c + 1)
</span><span class="boring">variable (h3 : c = d)
</span><span class="boring">variable (h4 : e = 1 + d)
</span>theorem T : a = e :=
  calc
    a = b      := by rw [h1]
    _ = c + 1  := by rw [h2]
    _ = d + 1  := by rw [h3]
    _ = 1 + d  := by rw [Nat.add_comm]
    _ = e      := by rw [h4]
</code></pre>
<p>Essentially, the <code>rw</code> tactic uses a given equality (which can be a
hypothesis, a theorem name, or a complex term) to &quot;rewrite&quot; the
goal. If doing so reduces the goal to an identity <code>t = t</code>, the
tactic applies reflexivity to prove it.</p>
<p>Rewrites can be applied sequentially, so that the proof above can be
shortened to this:</p>
<pre><code class="language-lean"><span class="boring">variable (a b c d e : Nat)
</span><span class="boring">variable (h1 : a = b)
</span><span class="boring">variable (h2 : b = c + 1)
</span><span class="boring">variable (h3 : c = d)
</span><span class="boring">variable (h4 : e = 1 + d)
</span>theorem T : a = e :=
  calc
    a = d + 1  := by rw [h1, h2, h3]
    _ = 1 + d  := by rw [Nat.add_comm]
    _ = e      := by rw [h4]
</code></pre>
<p>Or even this:</p>
<pre><code class="language-lean"><span class="boring">variable (a b c d e : Nat)
</span><span class="boring">variable (h1 : a = b)
</span><span class="boring">variable (h2 : b = c + 1)
</span><span class="boring">variable (h3 : c = d)
</span><span class="boring">variable (h4 : e = 1 + d)
</span>theorem T : a = e :=
  by rw [h1, h2, h3, Nat.add_comm, h4]
</code></pre>
<p>The <code>simp</code> tactic, instead, rewrites the goal by applying the given
identities repeatedly, in any order, anywhere they are applicable in a
term. It also uses other rules that have been previously declared to
the system, and applies commutativity wisely to avoid looping. As a
result, we can also prove the theorem as follows:</p>
<pre><code class="language-lean"><span class="boring">variable (a b c d e : Nat)
</span><span class="boring">variable (h1 : a = b)
</span><span class="boring">variable (h2 : b = c + 1)
</span><span class="boring">variable (h3 : c = d)
</span><span class="boring">variable (h4 : e = 1 + d)
</span>theorem T : a = e :=
  by simp [h1, h2, h3, Nat.add_comm, h4]
</code></pre>
<p>We will discuss variations of <code>rw</code> and <code>simp</code> in the next chapter.</p>
<p>The <code>calc</code> command can be configured for any relation that supports
some form of transitivity. It can even combine different relations.</p>
<pre><code class="language-lean">example (a b c d : Nat) (h1 : a = b) (h2 : b ≤ c) (h3 : c + 1 &lt; d) : a &lt; d :=
  calc
    a = b     := h1
    _ &lt; b + 1 := Nat.lt_succ_self b
    _ ≤ c + 1 := Nat.succ_le_succ h2
    _ &lt; d     := h3
</code></pre>
<p>You can &quot;teach&quot; <code>calc</code> new transitivity theorems by adding new instances
of the <code>Trans</code> type class. Type classes are introduced later, but the following
small example demonstrates how to extend the <code>calc</code> notation using new <code>Trans</code> instances.</p>
<pre><code class="language-lean">def divides (x y : Nat) : Prop :=
  ∃ k, k*x = y

def divides_trans (h₁ : divides x y) (h₂ : divides y z) : divides x z :=
  let ⟨k₁, d₁⟩ := h₁
  let ⟨k₂, d₂⟩ := h₂
  ⟨k₁ * k₂, by rw [Nat.mul_comm k₁ k₂, Nat.mul_assoc, d₁, d₂]⟩

def divides_mul (x : Nat) (k : Nat) : divides x (k*x) :=
  ⟨k, rfl⟩

instance : Trans divides divides divides where
  trans := divides_trans

example (h₁ : divides x y) (h₂ : y = z) : divides x (2*z) :=
  calc
    divides x y     := h₁
    _ = z           := h₂
    divides _ (2*z) := divides_mul ..

infix:50 &quot; ∣ &quot; =&gt; divides

example (h₁ : divides x y) (h₂ : y = z) : divides x (2*z) :=
  calc
    x ∣ y   := h₁
    _ = z   := h₂
    _ ∣ 2*z := divides_mul ..
</code></pre>
<p>The example above also makes it clear that you can use <code>calc</code> even if you
do not have an infix notation for your relation. Finally we remark that
the vertical bar <code>∣</code> in the example above is the unicode one. We use
unicode to make sure we do not overload the ASCII <code>|</code> used in the
<code>match .. with</code> expression.</p>
<p>With <code>calc</code>, we can write the proof in the last section in a more
natural and perspicuous way.</p>
<pre><code class="language-lean">example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  calc
    (x + y) * (x + y) = (x + y) * x + (x + y) * y  := by rw [Nat.mul_add]
    _ = x * x + y * x + (x + y) * y                := by rw [Nat.add_mul]
    _ = x * x + y * x + (x * y + y * y)            := by rw [Nat.add_mul]
    _ = x * x + y * x + x * y + y * y              := by rw [←Nat.add_assoc]
</code></pre>
<p>The alternative <code>calc</code> notation is worth considering here. When the
first expression is taking this much space, using <code>_</code> in the first
relation naturally aligns all relations:</p>
<pre><code class="language-lean">example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  calc (x + y) * (x + y)
    _ = (x + y) * x + (x + y) * y       := by rw [Nat.mul_add]
    _ = x * x + y * x + (x + y) * y     := by rw [Nat.add_mul]
    _ = x * x + y * x + (x * y + y * y) := by rw [Nat.add_mul]
    _ = x * x + y * x + x * y + y * y   := by rw [←Nat.add_assoc]
</code></pre>
<p>Here the left arrow before <code>Nat.add_assoc</code> tells rewrite to use the
identity in the opposite direction. (You can enter it with <code>\l</code> or
use the ascii equivalent, <code>&lt;-</code>.) If brevity is what we are after,
both <code>rw</code> and <code>simp</code> can do the job on their own:</p>
<pre><code class="language-lean">example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  by rw [Nat.mul_add, Nat.add_mul, Nat.add_mul, ←Nat.add_assoc]

example (x y : Nat) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
  by simp [Nat.mul_add, Nat.add_mul, Nat.add_assoc]
</code></pre>
<h2><a class="header" href="#the-existential-quantifier" id="the-existential-quantifier">The Existential Quantifier</a></h2>
<p>Finally, consider the existential quantifier, which can be written as
either <code>exists x : α, p x</code> or <code>∃ x : α, p x</code>.  Both versions are
actually notationally convenient abbreviations for a more long-winded
expression, <code>Exists (fun x : α =&gt; p x)</code>, defined in Lean's library.</p>
<p>As you should by now expect, the library includes both an introduction
rule and an elimination rule. The introduction rule is
straightforward: to prove <code>∃ x : α, p x</code>, it suffices to provide a
suitable term <code>t</code> and a proof of <code>p t</code>. Here are some examples:</p>
<pre><code class="language-lean">example : ∃ x : Nat, x &gt; 0 :=
  have h : 1 &gt; 0 := Nat.zero_lt_succ 0
  Exists.intro 1 h

example (x : Nat) (h : x &gt; 0) : ∃ y, y &lt; x :=
  Exists.intro 0 h

example (x y z : Nat) (hxy : x &lt; y) (hyz : y &lt; z) : ∃ w, x &lt; w ∧ w &lt; z :=
  Exists.intro y (And.intro hxy hyz)

#check @Exists.intro -- ∀ {α : Sort u_1} {p : α → Prop} (w : α), p w → Exists p
</code></pre>
<p>We can use the anonymous constructor notation <code>⟨t, h⟩</code> for
<code>Exists.intro t h</code>, when the type is clear from the context.</p>
<pre><code class="language-lean">example : ∃ x : Nat, x &gt; 0 :=
  have h : 1 &gt; 0 := Nat.zero_lt_succ 0
  ⟨1, h⟩

example (x : Nat) (h : x &gt; 0) : ∃ y, y &lt; x :=
  ⟨0, h⟩

example (x y z : Nat) (hxy : x &lt; y) (hyz : y &lt; z) : ∃ w, x &lt; w ∧ w &lt; z :=
  ⟨y, hxy, hyz⟩
</code></pre>
<p>Note that <code>Exists.intro</code> has implicit arguments: Lean has to infer
the predicate <code>p : α → Prop</code> in the conclusion <code>∃ x, p x</code>.  This
is not a trivial affair. For example, if we have
<code>hg : g 0 0 = 0</code> and write <code>Exists.intro 0 hg</code>, there are many possible values
for the predicate <code>p</code>, corresponding to the theorems <code>∃ x, g x x = x</code>,
<code>∃ x, g x x = 0</code>, <code>∃ x, g x 0 = x</code>, etc. Lean uses the
context to infer which one is appropriate. This is illustrated in the
following example, in which we set the option <code>pp.explicit</code> to true
to ask Lean's pretty-printer to show the implicit arguments.</p>
<pre><code class="language-lean">variable (g : Nat → Nat → Nat)
variable (hg : g 0 0 = 0)

theorem gex1 : ∃ x, g x x = x := ⟨0, hg⟩
theorem gex2 : ∃ x, g x 0 = x := ⟨0, hg⟩
theorem gex3 : ∃ x, g 0 0 = x := ⟨0, hg⟩
theorem gex4 : ∃ x, g x x = 0 := ⟨0, hg⟩

set_option pp.explicit true  -- display implicit arguments
#print gex1
#print gex2
#print gex3
#print gex4
</code></pre>
<p>We can view <code>Exists.intro</code> as an information-hiding operation, since
it hides the witness to the body of the assertion. The existential
elimination rule, <code>Exists.elim</code>, performs the opposite operation. It
allows us to prove a proposition <code>q</code> from <code>∃ x : α, p x</code>, by
showing that <code>q</code> follows from <code>p w</code> for an arbitrary value
<code>w</code>. Roughly speaking, since we know there is an <code>x</code> satisfying
<code>p x</code>, we can give it a name, say, <code>w</code>. If <code>q</code> does not mention
<code>w</code>, then showing that <code>q</code> follows from <code>p w</code> is tantamount to
showing that <code>q</code> follows from the existence of any such <code>x</code>. Here
is an example:</p>
<pre><code class="language-lean">variable (α : Type) (p q : α → Prop)

example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  Exists.elim h
    (fun w =&gt;
     fun hw : p w ∧ q w =&gt;
     show ∃ x, q x ∧ p x from ⟨w, hw.right, hw.left⟩)
</code></pre>
<p>It may be helpful to compare the exists-elimination rule to the
or-elimination rule: the assertion <code>∃ x : α, p x</code> can be thought of
as a big disjunction of the propositions <code>p a</code>, as <code>a</code> ranges over
all the elements of <code>α</code>. Note that the anonymous constructor
notation <code>⟨w, hw.right, hw.left⟩</code> abbreviates a nested constructor
application; we could equally well have written <code>⟨w, ⟨hw.right, hw.left⟩⟩</code>.</p>
<p>Notice that an existential proposition is very similar to a sigma
type, as described in dependent types section.  The difference is that
given <code>a : α</code> and <code>h : p a</code>, the term <code>Exists.intro a h</code> has
type <code>(∃ x : α, p x) : Prop</code> and <code>Sigma.mk a h</code> has type
<code>(Σ x : α, p x) : Type</code>. The similarity between <code>∃</code> and <code>Σ</code> is another
instance of the Curry-Howard isomorphism.</p>
<p>Lean provides a more convenient way to eliminate from an existential
quantifier with the <code>match</code> expression:</p>
<pre><code class="language-lean">variable (α : Type) (p q : α → Prop)

example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  match h with
  | ⟨w, hw⟩ =&gt; ⟨w, hw.right, hw.left⟩
</code></pre>
<p>The <code>match</code> expression is part of Lean's function definition system,
which provides convenient and expressive ways of defining complex
functions.  Once again, it is the Curry-Howard isomorphism that allows
us to co-opt this mechanism for writing proofs as well.  The <code>match</code>
statement &quot;destructs&quot; the existential assertion into the components
<code>w</code> and <code>hw</code>, which can then be used in the body of the statement
to prove the proposition. We can annotate the types used in the match
for greater clarity:</p>
<pre><code class="language-lean"><span class="boring">variable (α : Type) (p q : α → Prop)
</span>example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  match h with
  | ⟨(w : α), (hw : p w ∧ q w)⟩ =&gt; ⟨w, hw.right, hw.left⟩
</code></pre>
<p>We can even use the match statement to decompose the conjunction at the same time:</p>
<pre><code class="language-lean"><span class="boring">variable (α : Type) (p q : α → Prop)
</span>example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  match h with
  | ⟨w, hpw, hqw⟩ =&gt; ⟨w, hqw, hpw⟩
</code></pre>
<p>Lean also provides a pattern-matching <code>let</code> expression:</p>
<pre><code class="language-lean"><span class="boring">variable (α : Type) (p q : α → Prop)
</span>example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
  let ⟨w, hpw, hqw⟩ := h
  ⟨w, hqw, hpw⟩
</code></pre>
<p>This is essentially just alternative notation for the <code>match</code>
construct above. Lean will even allow us to use an implicit <code>match</code>
in the <code>fun</code> expression:</p>
<pre><code class="language-lean"><span class="boring">variable (α : Type) (p q : α → Prop)
</span>example : (∃ x, p x ∧ q x) → ∃ x, q x ∧ p x :=
  fun ⟨w, hpw, hqw⟩ =&gt; ⟨w, hqw, hpw⟩
</code></pre>
<p>We will see in <a href="./induction_and_recursion.html">Chapter Induction and Recursion</a> that all these variations are
instances of a more general pattern-matching construct.</p>
<p>In the following example, we define <code>is_even a</code> as <code>∃ b, a = 2 * b</code>,
and then we show that the sum of two even numbers is an even number.</p>
<pre><code class="language-lean">def is_even (a : Nat) := ∃ b, a = 2 * b

theorem even_plus_even (h1 : is_even a) (h2 : is_even b) : is_even (a + b) :=
  Exists.elim h1 (fun w1 (hw1 : a = 2 * w1) =&gt;
  Exists.elim h2 (fun w2 (hw2 : b = 2 * w2) =&gt;
    Exists.intro (w1 + w2)
      (calc a + b
        _ = 2 * w1 + 2 * w2 := by rw [hw1, hw2]
        _ = 2 * (w1 + w2)   := by rw [Nat.mul_add])))
</code></pre>
<p>Using the various gadgets described in this chapter --- the match
statement, anonymous constructors, and the <code>rewrite</code> tactic, we can
write this proof concisely as follows:</p>
<pre><code class="language-lean"><span class="boring">def is_even (a : Nat) := ∃ b, a = 2 * b
</span>theorem even_plus_even (h1 : is_even a) (h2 : is_even b) : is_even (a + b) :=
  match h1, h2 with
  | ⟨w1, hw1⟩, ⟨w2, hw2⟩ =&gt; ⟨w1 + w2, by rw [hw1, hw2, Nat.mul_add]⟩
</code></pre>
<p>Just as the constructive &quot;or&quot; is stronger than the classical &quot;or,&quot; so,
too, is the constructive &quot;exists&quot; stronger than the classical
&quot;exists&quot;. For example, the following implication requires classical
reasoning because, from a constructive standpoint, knowing that it is
not the case that every <code>x</code> satisfies <code>¬ p</code> is not the same as
having a particular <code>x</code> that satisfies <code>p</code>.</p>
<pre><code class="language-lean">open Classical
variable (p : α → Prop)

example (h : ¬ ∀ x, ¬ p x) : ∃ x, p x :=
  byContradiction
    (fun h1 : ¬ ∃ x, p x =&gt;
      have h2 : ∀ x, ¬ p x :=
        fun x =&gt;
        fun h3 : p x =&gt;
        have h4 : ∃ x, p x := ⟨x, h3⟩
        show False from h1 h4
      show False from h h2)
</code></pre>
<p>What follows are some common identities involving the existential
quantifier. In the exercises below, we encourage you to prove as many
as you can. We also leave it to you to determine which are
nonconstructive, and hence require some form of classical reasoning.</p>
<pre><code class="language-lean">open Classical

variable (α : Type) (p q : α → Prop)
variable (r : Prop)

example : (∃ x : α, r) → r := sorry
example (a : α) : r → (∃ x : α, r) := sorry
example : (∃ x, p x ∧ r) ↔ (∃ x, p x) ∧ r := sorry
example : (∃ x, p x ∨ q x) ↔ (∃ x, p x) ∨ (∃ x, q x) := sorry

example : (∀ x, p x) ↔ ¬ (∃ x, ¬ p x) := sorry
example : (∃ x, p x) ↔ ¬ (∀ x, ¬ p x) := sorry
example : (¬ ∃ x, p x) ↔ (∀ x, ¬ p x) := sorry
example : (¬ ∀ x, p x) ↔ (∃ x, ¬ p x) := sorry

example : (∀ x, p x → r) ↔ (∃ x, p x) → r := sorry
example (a : α) : (∃ x, p x → r) ↔ (∀ x, p x) → r := sorry
example (a : α) : (∃ x, r → p x) ↔ (r → ∃ x, p x) := sorry
</code></pre>
<p>Notice that the second example and the last two examples require the
assumption that there is at least one element <code>a</code> of type <code>α</code>.</p>
<p>Here are solutions to two of the more difficult ones:</p>
<pre><code class="language-lean">open Classical

variable (α : Type) (p q : α → Prop)
variable (a : α)
variable (r : Prop)

example : (∃ x, p x ∨ q x) ↔ (∃ x, p x) ∨ (∃ x, q x) :=
  Iff.intro
    (fun ⟨a, (h1 : p a ∨ q a)⟩ =&gt;
      Or.elim h1
        (fun hpa : p a =&gt; Or.inl ⟨a, hpa⟩)
        (fun hqa : q a =&gt; Or.inr ⟨a, hqa⟩))
    (fun h : (∃ x, p x) ∨ (∃ x, q x) =&gt;
      Or.elim h
        (fun ⟨a, hpa⟩ =&gt; ⟨a, (Or.inl hpa)⟩)
        (fun ⟨a, hqa⟩ =&gt; ⟨a, (Or.inr hqa)⟩))

example : (∃ x, p x → r) ↔ (∀ x, p x) → r :=
  Iff.intro
    (fun ⟨b, (hb : p b → r)⟩ =&gt;
     fun h2 : ∀ x, p x =&gt;
     show r from hb (h2 b))
    (fun h1 : (∀ x, p x) → r =&gt;
     show ∃ x, p x → r from
       byCases
         (fun hap : ∀ x, p x =&gt; ⟨a, λ h' =&gt; h1 hap⟩)
         (fun hnap : ¬ ∀ x, p x =&gt;
          byContradiction
            (fun hnex : ¬ ∃ x, p x → r =&gt;
              have hap : ∀ x, p x :=
                fun x =&gt;
                byContradiction
                  (fun hnp : ¬ p x =&gt;
                    have hex : ∃ x, p x → r := ⟨x, (fun hp =&gt; absurd hp hnp)⟩
                    show False from hnex hex)
              show False from hnap hap)))
</code></pre>
<h2><a class="header" href="#more-on-the-proof-language" id="more-on-the-proof-language">More on the Proof Language</a></h2>
<p>We have seen that keywords like <code>fun</code>, <code>have</code>, and <code>show</code> make
it possible to write formal proof terms that mirror the structure of
informal mathematical proofs. In this section, we discuss some
additional features of the proof language that are often convenient.</p>
<p>To start with, we can use anonymous &quot;have&quot; expressions to introduce an
auxiliary goal without having to label it. We can refer to the last
expression introduced in this way using the keyword <code>this</code>:</p>
<pre><code class="language-lean">variable (f : Nat → Nat)
variable (h : ∀ x : Nat, f x ≤ f (x + 1))

example : f 0 ≤ f 3 :=
  have : f 0 ≤ f 1 := h 0
  have : f 0 ≤ f 2 := Nat.le_trans this (h 1)
  show f 0 ≤ f 3 from Nat.le_trans this (h 2)
</code></pre>
<p>Often proofs move from one fact to the next, so this can be effective
in eliminating the clutter of lots of labels.</p>
<p>When the goal can be inferred, we can also ask Lean instead to fill in
the proof by writing <code>by assumption</code>:</p>
<pre><code class="language-lean"><span class="boring">variable (f : Nat → Nat)
</span><span class="boring">variable (h : ∀ x : Nat, f x ≤ f (x + 1))
</span>example : f 0 ≤ f 3 :=
  have : f 0 ≤ f 1 := h 0
  have : f 0 ≤ f 2 := Nat.le_trans (by assumption) (h 1)
  show f 0 ≤ f 3 from Nat.le_trans (by assumption) (h 2)
</code></pre>
<p>This tells Lean to use the <code>assumption</code> tactic, which, in turn,
proves the goal by finding a suitable hypothesis in the local
context. We will learn more about the <code>assumption</code> tactic in the
next chapter.</p>
<p>We can also ask Lean to fill in the proof by writing <code>‹p›</code>, where
<code>p</code> is the proposition whose proof we want Lean to find in the
context.  You can type these corner quotes using <code>\f&lt;</code> and <code>\f&gt;</code>,
respectively. The letter &quot;f&quot; is for &quot;French,&quot; since the unicode
symbols can also be used as French quotation marks. In fact, the
notation is defined in Lean as follows:</p>
<pre><code class="language-lean">notation &quot;‹&quot; p &quot;›&quot; =&gt; show p by assumption
</code></pre>
<p>This approach is more robust than using <code>by assumption</code>, because the
type of the assumption that needs to be inferred is given
explicitly. It also makes proofs more readable. Here is a more
elaborate example:</p>
<pre><code class="language-lean">variable (f : Nat → Nat)
variable (h : ∀ x : Nat, f x ≤ f (x + 1))

example : f 0 ≥ f 1 → f 1 ≥ f 2 → f 0 = f 2 :=
  fun _ : f 0 ≥ f 1 =&gt;
  fun _ : f 1 ≥ f 2 =&gt;
  have : f 0 ≥ f 2 := Nat.le_trans ‹f 1 ≥ f 2› ‹f 0 ≥ f 1›
  have : f 0 ≤ f 2 := Nat.le_trans (h 0) (h 1)
  show f 0 = f 2 from Nat.le_antisymm this ‹f 0 ≥ f 2›
</code></pre>
<p>Keep in mind that you can use the French quotation marks in this way
to refer to <em>anything</em> in the context, not just things that were
introduced anonymously. Its use is also not limited to propositions,
though using it for data is somewhat odd:</p>
<pre><code class="language-lean">example (n : Nat) : Nat := ‹Nat›
</code></pre>
<p>Later, we show how you can extend the proof language using the Lean macro system.</p>
<h2><a class="header" href="#exercises-1" id="exercises-1">Exercises</a></h2>
<ol>
<li>Prove these equivalences:</li>
</ol>
<pre><code class="language-lean">variable (α : Type) (p q : α → Prop)

example : (∀ x, p x ∧ q x) ↔ (∀ x, p x) ∧ (∀ x, q x) := sorry
example : (∀ x, p x → q x) → (∀ x, p x) → (∀ x, q x) := sorry
example : (∀ x, p x) ∨ (∀ x, q x) → ∀ x, p x ∨ q x := sorry
</code></pre>
<p>You should also try to understand why the reverse implication is not derivable in the last example.</p>
<ol start="2">
<li>It is often possible to bring a component of a formula outside a
universal quantifier, when it does not depend on the quantified
variable. Try proving these (one direction of the second of these
requires classical logic):</li>
</ol>
<pre><code class="language-lean">variable (α : Type) (p q : α → Prop)
variable (r : Prop)

example : α → ((∀ x : α, r) ↔ r) := sorry
example : (∀ x, p x ∨ r) ↔ (∀ x, p x) ∨ r := sorry
example : (∀ x, r → p x) ↔ (r → ∀ x, p x) := sorry
</code></pre>
<ol start="3">
<li>Consider the &quot;barber paradox,&quot; that is, the claim that in a certain
town there is a (male) barber that shaves all and only the men who
do not shave themselves. Prove that this is a contradiction:</li>
</ol>
<pre><code class="language-lean">variable (men : Type) (barber : men)
variable (shaves : men → men → Prop)

example (h : ∀ x : men, shaves barber x ↔ ¬ shaves x x) : False := sorry
</code></pre>
<ol start="4">
<li>Remember that, without any parameters, an expression of type
<code>Prop</code> is just an assertion. Fill in the definitions of <code>prime</code>
and <code>Fermat_prime</code> below, and construct each of the given
assertions. For example, you can say that there are infinitely many
primes by asserting that for every natural number <code>n</code>, there is a
prime number greater than <code>n</code>. Goldbach's weak conjecture states
that every odd number greater than 5 is the sum of three
primes. Look up the definition of a Fermat prime or any of the
other statements, if necessary.</li>
</ol>
<pre><code class="language-lean">def even (n : Nat) : Prop := sorry

def prime (n : Nat) : Prop := sorry

def infinitely_many_primes : Prop := sorry

def Fermat_prime (n : Nat) : Prop := sorry

def infinitely_many_Fermat_primes : Prop := sorry

def goldbach_conjecture : Prop := sorry

def Goldbach's_weak_conjecture : Prop := sorry

def Fermat's_last_theorem : Prop := sorry
</code></pre>
<ol start="5">
<li>Prove as many of the identities listed in the Existential
Quantifier section as you can.</li>
</ol>
<h1><a class="header" href="#tactics" id="tactics">Tactics</a></h1>
<p>In this chapter, we describe an alternative approach to constructing
proofs, using <em>tactics</em>.  A proof term is a representation of a
mathematical proof; tactics are commands, or instructions, that
describe how to build such a proof. Informally, you might begin a
mathematical proof by saying &quot;to prove the forward direction, unfold
the definition, apply the previous lemma, and simplify.&quot; Just as these
are instructions that tell the reader how to find the relevant proof,
tactics are instructions that tell Lean how to construct a proof
term. They naturally support an incremental style of writing proofs,
in which you decompose a proof and work on goals one step at a time.</p>
<p>We will describe proofs that consist of sequences of tactics as
&quot;tactic-style&quot; proofs, to contrast with the ways of writing proof
terms we have seen so far, which we will call &quot;term-style&quot;
proofs. Each style has its own advantages and disadvantages. For
example, tactic-style proofs can be harder to read, because they
require the reader to predict or guess the results of each
instruction. But they can also be shorter and easier to
write. Moreover, tactics offer a gateway to using Lean's automation,
since automated procedures are themselves tactics.</p>
<h2><a class="header" href="#entering-tactic-mode" id="entering-tactic-mode">Entering Tactic Mode</a></h2>
<p>Conceptually, stating a theorem or introducing a <code>have</code> statement
creates a goal, namely, the goal of constructing a term with the
expected type. For example, the following creates the goal of
constructing a term of type <code>p ∧ q ∧ p</code>, in a context with constants
<code>p q : Prop</code>, <code>hp : p</code> and <code>hq : q</code>:</p>
<pre><code class="language-lean">theorem test (p q : Prop) (hp : p) (hq : q) : p ∧ q ∧ p :=
  sorry
</code></pre>
<p>You can write this goal as follows:</p>
<pre><code>    p : Prop, q : Prop, hp : p, hq : q ⊢ p ∧ q ∧ p
</code></pre>
<p>Indeed, if you replace the &quot;sorry&quot; by an underscore in the example
above, Lean will report that it is exactly this goal that has been
left unsolved.</p>
<p>Ordinarily, you meet such a goal by writing an explicit term. But
wherever a term is expected, Lean allows us to insert instead a <code>by &lt;tactics&gt;</code> block, where <code>&lt;tactics&gt;</code> is a sequence of commands,
separated by semicolons or line breaks. You can prove the theorem above
in that way:</p>
<pre><code class="language-lean">theorem test (p q : Prop) (hp : p) (hq : q) : p ∧ q ∧ p :=
  by apply And.intro
     exact hp
     apply And.intro
     exact hq
     exact hp
</code></pre>
<p>We often put the <code>by</code> keyword on the preceding line, and write the
example above as:</p>
<pre><code class="language-lean">theorem test (p q : Prop) (hp : p) (hq : q) : p ∧ q ∧ p := by
  apply And.intro
  exact hp
  apply And.intro
  exact hq
  exact hp
</code></pre>
<p>The <code>apply</code> tactic applies an expression, viewed as denoting a
function with zero or more arguments. It unifies the conclusion with
the expression in the current goal, and creates new goals for the
remaining arguments, provided that no later arguments depend on
them. In the example above, the command <code>apply And.intro</code> yields two
subgoals:</p>
<pre><code>    case left
    p q : Prop
    hp : p
    hq : q
    ⊢ p

    case right
    p q : Prop
    hp : p
    hq : q
    ⊢ q ∧ p
</code></pre>
<p>The first goal is met with the command <code>exact hp</code>. The <code>exact</code>
command is just a variant of <code>apply</code> which signals that the
expression given should fill the goal exactly. It is good form to use
it in a tactic proof, since its failure signals that something has
gone wrong. It is also more robust than <code>apply</code>, since the
elaborator takes the expected type, given by the target of the goal,
into account when processing the expression that is being applied. In
this case, however, <code>apply</code> would work just as well.</p>
<p>You can see the resulting proof term with the <code>#print</code> command:</p>
<pre><code class="language-lean"><span class="boring">theorem test (p q : Prop) (hp : p) (hq : q) : p ∧ q ∧ p := by
</span><span class="boring"> apply And.intro
</span><span class="boring"> exact hp
</span><span class="boring"> apply And.intro
</span><span class="boring"> exact hq
</span><span class="boring"> exact hp
</span>#print test
</code></pre>
<p>You can write a tactic script incrementally. In VS Code, you can open
a window to display messages by pressing <code>Ctrl-Shift-Enter</code>, and
that window will then show you the current goal whenever the cursor is
in a tactic block. In Emacs, you can see the goal at the end of any
line by pressing <code>C-c C-g</code>, or see the remaining goal in an
incomplete proof by putting the cursor after the first character of
the last tactic. If the proof is incomplete, the token <code>by</code> is
decorated with a red squiggly line, and the error message contains the
remaining goals.</p>
<p>Tactic commands can take compound expressions, not just single
identifiers. The following is a shorter version of the preceding
proof:</p>
<pre><code class="language-lean">theorem test (p q : Prop) (hp : p) (hq : q) : p ∧ q ∧ p := by
  apply And.intro hp
  exact And.intro hq hp
</code></pre>
<p>Unsurprisingly, it produces exactly the same proof term.</p>
<pre><code class="language-lean"><span class="boring">theorem test (p q : Prop) (hp : p) (hq : q) : p ∧ q ∧ p := by
</span><span class="boring"> apply And.intro hp
</span><span class="boring"> exact And.intro hq hp
</span>#print test
</code></pre>
<p>Multiple tactic applications can be written in a single line by concatenating with a semicolon.</p>
<pre><code class="language-lean">theorem test (p q : Prop) (hp : p) (hq : q) : p ∧ q ∧ p := by
  apply And.intro hp; exact And.intro hq hp
</code></pre>
<p>Tactics that may produce multiple subgoals often tag them. For
example, the tactic <code>apply And.intro</code> tagged the first subgoal as
<code>left</code>, and the second as <code>right</code>. In the case of the <code>apply</code>
tactic, the tags are inferred from the parameters' names used in the
<code>And.intro</code> declaration. You can structure your tactics using the
notation <code>case &lt;tag&gt; =&gt; &lt;tactics&gt;</code>. The following is a structured
version of our first tactic proof in this chapter.</p>
<pre><code class="language-lean">theorem test (p q : Prop) (hp : p) (hq : q) : p ∧ q ∧ p := by
  apply And.intro
  case left =&gt; exact hp
  case right =&gt;
    apply And.intro
    case left =&gt; exact hq
    case right =&gt; exact hp
</code></pre>
<p>You can solve the subgoal <code>right</code> before <code>left</code> using the <code>case</code>
notation:</p>
<pre><code class="language-lean">theorem test (p q : Prop) (hp : p) (hq : q) : p ∧ q ∧ p := by
  apply And.intro
  case right =&gt;
    apply And.intro
    case left =&gt; exact hq
    case right =&gt; exact hp
  case left =&gt; exact hp
</code></pre>
<p>Note that Lean hides the other goals inside the <code>case</code> block. We say
it is &quot;focusing&quot; on the selected goal.  Moreover, Lean flags an error
if the selected goal is not fully solved at the end of the <code>case</code>
block.</p>
<p>For simple subgoals, it may not be worth selecting a subgoal using its
tag, but you may still want to structure the proof. Lean also provides
the &quot;bullet&quot; notation <code>. &lt;tactics&gt;</code> (or <code>· &lt;tactics&gt;</code>) for
structuring proof.</p>
<pre><code class="language-lean">theorem test (p q : Prop) (hp : p) (hq : q) : p ∧ q ∧ p := by
  apply And.intro
  . exact hp
  . apply And.intro
    . exact hq
    . exact hp
</code></pre>
<h2><a class="header" href="#basic-tactics" id="basic-tactics">Basic Tactics</a></h2>
<p>In addition to <code>apply</code> and <code>exact</code>, another useful tactic is
<code>intro</code>, which introduces a hypothesis. What follows is an example
of an identity from propositional logic that we proved in a previous
chapter, now proved using tactics.</p>
<pre><code class="language-lean">example (p q r : Prop) : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) := by
  apply Iff.intro
  . intro h
    apply Or.elim (And.right h)
    . intro hq
      apply Or.inl
      apply And.intro
      . exact And.left h
      . exact hq
    . intro hr
      apply Or.inr
      apply And.intro
      . exact And.left h
      . exact hr
  . intro h
    apply Or.elim h
    . intro hpq
      apply And.intro
      . exact And.left hpq
      . apply Or.inl
        exact And.right hpq
    . intro hpr
      apply And.intro
      . exact And.left hpr
      . apply Or.inr
        exact And.right hpr
</code></pre>
<p>The <code>intro</code> command can more generally be used to introduce a variable of any type:</p>
<pre><code class="language-lean">example (α : Type) : α → α := by
  intro a
  exact a

example (α : Type) : ∀ x : α, x = x := by
  intro x
  exact Eq.refl x
</code></pre>
<p>You can use it to introduce several variables:</p>
<pre><code class="language-lean">example : ∀ a b c : Nat, a = b → a = c → c = b := by
  intro a b c h₁ h₂
  exact Eq.trans (Eq.symm h₂) h₁
</code></pre>
<p>As the <code>apply</code> tactic is a command for constructing function
applications interactively, the <code>intro</code> tactic is a command for
constructing function abstractions interactively (i.e., terms of the
form <code>fun x =&gt; e</code>).  As with lambda abstraction notation, the
<code>intro</code> tactic allows us to use an implicit <code>match</code>.</p>
<pre><code class="language-lean">example (α : Type) (p q : α → Prop) : (∃ x, p x ∧ q x) → ∃ x, q x ∧ p x := by
  intro ⟨w, hpw, hqw⟩
  exact ⟨w, hqw, hpw⟩
</code></pre>
<p>You can also provide multiple alternatives like in the <code>match</code> expression.</p>
<pre><code class="language-lean">example (α : Type) (p q : α → Prop) : (∃ x, p x ∨ q x) → ∃ x, q x ∨ p x := by
  intro
  | ⟨w, Or.inl h⟩ =&gt; exact ⟨w, Or.inr h⟩
  | ⟨w, Or.inr h⟩ =&gt; exact ⟨w, Or.inl h⟩
</code></pre>
<p>The <code>intros</code> tactic can be used without any arguments, in which
case, it chooses names and introduces as many variables as it can. You
will see an example of this in a moment.</p>
<p>The <code>assumption</code> tactic looks through the assumptions in context of
the current goal, and if there is one matching the conclusion, it
applies it.</p>
<pre><code class="language-lean">example (x y z w : Nat) (h₁ : x = y) (h₂ : y = z) (h₃ : z = w) : x = w := by
  apply Eq.trans h₁
  apply Eq.trans h₂
  assumption   -- applied h₃
</code></pre>
<p>It will unify metavariables in the conclusion if necessary:</p>
<pre><code class="language-lean">example (x y z w : Nat) (h₁ : x = y) (h₂ : y = z) (h₃ : z = w) : x = w := by
  apply Eq.trans
  assumption      -- solves x = ?b with h₁
  apply Eq.trans
  assumption      -- solves y = ?h₂.b with h₂
  assumption      -- solves z = w with h₃
</code></pre>
<p>The following example uses the <code>intros</code> command to introduce the three variables and two hypotheses automatically:</p>
<pre><code class="language-lean">example : ∀ a b c : Nat, a = b → a = c → c = b := by
  intros
  apply Eq.trans
  apply Eq.symm
  assumption
  assumption
</code></pre>
<p>Note that names automatically generated by Lean are inaccessible by default. The motivation is to
ensure your tactic proofs do not rely on automatically generated names, and are consequently more robust.
However, you can use the combinator <code>unhygienic</code> to disable this restriction.</p>
<pre><code class="language-lean">example : ∀ a b c : Nat, a = b → a = c → c = b := by unhygienic
  intros
  apply Eq.trans
  apply Eq.symm
  exact a_2
  exact a_1
</code></pre>
<p>You can also use the <code>rename_i</code> tactic to rename the most recent inaccessible names in your context.
In the following example, the tactic <code>rename_i h1 _ h2</code> renames two of the last three hypotheses in
your context.</p>
<pre><code class="language-lean">example : ∀ a b c d : Nat, a = b → a = d → a = c → c = b := by
  intros
  rename_i h1 _ h2
  apply Eq.trans
  apply Eq.symm
  exact h2
  exact h1
</code></pre>
<p>The <code>rfl</code> tactic is syntactic sugar for <code>exact rfl</code>.</p>
<pre><code class="language-lean">example (y : Nat) : (fun x : Nat =&gt; 0) y = 0 :=
  by rfl
</code></pre>
<p>The <code>repeat</code> combinator can be used to apply a tactic several times.</p>
<pre><code class="language-lean">example : ∀ a b c : Nat, a = b → a = c → c = b := by
  intros
  apply Eq.trans
  apply Eq.symm
  repeat assumption
</code></pre>
<p>Another tactic that is sometimes useful is the <code>revert</code> tactic,
which is, in a sense, an inverse to <code>intro</code>.</p>
<pre><code class="language-lean">example (x : Nat) : x = x := by
  revert x
  -- goal is ⊢ ∀ (x : Nat), x = x
  intro y
  -- goal is y : Nat ⊢ y = y
  rfl
</code></pre>
<p>Moving a hypothesis into the goal yields an implication:</p>
<pre><code class="language-lean">example (x y : Nat) (h : x = y) : y = x := by
  revert h
  -- goal is x y : Nat ⊢ x = y → y = x
  intro h₁
  -- goal is x y : Nat, h₁ : x = y ⊢ y = x
  apply Eq.symm
  assumption
</code></pre>
<p>But <code>revert</code> is even more clever, in that it will revert not only an
element of the context but also all the subsequent elements of the
context that depend on it. For example, reverting <code>x</code> in the example
above brings <code>h</code> along with it:</p>
<pre><code class="language-lean">example (x y : Nat) (h : x = y) : y = x := by
  revert x
  -- goal is y : Nat ⊢ ∀ (x : Nat), x = y → y = x
  intros
  apply Eq.symm
  assumption
</code></pre>
<p>You can also revert multiple elements of the context at once:</p>
<pre><code class="language-lean">example (x y : Nat) (h : x = y) : y = x := by
  revert x y
  -- goal is ⊢ ∀ (x y : Nat), x = y → y = x
  intros
  apply Eq.symm
  assumption
</code></pre>
<p>You can only <code>revert</code> an element of the local context, that is, a
local variable or hypothesis. But you can replace an arbitrary
expression in the goal by a fresh variable using the <code>generalize</code>
tactic.</p>
<pre><code class="language-lean">example : 3 = 3 := by
  generalize 3 = x
  -- goal is x : Nat ⊢ x = x
  revert x
  -- goal is ⊢ ∀ (x : Nat), x = x
  intro y
  -- goal is y : Nat ⊢ y = y
  rfl
</code></pre>
<p>The mnemonic in the notation above is that you are generalizing the
goal by setting <code>3</code> to an arbitrary variable <code>x</code>. Be careful: not
every generalization preserves the validity of the goal. Here,
<code>generalize</code> replaces a goal that could be proved using
<code>rfl</code> with one that is not provable:</p>
<pre><code class="language-lean">example : 2 + 3 = 5 := by
  generalize 3 = x
  -- goal is x : Nat ⊢ 2 + x = 5
  admit
</code></pre>
<p>In this example, the <code>admit</code> tactic is the analogue of the <code>sorry</code>
proof term. It closes the current goal, producing the usual warning
that <code>sorry</code> has been used. To preserve the validity of the previous
goal, the <code>generalize</code> tactic allows us to record the fact that
<code>3</code> has been replaced by <code>x</code>. All you need to do is to provide a
label, and <code>generalize</code> uses it to store the assignment in the local
context:</p>
<pre><code class="language-lean">example : 2 + 3 = 5 := by
  generalize h : 3 = x
  -- goal is x : Nat, h : 3 = x ⊢ 2 + x = 5
  rw [← h]
</code></pre>
<p>Here the <code>rewrite</code> tactic, abbreviated <code>rw</code>, uses <code>h</code> to replace
<code>x</code> by <code>3</code> again. The <code>rewrite</code> tactic will be discussed below.</p>
<h2><a class="header" href="#more-tactics" id="more-tactics">More Tactics</a></h2>
<p>Some additional tactics are useful for constructing and destructing
propositions and data. For example, when applied to a goal of the form
<code>p ∨ q</code>, you use tactics such as <code>apply Or.inl</code> and <code>apply Or.inr</code>.  Conversely, the <code>cases</code> tactic can be used to decompose a
disjunction.</p>
<pre><code class="language-lean">example (p q : Prop) : p ∨ q → q ∨ p := by
  intro h
  cases h with
  | inl hp =&gt; apply Or.inr; exact hp
  | inr hq =&gt; apply Or.inl; exact hq
</code></pre>
<p>Note that the syntax is similar to the one used in <code>match</code> expressions.
The new subgoals can be solved in any order.</p>
<pre><code class="language-lean">example (p q : Prop) : p ∨ q → q ∨ p := by
  intro h
  cases h with
  | inr hq =&gt; apply Or.inl; exact hq
  | inl hp =&gt; apply Or.inr; exact hp
</code></pre>
<p>You can also use a (unstructured) <code>cases</code> without the <code>with</code> and a tactic
for each alternative.</p>
<pre><code class="language-lean">example (p q : Prop) : p ∨ q → q ∨ p := by
  intro h
  cases h
  apply Or.inr
  assumption
  apply Or.inl
  assumption
</code></pre>
<p>The (unstructured) <code>cases</code> is particularly useful when you can close several
subgoals using the same tactic.</p>
<pre><code class="language-lean">example (p : Prop) : p ∨ p → p := by
  intro h
  cases h
  repeat assumption
</code></pre>
<p>You can also use the combinator <code>tac1 &lt;;&gt; tac2</code> to apply <code>tac2</code> to each
subgoal produced by tactic <code>tac1</code>.</p>
<pre><code class="language-lean">example (p : Prop) : p ∨ p → p := by
  intro h
  cases h &lt;;&gt; assumption
</code></pre>
<p>You can combine the unstructured <code>cases</code> tactic with the <code>case</code> and <code>.</code> notation.</p>
<pre><code class="language-lean">example (p q : Prop) : p ∨ q → q ∨ p := by
  intro h
  cases h
  . apply Or.inr
    assumption
  . apply Or.inl
    assumption

example (p q : Prop) : p ∨ q → q ∨ p := by
  intro h
  cases h
  case inr h =&gt;
    apply Or.inl
    assumption
  case inl h =&gt;
    apply Or.inr
    assumption

example (p q : Prop) : p ∨ q → q ∨ p := by
  intro h
  cases h
  case inr h =&gt;
    apply Or.inl
    assumption
  . apply Or.inr
    assumption
</code></pre>
<p>The <code>cases</code> tactic can also be used to
decompose a conjunction.</p>
<pre><code class="language-lean">example (p q : Prop) : p ∧ q → q ∧ p := by
  intro h
  cases h with
  | intro hp hq =&gt; constructor; exact hq; exact hp
</code></pre>
<p>In this example, there is only one goal after the <code>cases</code> tactic is
applied, with <code>h : p ∧ q</code> replaced by a pair of assumptions,
<code>hp : p</code> and <code>hq : q</code>. The <code>constructor</code> tactic applies the unique
constructor for conjunction, <code>And.intro</code>. With these tactics, an
example from the previous section can be rewritten as follows:</p>
<pre><code class="language-lean">example (p q r : Prop) : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) := by
  apply Iff.intro
  . intro h
    cases h with
    | intro hp hqr =&gt;
      cases hqr
      . apply Or.inl; constructor &lt;;&gt; assumption
      . apply Or.inr; constructor &lt;;&gt; assumption
  . intro h
    cases h with
    | inl hpq =&gt;
      cases hpq with
      | intro hp hq =&gt; constructor; exact hp; apply Or.inl; exact hq
    | inr hpr =&gt;
      cases hpr with
      | intro hp hr =&gt; constructor; exact hp; apply Or.inr; exact hr
</code></pre>
<p>You will see in <a href="./inductive_types.html">Chapter Inductive Types</a> that
these tactics are quite general. The <code>cases</code> tactic can be used to
decompose any element of an inductively defined type; <code>constructor</code>
always applies the first applicable constructor of an inductively defined type.
For example, you can use <code>cases</code> and <code>constructor</code> with an existential quantifier:</p>
<pre><code class="language-lean">example (p q : Nat → Prop) : (∃ x, p x) → ∃ x, p x ∨ q x := by
  intro h
  cases h with
  | intro x px =&gt; constructor; apply Or.inl; exact px
</code></pre>
<p>Here, the <code>constructor</code> tactic leaves the first component of the
existential assertion, the value of <code>x</code>, implicit. It is represented
by a metavariable, which should be instantiated later on. In the
previous example, the proper value of the metavariable is determined
by the tactic <code>exact px</code>, since <code>px</code> has type <code>p x</code>. If you want
to specify a witness to the existential quantifier explicitly, you can
use the <code>exists</code> tactic instead:</p>
<pre><code class="language-lean">example (p q : Nat → Prop) : (∃ x, p x) → ∃ x, p x ∨ q x := by
  intro h
  cases h with
  | intro x px =&gt; exists x; apply Or.inl; exact px
</code></pre>
<p>Here is another example:</p>
<pre><code class="language-lean">example (p q : Nat → Prop) : (∃ x, p x ∧ q x) → ∃ x, q x ∧ p x := by
  intro h
  cases h with
  | intro x hpq =&gt;
    cases hpq with
    | intro hp hq =&gt;
      exists x
</code></pre>
<p>These tactics can be used on data just as well as propositions. In the
next example, they are used to define functions which swap the
components of the product and sum types:</p>
<pre><code class="language-lean">def swap_pair : α × β → β × α := by
  intro p
  cases p
  constructor &lt;;&gt; assumption

def swap_sum : Sum α β → Sum β α := by
  intro p
  cases p
  . apply Sum.inr; assumption
  . apply Sum.inl; assumption
</code></pre>
<p>Note that up to the names we have chosen for the variables, the
definitions are identical to the proofs of the analogous propositions
for conjunction and disjunction. The <code>cases</code> tactic will also do a
case distinction on a natural number:</p>
<pre><code class="language-lean">open Nat
example (P : Nat → Prop) (h₀ : P 0) (h₁ : ∀ n, P (succ n)) (m : Nat) : P m := by
  cases m with
  | zero    =&gt; exact h₀
  | succ m' =&gt; exact h₁ m'
</code></pre>
<p>The <code>cases</code> tactic, and its companion, the <code>induction</code> tactic, are discussed in greater detail in
the <a href="./inductive_types.html#tactics-for-inductive-types">Tactics for Inductive Types</a> section.</p>
<p>The <code>contradiction</code> tactic searches for a contradiction among the hypotheses of the current goal:</p>
<pre><code class="language-lean">example (p q : Prop) : p ∧ ¬ p → q := by
  intro h
  cases h
  contradiction
</code></pre>
<p>You can also use <code>match</code> in tactic blocks.</p>
<pre><code class="language-lean">example (p q r : Prop) : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) := by
  apply Iff.intro
  . intro h
    match h with
    | ⟨_, Or.inl _⟩ =&gt; apply Or.inl; constructor &lt;;&gt; assumption
    | ⟨_, Or.inr _⟩ =&gt; apply Or.inr; constructor &lt;;&gt; assumption
  . intro h
    match h with
    | Or.inl ⟨hp, hq⟩ =&gt; constructor; exact hp; apply Or.inl; exact hq
    | Or.inr ⟨hp, hr⟩ =&gt; constructor; exact hp; apply Or.inr; exact hr
</code></pre>
<p>You can &quot;combine&quot; <code>intro h</code> with <code>match h ...</code> and write the previous examples as follows</p>
<pre><code class="language-lean">example (p q r : Prop) : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) := by
  apply Iff.intro
  . intro
    | ⟨hp, Or.inl hq⟩ =&gt; apply Or.inl; constructor &lt;;&gt; assumption
    | ⟨hp, Or.inr hr⟩ =&gt; apply Or.inr; constructor &lt;;&gt; assumption
  . intro
    | Or.inl ⟨hp, hq⟩ =&gt; constructor; assumption; apply Or.inl; assumption
    | Or.inr ⟨hp, hr⟩ =&gt; constructor; assumption; apply Or.inr; assumption
</code></pre>
<h2><a class="header" href="#structuring-tactic-proofs" id="structuring-tactic-proofs">Structuring Tactic Proofs</a></h2>
<p>Tactics often provide an efficient way of building a proof, but long
sequences of instructions can obscure the structure of the
argument. In this section, we describe some means that help provide
structure to a tactic-style proof, making such proofs more readable
and robust.</p>
<p>One thing that is nice about Lean's proof-writing syntax is that it is
possible to mix term-style and tactic-style proofs, and pass between
the two freely. For example, the tactics <code>apply</code> and <code>exact</code>
expect arbitrary terms, which you can write using <code>have</code>, <code>show</code>,
and so on. Conversely, when writing an arbitrary Lean term, you can
always invoke the tactic mode by inserting a <code>by</code>
block. The following is a somewhat toy example:</p>
<pre><code class="language-lean">example (p q r : Prop) : p ∧ (q ∨ r) → (p ∧ q) ∨ (p ∧ r) := by
  intro h
  exact
    have hp : p := h.left
    have hqr : q ∨ r := h.right
    show (p ∧ q) ∨ (p ∧ r) by
      cases hqr with
      | inl hq =&gt; exact Or.inl ⟨hp, hq⟩
      | inr hr =&gt; exact Or.inr ⟨hp, hr⟩
</code></pre>
<p>The following is a more natural example:</p>
<pre><code class="language-lean">example (p q r : Prop) : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) := by
  apply Iff.intro
  . intro h
    cases h.right with
    | inl hq =&gt; exact Or.inl ⟨h.left, hq⟩
    | inr hr =&gt; exact Or.inr ⟨h.left, hr⟩
  . intro h
    cases h with
    | inl hpq =&gt; exact ⟨hpq.left, Or.inl hpq.right⟩
    | inr hpr =&gt; exact ⟨hpr.left, Or.inr hpr.right⟩
</code></pre>
<p>In fact, there is a <code>show</code> tactic, which is similar to the
<code>show</code> expression in a proof term. It simply declares the type of the
goal that is about to be solved, while remaining in tactic
mode.</p>
<pre><code class="language-lean">example (p q r : Prop) : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) := by
  apply Iff.intro
  . intro h
    cases h.right with
    | inl hq =&gt;
      show (p ∧ q) ∨ (p ∧ r)
      exact Or.inl ⟨h.left, hq⟩
    | inr hr =&gt;
      show (p ∧ q) ∨ (p ∧ r)
      exact Or.inr ⟨h.left, hr⟩
  . intro h
    cases h with
    | inl hpq =&gt;
      show p ∧ (q ∨ r)
      exact ⟨hpq.left, Or.inl hpq.right⟩
    | inr hpr =&gt;
      show p ∧ (q ∨ r)
      exact ⟨hpr.left, Or.inr hpr.right⟩
</code></pre>
<p>The <code>show</code> tactic can actually be used to rewrite a goal to something definitionally equivalent:</p>
<pre><code class="language-lean">example (n : Nat) : n + 1 = Nat.succ n := by
  show Nat.succ n = Nat.succ n
  rfl
</code></pre>
<p>There is also a <code>have</code> tactic, which introduces a new subgoal, just as when writing proof terms:</p>
<pre><code class="language-lean">example (p q r : Prop) : p ∧ (q ∨ r) → (p ∧ q) ∨ (p ∧ r) := by
  intro ⟨hp, hqr⟩
  show (p ∧ q) ∨ (p ∧ r)
  cases hqr with
  | inl hq =&gt;
    have hpq : p ∧ q := And.intro hp hq
    apply Or.inl
    exact hpq
  | inr hr =&gt;
    have hpr : p ∧ r := And.intro hp hr
    apply Or.inr
    exact hpr
</code></pre>
<p>As with proof terms, you can omit the label in the <code>have</code> tactic, in
which case, the default label <code>this</code> is used:</p>
<pre><code class="language-lean">example (p q r : Prop) : p ∧ (q ∨ r) → (p ∧ q) ∨ (p ∧ r) := by
  intro ⟨hp, hqr⟩
  show (p ∧ q) ∨ (p ∧ r)
  cases hqr with
  | inl hq =&gt;
    have : p ∧ q := And.intro hp hq
    apply Or.inl
    exact this
  | inr hr =&gt;
    have : p ∧ r := And.intro hp hr
    apply Or.inr
    exact this
</code></pre>
<p>The types in a <code>have</code> tactic can be omitted, so you can write <code>have hp := h.left</code> and <code>have hqr := h.right</code>.  In fact, with this
notation, you can even omit both the type and the label, in which case
the new fact is introduced with the label <code>this</code>.</p>
<pre><code class="language-lean">example (p q r : Prop) : p ∧ (q ∨ r) → (p ∧ q) ∨ (p ∧ r) := by
  intro ⟨hp, hqr⟩
  cases hqr with
  | inl hq =&gt;
    have := And.intro hp hq
    apply Or.inl; exact this
  | inr hr =&gt;
    have := And.intro hp hr
    apply Or.inr; exact this
</code></pre>
<p>Lean also has a <code>let</code> tactic, which is similar to the <code>have</code>
tactic, but is used to introduce local definitions instead of
auxiliary facts. It is the tactic analogue of a <code>let</code> in a proof
term.</p>
<pre><code class="language-lean">example : ∃ x, x + 2 = 8 := by
  let a : Nat := 3 * 2
  exists a
</code></pre>
<p>As with <code>have</code>, you can leave the type implicit by writing <code>let a := 3 * 2</code>. The difference between <code>let</code> and <code>have</code> is that
<code>let</code> introduces a local definition in the context, so that the
definition of the local declaration can be unfolded in the proof.</p>
<p>We have used <code>.</code> to create nested tactic blocks.  In a nested block,
Lean focuses on the first goal, and generates an error if it has not
been fully solved at the end of the block. This can be helpful in
indicating the separate proofs of multiple subgoals introduced by a
tactic. The notation <code>.</code> is whitespace sensitive and relies on the indentation
to detect whether the tactic block ends. Alternatively, you can
define tactic blocks using curly braces and semicolons.</p>
<pre><code class="language-lean">example (p q r : Prop) : p ∧ (q ∨ r) ↔ (p ∧ q) ∨ (p ∧ r) := by
  apply Iff.intro
  { intro h;
    cases h.right;
    { show (p ∧ q) ∨ (p ∧ r);
      exact Or.inl ⟨h.left, ‹q›⟩ }
    { show (p ∧ q) ∨ (p ∧ r);
      exact Or.inr ⟨h.left, ‹r›⟩ } }
  { intro h;
    cases h;
    { show p ∧ (q ∨ r);
      rename_i hpq;
      exact ⟨hpq.left, Or.inl hpq.right⟩ }
    { show p ∧ (q ∨ r);
      rename_i hpr;
      exact ⟨hpr.left, Or.inr hpr.right⟩ } }
</code></pre>
<p>It is useful to use indentation to structure proof: every time a tactic
leaves more than one subgoal, we separate the remaining subgoals by
enclosing them in blocks and indenting.  Thus if the application of
theorem <code>foo</code> to a single goal produces four subgoals, one would
expect the proof to look like this:</p>
<pre><code>  apply foo
  . &lt;proof of first goal&gt;
  . &lt;proof of second goal&gt;
  . &lt;proof of third goal&gt;
  . &lt;proof of final goal&gt;
</code></pre>
<p>or</p>
<pre><code>  apply foo
  case &lt;tag of first goal&gt;  =&gt; &lt;proof of first goal&gt;
  case &lt;tag of second goal&gt; =&gt; &lt;proof of second goal&gt;
  case &lt;tag of third goal&gt;  =&gt; &lt;proof of third goal&gt;
  case &lt;tag of final goal&gt;  =&gt; &lt;proof of final goal&gt;
</code></pre>
<p>or</p>
<pre><code>  apply foo
  { &lt;proof of first goal&gt;  }
  { &lt;proof of second goal&gt; }
  { &lt;proof of third goal&gt;  }
  { &lt;proof of final goal&gt;  }
</code></pre>
<h2><a class="header" href="#tactic-combinators" id="tactic-combinators">Tactic Combinators</a></h2>
<p><em>Tactic combinators</em> are operations that form new tactics from old
ones. A sequencing combinator is already implicit in the <code>by</code> block:</p>
<pre><code class="language-lean">example (p q : Prop) (hp : p) : p ∨ q :=
  by apply Or.inl; assumption
</code></pre>
<p>Here, <code>apply Or.inl; assumption</code> is functionally equivalent to a
single tactic which first applies <code>apply Or.inl</code> and then applies
<code>assumption</code>.</p>
<p>In <code>t₁ &lt;;&gt; t₂</code>, the <code>&lt;;&gt;</code> operator provides a <em>parallel</em> version of the sequencing operation:
<code>t₁</code> is applied to the current goal, and then <code>t₂</code> is applied to <em>all</em> the resulting subgoals:</p>
<pre><code class="language-lean">example (p q : Prop) (hp : p) (hq : q) : p ∧ q :=
  by constructor &lt;;&gt; assumption
</code></pre>
<p>This is especially useful when the resulting goals can be finished off
in a uniform way, or, at least, when it is possible to make progress
on all of them uniformly.</p>
<p>The <code>first | t₁ | t₂ | ... | tₙ</code> applies each <code>tᵢ</code> until one succeeds, or else fails:</p>
<pre><code class="language-lean">example (p q : Prop) (hp : p) : p ∨ q := by
  first | apply Or.inl; assumption | apply Or.inr; assumption

example (p q : Prop) (hq : q) : p ∨ q := by
  first | apply Or.inl; assumption | apply Or.inr; assumption
</code></pre>
<p>In the first example, the left branch succeeds, whereas in the second one, it is the right one that succeeds.
In the next three examples, the same compound tactic succeeds in each case.</p>
<pre><code class="language-lean">example (p q r : Prop) (hp : p) : p ∨ q ∨ r :=
  by repeat (first | apply Or.inl; assumption | apply Or.inr | assumption)

example (p q r : Prop) (hq : q) : p ∨ q ∨ r :=
  by repeat (first | apply Or.inl; assumption | apply Or.inr | assumption)

example (p q r : Prop) (hr : r) : p ∨ q ∨ r :=
  by repeat (first | apply Or.inl; assumption | apply Or.inr | assumption)
</code></pre>
<p>The tactic tries to solve the left disjunct immediately by assumption;
if that fails, it tries to focus on the right disjunct; and if that
doesn't work, it invokes the assumption tactic.</p>
<p>You will have no doubt noticed by now that tactics can fail. Indeed,
it is the &quot;failure&quot; state that causes the <em>first</em> combinator to
backtrack and try the next tactic. The <code>try</code> combinator builds a
tactic that always succeeds, though possibly in a trivial way:
<code>try t</code> executes <code>t</code> and reports success, even if <code>t</code> fails. It is
equivalent to <code>first | t | skip</code>, where <code>skip</code> is a tactic that does
nothing (and succeeds in doing so). In the next example, the second
<code>constructor</code> succeeds on the right conjunct <code>q ∧ r</code> (remember that
disjunction and conjunction associate to the right) but fails on the
first. The <code>try</code> tactic ensures that the sequential composition
succeeds.</p>
<pre><code class="language-lean">example (p q r : Prop) (hp : p) (hq : q) (hr : r) : p ∧ q ∧ r := by
  constructor &lt;;&gt; (try constructor) &lt;;&gt; assumption
</code></pre>
<p>Be careful: <code>repeat (try t)</code> will loop forever, because the inner tactic never fails.</p>
<p>In a proof, there are often multiple goals outstanding. Parallel
sequencing is one way to arrange it so that a single tactic is applied
to multiple goals, but there are other ways to do this. For example,
<code>all_goals t</code> applies <code>t</code> to all open goals:</p>
<pre><code class="language-lean">example (p q r : Prop) (hp : p) (hq : q) (hr : r) : p ∧ q ∧ r := by
  constructor
  all_goals (try constructor)
  all_goals assumption
</code></pre>
<p>In this case, the <code>any_goals</code> tactic provides a more robust solution.
It is similar to <code>all_goals</code>, except it succeeds if its argument
succeeds on at least one goal.</p>
<pre><code class="language-lean">example (p q r : Prop) (hp : p) (hq : q) (hr : r) : p ∧ q ∧ r := by
  constructor
  any_goals constructor
  any_goals assumption
</code></pre>
<p>The first tactic in the <code>by</code> block below repeatedly splits
conjunctions:</p>
<pre><code class="language-lean">example (p q r : Prop) (hp : p) (hq : q) (hr : r) :
      p ∧ ((p ∧ q) ∧ r) ∧ (q ∧ r ∧ p) := by
  repeat (any_goals constructor)
  all_goals assumption
</code></pre>
<p>In fact, we can compress the full tactic down to one line:</p>
<pre><code class="language-lean">example (p q r : Prop) (hp : p) (hq : q) (hr : r) :
      p ∧ ((p ∧ q) ∧ r) ∧ (q ∧ r ∧ p) := by
  repeat (any_goals (first | constructor | assumption))
</code></pre>
<p>The combinator <code>focus t</code> ensures that <code>t</code> only effects the current
goal, temporarily hiding the others from the scope. So, if <code>t</code>
ordinarily only effects the current goal, <code>focus (all_goals t)</code> has
the same effect as <code>t</code>.</p>
<h2><a class="header" href="#rewriting" id="rewriting">Rewriting</a></h2>
<p>The <code>rewrite</code> tactic (abbreviated <code>rw</code>) and the <code>simp</code> tactic
were introduced briefly in <a href="./quantifiers_and_equality.html#calculational-proofs">Calculational Proofs</a>. In this
section and the next, we discuss them in greater detail.</p>
<p>The <code>rewrite</code> tactic provides a basic mechanism for applying
substitutions to goals and hypotheses, providing a convenient and
efficient way of working with equality. The most basic form of the
tactic is <code>rewrite [t]</code>, where <code>t</code> is a term whose type asserts an
equality. For example, <code>t</code> can be a hypothesis <code>h : x = y</code> in the
context; it can be a general lemma, like
<code>add_comm : ∀ x y, x + y = y + x</code>, in which the rewrite tactic tries to find suitable
instantiations of <code>x</code> and <code>y</code>; or it can be any compound term
asserting a concrete or general equation. In the following example, we
use this basic form to rewrite the goal using a hypothesis.</p>
<pre><code class="language-lean">example (f : Nat → Nat) (k : Nat) (h₁ : f 0 = 0) (h₂ : k = 0) : f k = 0 := by
  rw [h₂] -- replace k with 0
  rw [h₁] -- replace f 0 with 0
</code></pre>
<p>In the example above, the first use of <code>rw</code> replaces <code>k</code> with
<code>0</code> in the goal <code>f k = 0</code>. Then, the second one replaces <code>f 0</code>
with <code>0</code>. The tactic automatically closes any goal of the form
<code>t = t</code>. Here is an example of rewriting using a compound expression:</p>
<pre><code class="language-lean">example (x y : Nat) (p : Nat → Prop) (q : Prop) (h : q → x = y)
        (h' : p y) (hq : q) : p x := by
  rw [h hq]; assumption
</code></pre>
<p>Here, <code>h hq</code> establishes the equation <code>x = y</code>.</p>
<p>Multiple rewrites can be combined using the notation <code>rw [t_1, ..., t_n]</code>,
which is just shorthand for <code>rw [t_1]; ...; rw [t_n]</code>. The previous example can be written as follows:</p>
<pre><code class="language-lean">example (f : Nat → Nat) (k : Nat) (h₁ : f 0 = 0) (h₂ : k = 0) : f k = 0 := by
  rw [h₂, h₁]
</code></pre>
<p>By default, <code>rw</code> uses an equation in the forward direction, matching
the left-hand side with an expression, and replacing it with the
right-hand side. The notation <code>←t</code> can be used to instruct the
tactic to use the equality <code>t</code> in the reverse direction.</p>
<pre><code class="language-lean">example (f : Nat → Nat) (a b : Nat) (h₁ : a = b) (h₂ : f a = 0) : f b = 0 := by
  rw [←h₁, h₂]
</code></pre>
<p>In this example, the term <code>←h₁</code> instructs the rewriter to replace
<code>b</code> with <code>a</code>. In the editors, you can type the backwards arrow as
<code>\l</code>. You can also use the ascii equivalent, <code>&lt;-</code>.</p>
<p>Sometimes the left-hand side of an identity can match more than one
subterm in the pattern, in which case the <code>rw</code> tactic chooses the
first match it finds when traversing the term. If that is not the one
you want, you can use additional arguments to specify the appropriate
subterm.</p>
<pre><code class="language-lean">example (a b c : Nat) : a + b + c = a + c + b := by
  rw [Nat.add_assoc, Nat.add_comm b, ← Nat.add_assoc]

example (a b c : Nat) : a + b + c = a + c + b := by
  rw [Nat.add_assoc, Nat.add_assoc, Nat.add_comm b]

example (a b c : Nat) : a + b + c = a + c + b := by
  rw [Nat.add_assoc, Nat.add_assoc, Nat.add_comm _ b]
</code></pre>
<p>In the first example above, the first step rewrites <code>a + b + c</code> to
<code>a + (b + c)</code>. The next step applies commutativity to the term
<code>b + c</code>; without specifying the argument, the tactic would instead rewrite
<code>a + (b + c)</code> to <code>(b + c) + a</code>. Finally, the last step applies
associativity in the reverse direction, rewriting <code>a + (c + b)</code> to
<code>a + c + b</code>. The next two examples instead apply associativity to
move the parenthesis to the right on both sides, and then switch <code>b</code>
and <code>c</code>. Notice that the last example specifies that the rewrite
should take place on the right-hand side by specifying the second
argument to <code>Nat.add_comm</code>.</p>
<p>By default, the <code>rewrite</code> tactic affects only the goal. The notation
<code>rw [t] at h</code> applies the rewrite <code>t</code> at hypothesis <code>h</code>.</p>
<pre><code class="language-lean">example (f : Nat → Nat) (a : Nat) (h : a + 0 = 0) : f a = f 0 := by
  rw [Nat.add_zero] at h
  rw [h]
</code></pre>
<p>The first step, <code>rw [Nat.add_zero] at h</code>, rewrites the hypothesis <code>a + 0 = 0</code> to <code>a = 0</code>.
Then the new hypothesis <code>a = 0</code> is used to rewrite the goal to <code>f 0 = f 0</code>.</p>
<p>The <code>rewrite</code> tactic is not restricted to propositions.
In the following example, we use <code>rw [h] at t</code> to rewrite the hypothesis <code>t : Tuple α n</code> to <code>t : Tuple α 0</code>.</p>
<pre><code class="language-lean">def Tuple (α : Type) (n : Nat) :=
  { as : List α // as.length = n }

example (n : Nat) (h : n = 0) (t : Tuple α n) : Tuple α 0 := by
  rw [h] at t
  exact t
</code></pre>
<h2><a class="header" href="#using-the-simplifier" id="using-the-simplifier">Using the Simplifier</a></h2>
<p>Whereas <code>rewrite</code> is designed as a surgical tool for manipulating a
goal, the simplifier offers a more powerful form of automation. A
number of identities in Lean's library have been tagged with the
<code>[simp]</code> attribute, and the <code>simp</code> tactic uses them to iteratively
rewrite subterms in an expression.</p>
<pre><code class="language-lean">example (x y z : Nat) : (x + 0) * (0 + y * 1 + z * 0) = x * y := by
  simp

example (x y z : Nat) (p : Nat → Prop) (h : p (x * y))
        : p ((x + 0) * (0 + y * 1 + z * 0)) := by
  simp; assumption
</code></pre>
<p>In the first example, the left-hand side of the equality in the goal
is simplified using the usual identities involving 0 and 1, reducing
the goal to <code>x * y = x * y</code>. At that point, <code>simp</code> applies
reflexivity to finish it off. In the second example, <code>simp</code> reduces
the goal to <code>p (x * y)</code>, at which point the assumption <code>h</code>
finishes it off. Here are some more examples
with lists:</p>
<pre><code class="language-lean">open List

example (xs : List Nat)
        : reverse (xs ++ [1, 2, 3]) = [3, 2, 1] ++ reverse xs := by
  simp

example (xs ys : List α)
        : length (reverse (xs ++ ys)) = length xs + length ys := by
  simp [Nat.add_comm]
</code></pre>
<p>As with <code>rw</code>, you can use the keyword <code>at</code> to simplify a hypothesis:</p>
<pre><code class="language-lean">example (x y z : Nat) (p : Nat → Prop)
        (h : p ((x + 0) * (0 + y * 1 + z * 0))) : p (x * y) := by
  simp at h; assumption
</code></pre>
<p>Moreover, you can use a &quot;wildcard&quot; asterisk to simplify all the hypotheses and the goal:</p>
<pre><code class="language-lean">attribute [local simp] Nat.mul_comm Nat.mul_assoc Nat.mul_left_comm
attribute [local simp] Nat.add_assoc Nat.add_comm Nat.add_left_comm

example (w x y z : Nat) (p : Nat → Prop)
        (h : p (x * y + z * w * x)) : p (x * w * z + y * x) := by
  simp at *; assumption

example (x y z : Nat) (p : Nat → Prop)
        (h₁ : p (1 * x + y)) (h₂ : p (x * z * 1))
        : p (y + 0 + x) ∧ p (z * x) := by
  simp at * &lt;;&gt; constructor &lt;;&gt; assumption
</code></pre>
<p>For operations that are commutative and associative, like
multiplication on the natural numbers, the simplifier uses these two
facts to rewrite an expression, as well as <em>left commutativity</em>. In
the case of multiplication the latter is expressed as follows:
<code>x * (y * z) = y * (x * z)</code>. The <code>local</code> modifier tells the simplifier
to use these rules in the current file (or section or namespace, as
the case may be). It may seem that commutativity and
left-commutativity are problematic, in that repeated application of
either causes looping. But the simplifier detects identities that
permute their arguments, and uses a technique known as <em>ordered
rewriting</em>. This means that the system maintains an internal ordering
of terms, and only applies the identity if doing so decreases the
order. With the three identities mentioned above, this has the effect
that all the parentheses in an expression are associated to the right,
and the expressions are ordered in a canonical (though somewhat
arbitrary) way. Two expressions that are equivalent up to
associativity and commutativity are then rewritten to the same
canonical form.</p>
<pre><code class="language-lean"><span class="boring">attribute [local simp] Nat.mul_comm Nat.mul_assoc Nat.mul_left_comm
</span><span class="boring">attribute [local simp] Nat.add_assoc Nat.add_comm Nat.add_left_comm
</span>example (w x y z : Nat) (p : Nat → Prop)
        : x * y + z * w * x = x * w * z + y * x := by
  simp

example (w x y z : Nat) (p : Nat → Prop)
        (h : p (x * y + z * w * x)) : p (x * w * z + y * x) := by
  simp; simp at h; assumption
</code></pre>
<p>As with <code>rewrite</code>, you can send <code>simp</code> a list of facts to use,
including general lemmas, local hypotheses, definitions to unfold, and
compound expressions. The <code>simp</code> tactic also recognizes the <code>←t</code>
syntax that <code>rewrite</code> does. In any case, the additional rules are
added to the collection of identities that are used to simplify a
term.</p>
<pre><code class="language-lean">def f (m n : Nat) : Nat :=
  m + n + m

example {m n : Nat} (h : n = 1) (h' : 0 = m) : (f m n) = n := by
  simp [h, ←h', f]
</code></pre>
<p>A common idiom is to simplify a goal using local hypotheses:</p>
<pre><code class="language-lean">example (f : Nat → Nat) (k : Nat) (h₁ : f 0 = 0) (h₂ : k = 0) : f k = 0 := by
  simp [h₁, h₂]
</code></pre>
<p>To use all the hypotheses present in the local context when
simplifying, we can use the wildcard symbol, <code>*</code>:</p>
<pre><code class="language-lean">example (f : Nat → Nat) (k : Nat) (h₁ : f 0 = 0) (h₂ : k = 0) : f k = 0 := by
  simp [*]
</code></pre>
<p>Here is another example:</p>
<pre><code class="language-lean">example (u w x y z : Nat) (h₁ : x = y + z) (h₂ : w = u + x)
        : w = z + y + u := by
  simp [*, Nat.add_assoc, Nat.add_comm, Nat.add_left_comm]
</code></pre>
<p>The simplifier will also do propositional rewriting. For example,
using the hypothesis <code>p</code>, it rewrites <code>p ∧ q</code> to <code>q</code> and <code>p ∨ q</code> to <code>true</code>, which it then proves trivially. Iterating such
rewrites produces nontrivial propositional reasoning.</p>
<pre><code class="language-lean">example (p q : Prop) (hp : p) : p ∧ q ↔ q := by
  simp [*]

example (p q : Prop) (hp : p) : p ∨ q := by
  simp [*]

example (p q r : Prop) (hp : p) (hq : q) : p ∧ (q ∨ r) := by
  simp [*]
</code></pre>
<p>The next example simplifies all the hypotheses, and then uses them to prove the goal.</p>
<pre><code class="language-lean">example (u w x x' y y' z : Nat) (p : Nat → Prop)
        (h₁ : x + 0 = x') (h₂ : y + 0 = y')
        : x + y + 0 = x' + y' := by
  simp at *
  simp [*]
</code></pre>
<p>One thing that makes the simplifier especially useful is that its
capabilities can grow as a library develops. For example, suppose we
define a list operation that symmetrizes its input by appending its
reversal:</p>
<pre><code class="language-lean">def mk_symm (xs : List α) :=
  xs ++ xs.reverse
</code></pre>
<p>Then for any list <code>xs</code>, <code>reverse (mk_symm xs)</code> is equal to <code>mk_symm xs</code>,
which can easily be proved by unfolding the definition:</p>
<pre><code class="language-lean"><span class="boring">def mk_symm (xs : List α) :=
</span><span class="boring"> xs ++ xs.reverse
</span>theorem reverse_mk_symm (xs : List α)
        : (mk_symm xs).reverse = mk_symm xs := by
  simp [mk_symm]
</code></pre>
<p>We can now use this theorem to prove new results:</p>
<pre><code class="language-lean"><span class="boring">def mk_symm (xs : List α) :=
</span><span class="boring"> xs ++ xs.reverse
</span><span class="boring">theorem reverse_mk_symm (xs : List α)
</span><span class="boring">       : (mk_symm xs).reverse = mk_symm xs := by
</span><span class="boring"> simp [mk_symm]
</span>example (xs ys : List Nat)
        : (xs ++ mk_symm ys).reverse = mk_symm ys ++ xs.reverse := by
  simp [reverse_mk_symm]

example (xs ys : List Nat) (p : List Nat → Prop)
        (h : p (xs ++ mk_symm ys).reverse)
        : p (mk_symm ys ++ xs.reverse) := by
  simp [reverse_mk_symm] at h; assumption
</code></pre>
<p>But using <code>reverse_mk_symm</code> is generally the right thing to do, and
it would be nice if users did not have to invoke it explicitly. You can
achieve that by marking it as a simplification rule when the theorem
is defined:</p>
<pre><code class="language-lean"><span class="boring">def mk_symm (xs : List α) :=
</span><span class="boring"> xs ++ xs.reverse
</span>@[simp] theorem reverse_mk_symm (xs : List α)
        : (mk_symm xs).reverse = mk_symm xs := by
  simp [mk_symm]

example (xs ys : List Nat)
        : (xs ++ mk_symm ys).reverse = mk_symm ys ++ xs.reverse := by
  simp

example (xs ys : List Nat) (p : List Nat → Prop)
        (h : p (xs ++ mk_symm ys).reverse)
        : p (mk_symm ys ++ xs.reverse) := by
  simp at h; assumption
</code></pre>
<p>The notation <code>@[simp]</code> declares <code>reverse_mk_symm</code> to have the
<code>[simp]</code> attribute, and can be spelled out more explicitly:</p>
<pre><code class="language-lean"><span class="boring">def mk_symm (xs : List α) :=
</span><span class="boring"> xs ++ xs.reverse
</span>theorem reverse_mk_symm (xs : List α)
        : (mk_symm xs).reverse = mk_symm xs := by
  simp [mk_symm]

attribute [simp] reverse_mk_symm

example (xs ys : List Nat)
        : (xs ++ mk_symm ys).reverse = mk_symm ys ++ xs.reverse := by
  simp

example (xs ys : List Nat) (p : List Nat → Prop)
        (h : p (xs ++ mk_symm ys).reverse)
        : p (mk_symm ys ++ xs.reverse) := by
  simp at h; assumption
</code></pre>
<p>The attribute can also be applied any time after the theorem is declared:</p>
<pre><code class="language-lean"><span class="boring">def mk_symm (xs : List α) :=
</span><span class="boring"> xs ++ xs.reverse
</span>theorem reverse_mk_symm (xs : List α)
        : (mk_symm xs).reverse = mk_symm xs := by
  simp [mk_symm]

example (xs ys : List Nat)
        : (xs ++ mk_symm ys).reverse = mk_symm ys ++ xs.reverse := by
  simp [reverse_mk_symm]

attribute [simp] reverse_mk_symm

example (xs ys : List Nat) (p : List Nat → Prop)
        (h : p (xs ++ mk_symm ys).reverse)
        : p (mk_symm ys ++ xs.reverse) := by
  simp at h; assumption
</code></pre>
<p>Once the attribute is applied, however, there is no way to permanently
remove it; it persists in any file that imports the one where the
attribute is assigned. As we will discuss further in
<a href="./interacting_with_lean.html#attributes">Attributes</a>, one can limit the scope of an attribute to the
current file or section using the <code>local</code> modifier:</p>
<pre><code class="language-lean"><span class="boring">def mk_symm (xs : List α) :=
</span><span class="boring"> xs ++ xs.reverse
</span>theorem reverse_mk_symm (xs : List α)
        : (mk_symm xs).reverse = mk_symm xs := by
  simp [mk_symm]

section
attribute [local simp] reverse_mk_symm

example (xs ys : List Nat)
        : (xs ++ mk_symm ys).reverse = mk_symm ys ++ xs.reverse := by
  simp

example (xs ys : List Nat) (p : List Nat → Prop)
        (h : p (xs ++ mk_symm ys).reverse)
        : p (mk_symm ys ++ xs.reverse) := by
  simp at h; assumption
end
</code></pre>
<p>Outside the section, the simplifier will no longer use
<code>reverse_mk_symm</code> by default.</p>
<p>Note that the various <code>simp</code> options we have discussed --- giving an
explicit list of rules, and using <code>at</code> to specify the location --- can be combined,
but the order they are listed is rigid. You can see the correct order
in an editor by placing the cursor on the <code>simp</code> identifier to see
the documentation string that is associated with it.</p>
<p>There are two additional modifiers that are useful. By default,
<code>simp</code> includes all theorems that have been marked with the
attribute <code>[simp]</code>. Writing <code>simp only</code> excludes these defaults,
allowing you to use a more explicitly crafted list of
rules. In the examples below, the minus sign and
<code>only</code> are used to block the application of <code>reverse_mk_symm</code>.</p>
<pre><code class="language-lean">def mk_symm (xs : List α) :=
  xs ++ xs.reverse
@[simp] theorem reverse_mk_symm (xs : List α)
        : (mk_symm xs).reverse = mk_symm xs := by
  simp [mk_symm]

example (xs ys : List Nat) (p : List Nat → Prop)
        (h : p (xs ++ mk_symm ys).reverse)
        : p (mk_symm ys ++ xs.reverse) := by
  simp at h; assumption

example (xs ys : List Nat) (p : List Nat → Prop)
        (h : p (xs ++ mk_symm ys).reverse)
        : p ((mk_symm ys).reverse ++ xs.reverse) := by
  simp [-reverse_mk_symm] at h; assumption

example (xs ys : List Nat) (p : List Nat → Prop)
        (h : p (xs ++ mk_symm ys).reverse)
        : p ((mk_symm ys).reverse ++ xs.reverse) := by
  simp only [List.reverse_append] at h; assumption
</code></pre>
<p>The <code>simp</code> tactic has many configuration options. For example, we can enable contextual simplifications as follows.</p>
<pre><code class="language-lean">example : if x = 0 then y + x = y else x ≠ 0 := by
  simp (config := { contextual := true })
</code></pre>
<p>when <code>contextual := true</code>, <code>simp</code> uses the fact that <code>x = 0</code> when simplifying <code>y + x = y</code>, and
<code>x ≠ 0</code> when simplifying the other branch. Here is another example.</p>
<pre><code class="language-lean">example : ∀ (x : Nat) (h : x = 0), y + x = y := by
  simp (config := { contextual := true })
</code></pre>
<p>Another useful configuration option is <code>arith := true</code> which enables arithmetical simplifications. It is so useful
that <code>simp_arith</code> is a shorthand for <code>simp (config := { arith := true })</code>.</p>
<pre><code class="language-lean">example : 0 &lt; 1 + x ∧ x + y + 2 ≥ y + 1 := by
  simp_arith
</code></pre>
<h2><a class="header" href="#split-tactic" id="split-tactic">Split Tactic</a></h2>
<p>The <code>split</code> tactic is useful for breaking nested <code>if-then-else</code> and <code>match</code> expressions in cases.
For a <code>match</code> expression with <code>n</code> cases, the <code>split</code> tactic generates at most <code>n</code> subgoals. Here is an example.</p>
<pre><code class="language-lean">def f (x y z : Nat) : Nat :=
  match x, y, z with
  | 5, _, _ =&gt; y
  | _, 5, _ =&gt; y
  | _, _, 5 =&gt; y
  | _, _, _ =&gt; 1

example (x y z : Nat) : x ≠ 5 → y ≠ 5 → z ≠ 5 → z = w → f x y w = 1 := by
  intros
  simp [f]
  split
  . contradiction
  . contradiction
  . contradiction
  . rfl
</code></pre>
<p>We can compress the tactic proof above as follows.</p>
<pre><code class="language-lean"><span class="boring">def f (x y z : Nat) : Nat :=
</span><span class="boring"> match x, y, z with
</span><span class="boring"> | 5, _, _ =&gt; y
</span><span class="boring"> | _, 5, _ =&gt; y
</span><span class="boring"> | _, _, 5 =&gt; y
</span><span class="boring"> | _, _, _ =&gt; 1
</span>example (x y z : Nat) : x ≠ 5 → y ≠ 5 → z ≠ 5 → z = w → f x y w = 1 := by
  intros; simp [f]; split &lt;;&gt; first | contradiction | rfl
</code></pre>
<p>The tactic <code>split &lt;;&gt; first | contradiction | rfl</code> first applies the <code>split</code> tactic,
and then for each generated goal it tries <code>contradiction</code>, and then <code>rfl</code> if <code>contradiction</code> fails.
Like <code>simp</code>, we can apply <code>split</code> to a particular hypothesis.</p>
<pre><code class="language-lean">def g (xs ys : List Nat) : Nat :=
  match xs, ys with
  | [a, b], _ =&gt; a+b+1
  | _, [b, c] =&gt; b+1
  | _, _      =&gt; 1

example (xs ys : List Nat) (h : g xs ys = 0) : False := by
  simp [g] at h; split at h &lt;;&gt; simp_arith at h
</code></pre>
<h2><a class="header" href="#extensible-tactics" id="extensible-tactics">Extensible Tactics</a></h2>
<p>In the following example, we define the notation <code>triv</code> using the command <code>syntax</code>.
Then, we use the command <code>macro_rules</code> to specify what should
be done when <code>triv</code> is used. You can provide different expansions, and the tactic
interpreter will try all of them until one succeeds.</p>
<pre><code class="language-lean">-- Define a new tactic notation
syntax &quot;triv&quot; : tactic

macro_rules
  | `(tactic| triv) =&gt; `(tactic| assumption)

example (h : p) : p := by
  triv

-- You cannot prove the following theorem using `triv`
-- example (x : α) : x = x := by
--  triv

-- Let's extend `triv`. The tactic interpreter
-- tries all possible macro extensions for `triv` until one succeeds
macro_rules
  | `(tactic| triv) =&gt; `(tactic| rfl)

example (x : α) : x = x := by
  triv

example (x : α) (h : p) : x = x ∧ p := by
  apply And.intro &lt;;&gt; triv

-- We now add a (recursive) extension
macro_rules | `(tactic| triv) =&gt; `(tactic| apply And.intro &lt;;&gt; triv)

example (x : α) (h : p) : x = x ∧ p := by
  triv
</code></pre>
<h2><a class="header" href="#exercises-2" id="exercises-2">Exercises</a></h2>
<ol>
<li>
<p>Go back to the exercises in <a href="./propositions_and_proofs.html">Chapter Propositions and
Proofs</a> and
<a href="./quantifiers_and_equality.html">Chapter Quantifiers and Equality</a> and
redo as many as you can now with tactic proofs, using also <code>rw</code>
and <code>simp</code> as appropriate.</p>
</li>
<li>
<p>Use tactic combinators to obtain a one line proof of the following:</p>
</li>
</ol>
<pre><code class="language-lean">example (p q r : Prop) (hp : p)
        : (p ∨ q ∨ r) ∧ (q ∨ p ∨ r) ∧ (q ∨ r ∨ p) := by
  admit
</code></pre>
<h1><a class="header" href="#interacting-with-lean" id="interacting-with-lean">Interacting with Lean</a></h1>
<p>You are now familiar with the fundamentals of dependent type theory,
both as a language for defining mathematical objects and a language
for constructing proofs. The one thing you are missing is a mechanism
for defining new data types. We will fill this gap in the next
chapter, which introduces the notion of an <em>inductive data type</em>. But
first, in this chapter, we take a break from the mechanics of type
theory to explore some pragmatic aspects of interacting with Lean.</p>
<p>Not all of the information found here will be useful to you right
away. We recommend skimming this section to get a sense of Lean's
features, and then returning to it as necessary.</p>
<h2><a class="header" href="#importing-files" id="importing-files">Importing Files</a></h2>
<p>The goal of Lean's front end is to interpret user input, construct
formal expressions, and check that they are well formed and type
correct. Lean also supports the use of various editors, which provide
continuous checking and feedback. More information can be found on the
Lean <a href="https://lean-lang.org/documentation/">documentation pages</a>.</p>
<p>The definitions and theorems in Lean's standard library are spread
across multiple files. Users may also wish to make use of additional
libraries, or develop their own projects across multiple files. When
Lean starts, it automatically imports the contents of the library
<code>Init</code> folder, which includes a number of fundamental definitions
and constructions. As a result, most of the examples we present here
work &quot;out of the box.&quot;</p>
<p>If you want to use additional files, however, they need to be imported
manually, via an <code>import</code> statement at the beginning of a file. The
command</p>
<pre><code>import Bar.Baz.Blah
</code></pre>
<p>imports the file <code>Bar/Baz/Blah.olean</code>, where the descriptions are
interpreted relative to the Lean <em>search path</em>. Information as to how
the search path is determined can be found on the
<a href="https://lean-lang.org/documentation/">documentation pages</a>.
By default, it includes the standard library directory, and (in some contexts)
the root of the user's local project.</p>
<p>Importing is transitive. In other words, if you import <code>Foo</code> and <code>Foo</code> imports <code>Bar</code>,
then you also have access to the contents of <code>Bar</code>, and do not need to import it explicitly.</p>
<h2><a class="header" href="#more-on-sections" id="more-on-sections">More on Sections</a></h2>
<p>Lean provides various sectioning mechanisms to help structure a
theory. You saw in <a href="./dependent_type_theory.html#variables-and-sections">Variables and Sections</a> that the
<code>section</code> command makes it possible not only to group together
elements of a theory that go together, but also to declare variables
that are inserted as arguments to theorems and definitions, as
necessary. Remember that the point of the <code>variable</code> command is to
declare variables for use in theorems, as in the following example:</p>
<pre><code class="language-lean">section
variable (x y : Nat)

def double := x + x

#check double y
#check double (2 * x)

attribute [local simp] Nat.add_assoc Nat.add_comm Nat.add_left_comm

theorem t1 : double (x + y) = double x + double y := by
  simp [double]

#check t1 y
#check t1 (2 * x)

theorem t2 : double (x * y) = double x * y := by
  simp [double, Nat.add_mul]

end
</code></pre>
<p>The definition of <code>double</code> does not have to declare <code>x</code> as an
argument; Lean detects the dependence and inserts it
automatically. Similarly, Lean detects the occurrence of <code>x</code> in
<code>t1</code> and <code>t2</code>, and inserts it automatically there, too.
Note that <code>double</code> does <em>not</em> have <code>y</code> as argument. Variables are only
included in declarations where they are actually used.</p>
<h2><a class="header" href="#more-on-namespaces" id="more-on-namespaces">More on Namespaces</a></h2>
<p>In Lean, identifiers are given by hierarchical <em>names</em> like
<code>Foo.Bar.baz</code>. We saw in <a href="./dependent_type_theory.html#namespaces">Namespaces</a> that Lean provides
mechanisms for working with hierarchical names. The command
<code>namespace foo</code> causes <code>foo</code> to be prepended to the name of each
definition and theorem until <code>end foo</code> is encountered. The command
<code>open foo</code> then creates temporary <em>aliases</em> to definitions and
theorems that begin with prefix <code>foo</code>.</p>
<pre><code class="language-lean">namespace Foo
def bar : Nat := 1
end Foo

open Foo

#check bar
#check Foo.bar
</code></pre>
<p>The following definition</p>
<pre><code class="language-lean">def Foo.bar : Nat := 1
</code></pre>
<p>is treated as a macro, and expands to</p>
<pre><code class="language-lean">namespace Foo
def bar : Nat := 1
end Foo
</code></pre>
<p>Although the names of theorems and definitions have to be unique, the
aliases that identify them do not. When we open a namespace, an
identifier may be ambiguous. Lean tries to use type information to
disambiguate the meaning in context, but you can always disambiguate
by giving the full name. To that end, the string <code>_root_</code> is an
explicit description of the empty prefix.</p>
<pre><code class="language-lean">def String.add (a b : String) : String :=
  a ++ b

def Bool.add (a b : Bool) : Bool :=
  a != b

def add (α β : Type) : Type := Sum α β

open Bool
open String
-- #check add -- ambiguous
#check String.add           -- String → String → String
#check Bool.add             -- Bool → Bool → Bool
#check _root_.add           -- Type → Type → Type

#check add &quot;hello&quot; &quot;world&quot;  -- String
#check add true false       -- Bool
#check add Nat Nat          -- Type
</code></pre>
<p>We can prevent the shorter alias from being created by using the <code>protected</code> keyword:</p>
<pre><code class="language-lean">protected def Foo.bar : Nat := 1

open Foo

-- #check bar -- error
#check Foo.bar
</code></pre>
<p>This is often used for names like <code>Nat.rec</code> and <code>Nat.recOn</code>, to prevent
overloading of common names.</p>
<p>The <code>open</code> command admits variations. The command</p>
<pre><code class="language-lean">open Nat (succ zero gcd)
#check zero     -- Nat
#eval gcd 15 6  -- 3
</code></pre>
<p>creates aliases for only the identifiers listed. The command</p>
<pre><code class="language-lean">open Nat hiding succ gcd
#check zero     -- Nat
-- #eval gcd 15 6  -- error
#eval Nat.gcd 15 6  -- 3
</code></pre>
<p>creates aliases for everything in the <code>Nat</code> namespace <em>except</em> the identifiers listed.</p>
<pre><code class="language-lean">open Nat renaming mul → times, add → plus
#eval plus (times 2 2) 3  -- 7
</code></pre>
<p>creates aliases renaming <code>Nat.mul</code> to <code>times</code> and <code>Nat.add</code> to <code>plus</code>.</p>
<p>It is sometimes useful to <code>export</code> aliases from one namespace to another, or to the top level. The command</p>
<pre><code class="language-lean">export Nat (succ add sub)
</code></pre>
<p>creates aliases for <code>succ</code>, <code>add</code>, and <code>sub</code> in the current
namespace, so that whenever the namespace is open, these aliases are
available. If this command is used outside a namespace, the aliases
are exported to the top level.</p>
<h2><a class="header" href="#attributes" id="attributes">Attributes</a></h2>
<p>The main function of Lean is to translate user input to formal
expressions that are checked by the kernel for correctness and then
stored in the environment for later use. But some commands have other
effects on the environment, either assigning attributes to objects in
the environment, defining notation, or declaring instances of type
classes, as described in <a href="./type_classes.html">Chapter Type Classes</a>. Most of
these commands have global effects, which is to say, that they remain
in effect not only in the current file, but also in any file that
imports it. However, such commands often support the <code>local</code> modifier,
which indicates that they only have effect until
the current <code>section</code> or <code>namespace</code> is closed, or until the end
of the current file.</p>
<p>In <a href="./tactics.html#using-the-simplifier">Section Using the Simplifier</a>,
we saw that theorems can be annotated with the <code>[simp]</code> attribute,
which makes them available for use by the simplifier.
The following example defines the prefix relation on lists,
proves that this relation is reflexive, and assigns the <code>[simp]</code> attribute to that theorem.</p>
<pre><code class="language-lean">def isPrefix (l₁ : List α) (l₂ : List α) : Prop :=
  ∃ t, l₁ ++ t = l₂

@[simp] theorem List.isPrefix_self (as : List α) : isPrefix as as :=
  ⟨[], by simp⟩

example : isPrefix [1, 2, 3] [1, 2, 3] := by
  simp
</code></pre>
<p>The simplifier then proves <code>isPrefix [1, 2, 3] [1, 2, 3]</code> by rewriting it to <code>True</code>.</p>
<p>One can also assign the attribute any time after the definition takes place:</p>
<pre><code class="language-lean"><span class="boring">def isPrefix (l₁ : List α) (l₂ : List α) : Prop :=
</span><span class="boring"> ∃ t, l₁ ++ t = l₂
</span>theorem List.isPrefix_self (as : List α) : isPrefix as as :=
  ⟨[], by simp⟩

attribute [simp] List.isPrefix_self
</code></pre>
<p>In all these cases, the attribute remains in effect in any file that
imports the one in which the declaration occurs. Adding the <code>local</code>
modifier restricts the scope:</p>
<pre><code class="language-lean"><span class="boring">def isPrefix (l₁ : List α) (l₂ : List α) : Prop :=
</span><span class="boring"> ∃ t, l₁ ++ t = l₂
</span>section

theorem List.isPrefix_self (as : List α) : isPrefix as as :=
  ⟨[], by simp⟩

attribute [local simp] List.isPrefix_self

example : isPrefix [1, 2, 3] [1, 2, 3] := by
  simp

end

-- Error:
-- example : isPrefix [1, 2, 3] [1, 2, 3] := by
--  simp
</code></pre>
<p>For another example, we can use the <code>instance</code> command to assign the
notation <code>≤</code> to the <code>isPrefix</code> relation. That command, which will
be explained in <a href="./type_classes.html">Chapter Type Classes</a>, works by
assigning an <code>[instance]</code> attribute to the associated definition.</p>
<pre><code class="language-lean">def isPrefix (l₁ : List α) (l₂ : List α) : Prop :=
  ∃ t, l₁ ++ t = l₂

instance : LE (List α) where
  le := isPrefix

theorem List.isPrefix_self (as : List α) : as ≤ as :=
  ⟨[], by simp⟩
</code></pre>
<p>That assignment can also be made local:</p>
<pre><code class="language-lean"><span class="boring">def isPrefix (l₁ : List α) (l₂ : List α) : Prop :=
</span><span class="boring">  ∃ t, l₁ ++ t = l₂
</span>def instLe : LE (List α) :=
  { le := isPrefix }

section
attribute [local instance] instLe

example (as : List α) : as ≤ as :=
  ⟨[], by simp⟩

end

-- Error:
-- example (as : List α) : as ≤ as :=
--  ⟨[], by simp⟩
</code></pre>
<p>In <a href="interacting_with_lean.html#notation">Section Notation</a> below, we will discuss Lean's
mechanisms for defining notation, and see that they also support the
<code>local</code> modifier. However, in <a href="interacting_with_lean.html#setting-options">Section Setting Options</a>, we will
discuss Lean's mechanisms for setting options, which does <em>not</em> follow
this pattern: options can <em>only</em> be set locally, which is to say,
their scope is always restricted to the current section or current
file.</p>
<h2><a class="header" href="#more-on-implicit-arguments" id="more-on-implicit-arguments">More on Implicit Arguments</a></h2>
<p>In <a href="./dependent_type_theory.html#implicit-arguments">Section Implicit Arguments</a>,
we saw that if Lean displays the type
of a term <code>t</code> as <code>{x : α} → β x</code>, then the curly brackets
indicate that <code>x</code> has been marked as an <em>implicit argument</em> to
<code>t</code>. This means that whenever you write <code>t</code>, a placeholder, or
&quot;hole,&quot; is inserted, so that <code>t</code> is replaced by <code>@t _</code>. If you
don't want that to happen, you have to write <code>@t</code> instead.</p>
<p>Notice that implicit arguments are inserted eagerly. Suppose we define
a function <code>f (x : Nat) {y : Nat} (z : Nat)</code> with the arguments
shown. Then, when we write the expression <code>f 7</code> without further
arguments, it is parsed as <code>f 7 _</code>. Lean offers a weaker annotation,
<code>{{y : Nat}}</code>, which specifies that a placeholder should only be added
<em>before</em> a subsequent explicit argument. This annotation can also be
written using as <code>⦃y : Nat⦄</code>, where the unicode brackets are entered
as <code>\{{</code> and <code>\}}</code>, respectively. With this annotation, the
expression <code>f 7</code> would be parsed as is, whereas <code>f 7 3</code> would be
parsed as <code>f 7 _ 3</code>, just as it would be with the strong annotation.</p>
<p>To illustrate the difference, consider the following example, which
shows that a reflexive euclidean relation is both symmetric and
transitive.</p>
<pre><code class="language-lean">def reflexive {α : Type u} (r : α → α → Prop) : Prop :=
  ∀ (a : α), r a a

def symmetric {α : Type u} (r : α → α → Prop) : Prop :=
  ∀ {a b : α}, r a b → r b a

def transitive {α : Type u} (r : α → α → Prop) : Prop :=
  ∀ {a b c : α}, r a b → r b c → r a c

def euclidean {α : Type u} (r : α → α → Prop) : Prop :=
  ∀ {a b c : α}, r a b → r a c → r b c

theorem th1 {α : Type u} {r : α → α → Prop}
            (reflr : reflexive r) (euclr : euclidean r)
            : symmetric r :=
  fun {a b : α} =&gt;
  fun (h : r a b) =&gt;
  show r b a from euclr h (reflr _)

theorem th2 {α : Type u} {r : α → α → Prop}
            (symmr : symmetric r) (euclr : euclidean r)
            : transitive r :=
  fun {a b c : α} =&gt;
  fun (rab : r a b) (rbc : r b c) =&gt;
  euclr (symmr rab) rbc

theorem th3 {α : Type u} {r : α → α → Prop}
            (reflr : reflexive r) (euclr : euclidean r)
            : transitive r :=
 th2 (th1 reflr @euclr) @euclr

variable (r : α → α → Prop)
variable (euclr : euclidean r)

#check euclr  -- r ?m1 ?m2 → r ?m1 ?m3 → r ?m2 ?m3
</code></pre>
<p>The results are broken down into small steps: <code>th1</code> shows that a
relation that is reflexive and euclidean is symmetric, and <code>th2</code>
shows that a relation that is symmetric and euclidean is
transitive. Then <code>th3</code> combines the two results. But notice that we
have to manually disable the implicit arguments in <code>euclr</code>, because
otherwise too many implicit arguments are inserted. The problem goes
away if we use weak implicit arguments:</p>
<pre><code class="language-lean">def reflexive {α : Type u} (r : α → α → Prop) : Prop :=
  ∀ (a : α), r a a

def symmetric {α : Type u} (r : α → α → Prop) : Prop :=
  ∀ {{a b : α}}, r a b → r b a

def transitive {α : Type u} (r : α → α → Prop) : Prop :=
  ∀ {{a b c : α}}, r a b → r b c → r a c

def euclidean {α : Type u} (r : α → α → Prop) : Prop :=
  ∀ {{a b c : α}}, r a b → r a c → r b c

theorem th1 {α : Type u} {r : α → α → Prop}
            (reflr : reflexive r) (euclr : euclidean r)
            : symmetric r :=
  fun {a b : α} =&gt;
  fun (h : r a b) =&gt;
  show r b a from euclr h (reflr _)

theorem th2 {α : Type u} {r : α → α → Prop}
            (symmr : symmetric r) (euclr : euclidean r)
            : transitive r :=
  fun {a b c : α} =&gt;
  fun (rab : r a b) (rbc : r b c) =&gt;
  euclr (symmr rab) rbc

theorem th3 {α : Type u} {r : α → α → Prop}
            (reflr : reflexive r) (euclr : euclidean r)
            : transitive r :=
  th2 (th1 reflr euclr) euclr

variable (r : α → α → Prop)
variable (euclr : euclidean r)

#check euclr  -- euclidean r
</code></pre>
<p>There is a third kind of implicit argument that is denoted with square
brackets, <code>[</code> and <code>]</code>. These are used for type classes, as
explained in <a href="./type_classes.html">Chapter Type Classes</a>.</p>
<h2><a class="header" href="#notation" id="notation">Notation</a></h2>
<p>Identifiers in Lean can include any alphanumeric characters, including
Greek characters (other than ∀ , Σ , and λ , which, as we have seen,
have a special meaning in the dependent type theory). They can also
include subscripts, which can be entered by typing <code>\_</code> followed by
the desired subscripted character.</p>
<p>Lean's parser is extensible, which is to say, we can define new notation.</p>
<p>Lean's syntax can be extended and customized by users at every level,
ranging from basic &quot;mixfix&quot; notations to custom elaborators.  In fact,
all builtin syntax is parsed and processed using the same mechanisms
and APIs open to users.  In this section, we will describe and explain
the various extension points.</p>
<p>While introducing new notations is a relatively rare feature in
programming languages and sometimes even frowned upon because of its
potential to obscure code, it is an invaluable tool in formalization
for expressing established conventions and notations of the respective
field succinctly in code.  Going beyond basic notations, Lean's
ability to factor out common boilerplate code into (well-behaved)
macros and to embed entire custom domain specific languages (DSLs) to
textually encode subproblems efficiently and readably can be of great
benefit to both programmers and proof engineers alike.</p>
<h3><a class="header" href="#notations-and-precedence" id="notations-and-precedence">Notations and Precedence</a></h3>
<p>The most basic syntax extension commands allow introducing new (or
overloading existing) prefix, infix, and postfix operators.</p>
<pre><code class="language-lean">infixl:65   &quot; + &quot; =&gt; HAdd.hAdd  -- left-associative
infix:50    &quot; = &quot; =&gt; Eq         -- non-associative
infixr:80   &quot; ^ &quot; =&gt; HPow.hPow  -- right-associative
prefix:100  &quot;-&quot;   =&gt; Neg.neg
<span class="boring">set_option quotPrecheck false
</span>postfix:max &quot;⁻¹&quot;  =&gt; Inv.inv
</code></pre>
<p>After the initial command name describing the operator kind (its
&quot;fixity&quot;), we give the <em>parsing precedence</em> of the operator preceded
by a colon <code>:</code>, then a new or existing token surrounded by double
quotes (the whitespace is used for pretty printing), then the function
this operator should be translated to after the arrow <code>=&gt;</code>.</p>
<p>The precedence is a natural number describing how &quot;tightly&quot; an
operator binds to its arguments, encoding the order of operations.  We
can make this more precise by looking at the commands the above unfold to:</p>
<pre><code class="language-lean">notation:65 lhs:65 &quot; + &quot; rhs:66 =&gt; HAdd.hAdd lhs rhs
notation:50 lhs:51 &quot; = &quot; rhs:51 =&gt; Eq lhs rhs
notation:80 lhs:81 &quot; ^ &quot; rhs:80 =&gt; HPow.hPow lhs rhs
notation:100 &quot;-&quot; arg:100 =&gt; Neg.neg arg
<span class="boring">set_option quotPrecheck false
</span>notation:1024 arg:1024 &quot;⁻¹&quot; =&gt; Inv.inv arg  -- `max` is a shorthand for precedence 1024
</code></pre>
<p>It turns out that all commands from the first code block are in fact
command <em>macros</em> translating to the more general <code>notation</code> command.
We will learn about writing such macros below.  Instead of a single
token, the <code>notation</code> command accepts a mixed sequence of tokens and
named term placeholders with precedences, which can be referenced on
the right-hand side of <code>=&gt;</code> and will be replaced by the respective
term parsed at that position.  A placeholder with precedence <code>p</code>
accepts only notations with precedence at least <code>p</code> in that place.
Thus the string <code>a + b + c</code> cannot be parsed as the equivalent of
<code>a + (b + c)</code> because the right-hand side operand of an <code>infixl</code> notation
has precedence one greater than the notation itself.  In contrast,
<code>infixr</code> reuses the notation's precedence for the right-hand side
operand, so <code>a ^ b ^ c</code> <em>can</em> be parsed as <code>a ^ (b ^ c)</code>.  Note that
if we used <code>notation</code> directly to introduce an infix notation like</p>
<pre><code class="language-lean"><span class="boring">set_option quotPrecheck false
</span>notation:65 lhs:65 &quot; ~ &quot; rhs:65 =&gt; wobble lhs rhs
</code></pre>
<p>where the precedences do not sufficiently determine associativity,
Lean's parser will default to right associativity.  More precisely,
Lean's parser follows a local <em>longest parse</em> rule in the presence of
ambiguous grammars: when parsing the right-hand side of <code>a ~</code> in
<code>a ~ b ~ c</code>, it will continue parsing as long as possible (as the current
precedence allows), not stopping after <code>b</code> but parsing <code>~ c</code> as well.
Thus the term is equivalent to <code>a ~ (b ~ c)</code>.</p>
<p>As mentioned above, the <code>notation</code> command allows us to define
arbitrary <em>mixfix</em> syntax freely mixing tokens and placeholders.</p>
<pre><code class="language-lean"><span class="boring">set_option quotPrecheck false
</span>notation:max &quot;(&quot; e &quot;)&quot; =&gt; e
notation:10 Γ &quot; ⊢ &quot; e &quot; : &quot; τ =&gt; Typing Γ e τ
</code></pre>
<p>Placeholders without precedence default to <code>0</code>, i.e. they accept notations of any precedence in their place.
If two notations overlap, we again apply the longest parse rule:</p>
<pre><code class="language-lean">notation:65 a &quot; + &quot; b:66 &quot; + &quot; c:66 =&gt; a + b - c
#eval 1 + 2 + 3  -- 0
</code></pre>
<p>The new notation is preferred to the binary notation since the latter,
before chaining, would stop parsing after <code>1 + 2</code>.  If there are
multiple notations accepting the same longest parse, the choice will
be delayed until elaboration, which will fail unless exactly one
overload is type correct.</p>
<h2><a class="header" href="#coercions" id="coercions">Coercions</a></h2>
<p>In Lean, the type of natural numbers, <code>Nat</code>, is different from the
type of integers, <code>Int</code>. But there is a function <code>Int.ofNat</code> that
embeds the natural numbers in the integers, meaning that we can view
any natural number as an integer, when needed. Lean has mechanisms to
detect and insert <em>coercions</em> of this sort.</p>
<pre><code class="language-lean">variable (m n : Nat)
variable (i j : Int)

#check i + m      -- i + Int.ofNat m : Int
#check i + m + j  -- i + Int.ofNat m + j : Int
#check i + m + n  -- i + Int.ofNat m + Int.ofNat n : Int
</code></pre>
<h2><a class="header" href="#displaying-information" id="displaying-information">Displaying Information</a></h2>
<p>There are a number of ways in which you can query Lean for information
about its current state and the objects and theorems that are
available in the current context. You have already seen two of the
most common ones, <code>#check</code> and <code>#eval</code>. Remember that <code>#check</code>
is often used in conjunction with the <code>@</code> operator, which makes all
of the arguments to a theorem or definition explicit. In addition, you
can use the <code>#print</code> command to get information about any
identifier. If the identifier denotes a definition or theorem, Lean
prints the type of the symbol, and its definition. If it is a constant
or an axiom, Lean indicates that fact, and shows the type.</p>
<pre><code class="language-lean">-- examples with equality
#check Eq
#check @Eq
#check Eq.symm
#check @Eq.symm

#print Eq.symm

-- examples with And
#check And
#check And.intro
#check @And.intro

-- a user-defined function
def foo {α : Type u} (x : α) : α := x

#check foo
#check @foo
#print foo
</code></pre>
<h2><a class="header" href="#setting-options" id="setting-options">Setting Options</a></h2>
<p>Lean maintains a number of internal variables that can be set by users
to control its behavior. The syntax for doing so is as follows:</p>
<pre><code>set_option &lt;name&gt; &lt;value&gt;
</code></pre>
<p>One very useful family of options controls the way Lean's <em>pretty- printer</em> displays terms. The following options take an input of true or false:</p>
<pre><code>pp.explicit  : display implicit arguments
pp.universes : display hidden universe parameters
pp.notation  : display output using defined notations
</code></pre>
<p>As an example, the following settings yield much longer output:</p>
<pre><code class="language-lean">set_option pp.explicit true
set_option pp.universes true
set_option pp.notation false

#check 2 + 2 = 4
#reduce (fun x =&gt; x + 2) = (fun x =&gt; x + 3)
#check (fun x =&gt; x + 1) 1
</code></pre>
<p>The command <code>set_option pp.all true</code> carries out these settings all
at once, whereas <code>set_option pp.all false</code> reverts to the previous
values. Pretty printing additional information is often very useful
when you are debugging a proof, or trying to understand a cryptic
error message. Too much information can be overwhelming, though, and
Lean's defaults are generally sufficient for ordinary interactions.</p>
<!--
Elaboration Hints
-----------------

When you ask Lean to process an expression like ``λ x y z, f (x + y) z``, you are leaving information implicit. For example, the types of ``x``, ``y``, and ``z`` have to be inferred from the context, the notation ``+`` may be overloaded, and there may be implicit arguments to ``f`` that need to be filled in as well. Moreover, we will see in :numref:`Chapter %s <type_classes>` that some implicit arguments are synthesized by a process known as *type class resolution*. And we have also already seen in the last chapter that some parts of an expression can be constructed by the tactic framework.

Inferring some implicit arguments is straightforward. For example, suppose a function ``f`` has type ``Π {α : Type*}, α → α → α`` and Lean is trying to parse the expression ``f n``, where ``n`` can be inferred to have type ``nat``. Then it is clear that the implicit argument ``α`` has to be ``nat``. However, some inference problems are *higher order*. For example, the substitution operation for equality, ``eq.subst``, has the following type:

.. code-block:: text

    eq.subst : ∀ {α : Sort u} {p : α → Prop} {a b : α},
                 a = b → p a → p b

Now suppose we are given ``a b : ℕ`` and ``h₁ : a = b`` and ``h₂ : a * b > a``. Then, in the expression ``eq.subst h₁ h₂``, ``P`` could be any of the following:

-  ``λ x, x * b > x``
-  ``λ x, x * b > a``
-  ``λ x, a * b > x``
-  ``λ x, a * b > a``

In other words, our intent may be to replace either the first or second ``a`` in ``h₂``, or both, or neither. Similar ambiguities arise in inferring induction predicates, or inferring function arguments. Even second-order unification is known to be undecidable. Lean therefore relies on heuristics to fill in such arguments, and when it fails to guess the right ones, they need to be provided explicitly.

To make matters worse, sometimes definitions need to be unfolded, and sometimes expressions need to be reduced according to the computational rules of the underlying logical framework. Once again, Lean has to rely on heuristics to determine what to unfold or reduce, and when.

There are attributes, however, that can be used to provide hints to the elaborator. One class of attributes determines how eagerly definitions are unfolded: constants can be marked with the attribute ``[reducible]``, ``[semireducible]``, or ``[irreducible]``. Definitions are marked ``[semireducible]`` by default. A definition with the ``[reducible]`` attribute is unfolded eagerly; if you think of a definition as serving as an abbreviation, this attribute would be appropriate. The elaborator avoids unfolding definitions with the ``[irreducible]`` attribute. Theorems are marked ``[irreducible]`` by default, because typically proofs are not relevant to the elaboration process.

It is worth emphasizing that these attributes are only hints to the elaborator. When checking an elaborated term for correctness, Lean's kernel will unfold whatever definitions it needs to unfold. As with other attributes, the ones above can be assigned with the ``local`` modifier, so that they are in effect only in the current section or file.

Lean also has a family of attributes that control the elaboration strategy. A definition or theorem can be marked ``[elab_with_expected_type]``, ``[elab_simple]``. or ``[elab_as_eliminator]``. When applied to a definition ``f``, these bear on elaboration of an expression ``f a b c ...`` in which ``f`` is applied to arguments. With the default attribute, ``[elab_with_expected_type]``, the arguments ``a``, ``b``, ``c``, ... are elaborating using information about their expected type, inferred from ``f`` and the previous arguments. In contrast, with ``[elab_simple]``, the arguments are elaborated from left to right without propagating information about their types. The last attribute, ``[elab_as_eliminator]``, is commonly used for eliminators like recursors, induction principles, and ``eq.subst``. It uses a separate heuristic to infer higher-order parameters. We will consider such operations in more detail in the next chapter.

Once again, these attributes can be assigned and reassigned after an object is defined, and you can use the ``local`` modifier to limit their scope. Moreover, using the ``@`` symbol in front of an identifier in an expression instructs the elaborator to use the ``[elab_simple]`` strategy; the idea is that, when you provide the tricky parameters explicitly, you want the elaborator to weigh that information heavily. In fact, Lean offers an alternative annotation, ``@@``, which leaves parameters before the first higher-order parameter implicit. For example, ``@@eq.subst`` leaves the type of the equation implicit, but makes the context of the substitution explicit.

-->
<h2><a class="header" href="#using-the-library" id="using-the-library">Using the Library</a></h2>
<p>To use Lean effectively you will inevitably need to make use of
definitions and theorems in the library. Recall that the <code>import</code>
command at the beginning of a file imports previously compiled results
from other files, and that importing is transitive; if you import
<code>Foo</code> and <code>Foo</code> imports <code>Bar</code>, then the definitions and theorems
from <code>Bar</code> are available to you as well. But the act of opening a
namespace, which provides shorter names, does not carry over. In each
file, you need to open the namespaces you wish to use.</p>
<p>In general, it is important for you to be familiar with the library
and its contents, so you know what theorems, definitions, notations,
and resources are available to you. Below we will see that Lean's
editor modes can also help you find things you need, but studying the
contents of the library directly is often unavoidable. Lean's standard
library can be found online, on GitHub:</p>
<ul>
<li>
<p><a href="https://github.com/leanprover/lean4/tree/master/src/Init">https://github.com/leanprover/lean4/tree/master/src/Init</a></p>
</li>
<li>
<p><a href="https://github.com/leanprover/std4/tree/main/Std">https://github.com/leanprover/std4/tree/main/Std</a></p>
</li>
</ul>
<p>You can see the contents of these directories and files using GitHub's
browser interface. If you have installed Lean on your own computer,
you can find the library in the <code>lean</code> folder, and explore it with
your file manager. Comment headers at the top of each file provide
additional information.</p>
<p>Lean's library developers follow general naming guidelines to make it
easier to guess the name of a theorem you need, or to find it using
tab completion in editors with a Lean mode that supports this, which
is discussed in the next section. Identifiers are generally
<code>camelCase</code>, and types are <code>CamelCase</code>. For theorem names,
we rely on descriptive names where the different components are separated
by <code>_</code>s. Often the name of theorem simply describes the conclusion:</p>
<pre><code class="language-lean">#check Nat.succ_ne_zero
#check Nat.zero_add
#check Nat.mul_one
#check Nat.le_of_succ_le_succ
</code></pre>
<p>Remember that identifiers in Lean can be organized into hierarchical
namespaces. For example, the theorem named <code>le_of_succ_le_succ</code> in the
namespace <code>Nat</code> has full name <code>Nat.le_of_succ_le_succ</code>, but the shorter
name is made available by the command <code>open Nat</code> (for names not marked as
<code>protected</code>). We will see in <a href="./inductive_types.html">Chapter Inductive Types</a>
and <a href="./structures_and_records.html">Chapter Structures and Records</a>
that defining structures and inductive data types in Lean generates
associated operations, and these are stored in
a namespace with the same name as the type under definition. For
example, the product type comes with the following operations:</p>
<pre><code class="language-lean">#check @Prod.mk
#check @Prod.fst
#check @Prod.snd
#check @Prod.rec
</code></pre>
<p>The first is used to construct a pair, whereas the next two,
<code>Prod.fst</code> and <code>Prod.snd</code>, project the two elements. The last,
<code>Prod.rec</code>, provides another mechanism for defining functions on a
product in terms of a function on the two components. Names like
<code>Prod.rec</code> are <em>protected</em>, which means that one has to use the full
name even when the <code>Prod</code> namespace is open.</p>
<p>With the propositions as types correspondence, logical connectives are
also instances of inductive types, and so we tend to use dot notation
for them as well:</p>
<pre><code class="language-lean">#check @And.intro
#check @And.casesOn
#check @And.left
#check @And.right
#check @Or.inl
#check @Or.inr
#check @Or.elim
#check @Exists.intro
#check @Exists.elim
#check @Eq.refl
#check @Eq.subst
</code></pre>
<h2><a class="header" href="#auto-bound-implicit-arguments" id="auto-bound-implicit-arguments">Auto Bound Implicit Arguments</a></h2>
<p>In the previous section, we have shown how implicit arguments make functions more convenient to use.
However, functions such as <code>compose</code> are still quite verbose to define. Note that the universe
polymorphic <code>compose</code> is even more verbose than the one previously defined.</p>
<pre><code class="language-lean">universe u v w
def compose {α : Type u} {β : Type v} {γ : Type w}
            (g : β → γ) (f : α → β) (x : α) : γ :=
  g (f x)
</code></pre>
<p>You can avoid the <code>universe</code> command by providing the universe parameters when defining <code>compose</code>.</p>
<pre><code class="language-lean">def compose.{u, v, w}
            {α : Type u} {β : Type v} {γ : Type w}
            (g : β → γ) (f : α → β) (x : α) : γ :=
  g (f x)
</code></pre>
<p>Lean 4 supports a new feature called <em>auto bound implicit arguments</em>. It makes functions such as
<code>compose</code> much more convenient to write. When Lean processes the header of a declaration,
any unbound identifier is automatically added as an implicit argument <em>if</em> it is a single lower case or
greek letter. With this feature we can write <code>compose</code> as</p>
<pre><code class="language-lean">def compose (g : β → γ) (f : α → β) (x : α) : γ :=
  g (f x)

#check @compose
-- {β : Sort u_1} → {γ : Sort u_2} → {α : Sort u_3} → (β → γ) → (α → β) → α → γ
</code></pre>
<p>Note that Lean inferred a more general type using <code>Sort</code> instead of <code>Type</code>.</p>
<p>Although we love this feature and use it extensively when implementing Lean,
we realize some users may feel uncomfortable with it. Thus, you can disable it using
the command <code>set_option autoImplicit false</code>.</p>
<pre><code class="language-lean">set_option autoImplicit false
/- The following definition produces `unknown identifier` errors -/
-- def compose (g : β → γ) (f : α → β) (x : α) : γ :=
--   g (f x)
</code></pre>
<h2><a class="header" href="#implicit-lambdas" id="implicit-lambdas">Implicit Lambdas</a></h2>
<p>In Lean 3 stdlib, we find many
<a href="https://github.com/leanprover/lean/blob/master/library/init/category/reader.lean#L39">instances</a> of the dreadful <code>@</code>+<code>_</code> idiom.
It is often used when the expected type is a function type with implicit arguments,
and we have a constant (<code>reader_t.pure</code> in the example) which also takes implicit arguments. In Lean 4, the elaborator automatically introduces lambdas
for consuming implicit arguments. We are still exploring this feature and analyzing its impact, but the experience so far has been very positive. Here is the example from the link above using Lean 4 implicit lambdas.</p>
<pre><code class="language-lean"><span class="boring">variable (ρ : Type) (m : Type → Type) [Monad m]
</span>instance : Monad (ReaderT ρ m) where
  pure := ReaderT.pure
  bind := ReaderT.bind
</code></pre>
<p>Users can disable the implicit lambda feature by using <code>@</code> or writing
a lambda expression with <code>{}</code> or <code>[]</code> binder annotations.  Here are
few examples</p>
<pre><code class="language-lean"><span class="boring">namespace ex2
</span>def id1 : {α : Type} → α → α :=
  fun x =&gt; x

def listId : List ({α : Type} → α → α) :=
  (fun x =&gt; x) :: []

-- In this example, implicit lambda introduction has been disabled because
-- we use `@` before `fun`
def id2 : {α : Type} → α → α :=
  @fun α (x : α) =&gt; id1 x

def id3 : {α : Type} → α → α :=
  @fun α x =&gt; id1 x

def id4 : {α : Type} → α → α :=
  fun x =&gt; id1 x

-- In this example, implicit lambda introduction has been disabled
-- because we used the binder annotation `{...}`
def id5 : {α : Type} → α → α :=
  fun {α} x =&gt; id1 x
<span class="boring">end ex2
</span></code></pre>
<h2><a class="header" href="#sugar-for-simple-functions" id="sugar-for-simple-functions">Sugar for Simple Functions</a></h2>
<p>In Lean 3, we can create simple functions from infix operators by
using parentheses. For example, <code>(+1)</code> is sugar for <code>fun x, x + 1</code>. In
Lean 4, we generalize this notation using <code>·</code> as a placeholder. Here
are a few examples:</p>
<pre><code class="language-lean"><span class="boring">namespace ex3
</span>#check (· + 1)
-- fun a =&gt; a + 1
#check (2 - ·)
-- fun a =&gt; 2 - a
#eval [1, 2, 3, 4, 5].foldl (·*·) 1
-- 120

def f (x y z : Nat) :=
  x + y + z

#check (f · 1 ·)
-- fun a b =&gt; f a 1 b

#eval [(1, 2), (3, 4), (5, 6)].map (·.1)
-- [1, 3, 5]
<span class="boring">end ex3
</span></code></pre>
<p>As in Lean 3, the notation is activated using parentheses, and the lambda abstraction is created by collecting the nested <code>·</code>s.
The collection is interrupted by nested parentheses. In the following example, two different lambda expressions are created.</p>
<pre><code class="language-lean">#check (Prod.mk · (· + 1))
-- fun a =&gt; (a, fun b =&gt; b + 1)
</code></pre>
<h2><a class="header" href="#named-arguments" id="named-arguments">Named Arguments</a></h2>
<p>Named arguments enable you to specify an argument for a parameter by
matching the argument with its name rather than with its position in
the parameter list.  If you don't remember the order of the parameters
but know their names, you can send the arguments in any order. You may
also provide the value for an implicit parameter when Lean failed to
infer it. Named arguments also improve the readability of your code by
identifying what each argument represents.</p>
<pre><code class="language-lean">def sum (xs : List Nat) :=
  xs.foldl (init := 0) (·+·)

#eval sum [1, 2, 3, 4]
-- 10

example {a b : Nat} {p : Nat → Nat → Nat → Prop} (h₁ : p a b b) (h₂ : b = a)
    : p a a b :=
  Eq.subst (motive := fun x =&gt; p a x b) h₂ h₁
</code></pre>
<p>In the following examples, we illustrate the interaction between named
and default arguments.</p>
<pre><code class="language-lean">def f (x : Nat) (y : Nat := 1) (w : Nat := 2) (z : Nat) :=
  x + y + w - z

example (x z : Nat) : f (z := z) x = x + 1 + 2 - z := rfl

example (x z : Nat) : f x (z := z) = x + 1 + 2 - z := rfl

example (x y : Nat) : f x y = fun z =&gt; x + y + 2 - z := rfl

example : f = (fun x z =&gt; x + 1 + 2 - z) := rfl

example (x : Nat) : f x = fun z =&gt; x + 1 + 2 - z := rfl

example (y : Nat) : f (y := 5) = fun x z =&gt; x + 5 + 2 - z := rfl

def g {α} [Add α] (a : α) (b? : Option α := none) (c : α) : α :=
  match b? with
  | none   =&gt; a + c
  | some b =&gt; a + b + c

variable {α} [Add α]

example : g = fun (a c : α) =&gt; a + c := rfl

example (x : α) : g (c := x) = fun (a : α) =&gt; a + x := rfl

example (x : α) : g (b? := some x) = fun (a c : α) =&gt; a + x + c := rfl

example (x : α) : g x = fun (c : α) =&gt; x + c := rfl

example (x y : α) : g x y = fun (c : α) =&gt; x + y + c := rfl
</code></pre>
<p>You can use <code>..</code> to provide missing explicit arguments as <code>_</code>.
This feature combined with named arguments is useful for writing patterns. Here is an example:</p>
<pre><code class="language-lean">inductive Term where
  | var    (name : String)
  | num    (val : Nat)
  | app    (fn : Term) (arg : Term)
  | lambda (name : String) (type : Term) (body : Term)

def getBinderName : Term → Option String
  | Term.lambda (name := n) .. =&gt; some n
  | _ =&gt; none

def getBinderType : Term → Option Term
  | Term.lambda (type := t) .. =&gt; some t
  | _ =&gt; none
</code></pre>
<p>Ellipses are also useful when explicit arguments can be automatically
inferred by Lean, and we want to avoid a sequence of <code>_</code>s.</p>
<pre><code class="language-lean">example (f : Nat → Nat) (a b c : Nat) : f (a + b + c) = f (a + (b + c)) :=
  congrArg f (Nat.add_assoc ..)
</code></pre>
<h1><a class="header" href="#inductive-types" id="inductive-types">Inductive Types</a></h1>
<p>We have seen that Lean's formal foundation includes basic types,
<code>Prop, Type 0, Type 1, Type 2, ...</code>, and allows for the formation of
dependent function types, <code>(x : α) → β</code>. In the examples, we have
also made use of additional types like <code>Bool</code>, <code>Nat</code>, and <code>Int</code>,
and type constructors, like <code>List</code>, and product, <code>×</code>. In fact, in
Lean's library, every concrete type other than the universes and every
type constructor other than dependent arrows is an instance of a general family of
type constructions known as <em>inductive types</em>. It is remarkable that
it is possible to construct a substantial edifice of mathematics based
on nothing more than the type universes, dependent arrow types, and inductive
types; everything else follows from those.</p>
<p>Intuitively, an inductive type is built up from a specified list of
constructors. In Lean, the syntax for specifying such a type is as
follows:</p>
<pre><code>inductive Foo where
  | constructor₁ : ... → Foo
  | constructor₂ : ... → Foo
  ...
  | constructorₙ : ... → Foo
</code></pre>
<p>The intuition is that each constructor specifies a way of building new
objects of <code>Foo</code>, possibly from previously constructed values. The
type <code>Foo</code> consists of nothing more than the objects that are
constructed in this way. The first character <code>|</code> in an inductive
declaration is optional. We can also separate constructors using a
comma instead of <code>|</code>.</p>
<p>We will see below that the arguments of the constructors can include
objects of type <code>Foo</code>, subject to a certain &quot;positivity&quot; constraint,
which guarantees that elements of <code>Foo</code> are built from the bottom
up. Roughly speaking, each <code>...</code> can be any arrow type constructed from
<code>Foo</code> and previously defined types, in which <code>Foo</code> appears, if at
all, only as the &quot;target&quot; of the dependent arrow type.</p>
<p>We will provide a number of examples of inductive types. We will also
consider slight generalizations of the scheme above, to mutually
defined inductive types, and so-called <em>inductive families</em>.</p>
<p>As with the logical connectives, every inductive type comes with
introduction rules, which show how to construct an element of the
type, and elimination rules, which show how to &quot;use&quot; an element of the
type in another construction. The analogy to the logical connectives
should not come as a surprise; as we will see below, they, too, are
examples of inductive type constructions. You have already seen the
introduction rules for an inductive type: they are just the
constructors that are specified in the definition of the type. The
elimination rules provide for a principle of recursion on the type,
which includes, as a special case, a principle of induction as well.</p>
<p>In the next chapter, we will describe Lean's function definition
package, which provides even more convenient ways to define functions
on inductive types and carry out inductive proofs. But because the
notion of an inductive type is so fundamental, we feel it is important
to start with a low-level, hands-on understanding. We will start with
some basic examples of inductive types, and work our way up to more
elaborate and complex examples.</p>
<h2><a class="header" href="#enumerated-types" id="enumerated-types">Enumerated Types</a></h2>
<p>The simplest kind of inductive type is a type with a finite, enumerated list of elements.</p>
<pre><code class="language-lean">inductive Weekday where
  | sunday : Weekday
  | monday : Weekday
  | tuesday : Weekday
  | wednesday : Weekday
  | thursday : Weekday
  | friday : Weekday
  | saturday : Weekday
</code></pre>
<p>The <code>inductive</code> command creates a new type, <code>Weekday</code>. The
constructors all live in the <code>Weekday</code> namespace.</p>
<pre><code class="language-lean"><span class="boring">inductive Weekday where
</span><span class="boring"> | sunday : Weekday
</span><span class="boring"> | monday : Weekday
</span><span class="boring"> | tuesday : Weekday
</span><span class="boring"> | wednesday : Weekday
</span><span class="boring"> | thursday : Weekday
</span><span class="boring"> | friday : Weekday
</span><span class="boring"> | saturday : Weekday
</span>#check Weekday.sunday
#check Weekday.monday

open Weekday

#check sunday
#check monday
</code></pre>
<p>You can omit <code>: Weekday</code> when declaring the <code>Weekday</code> inductive type.</p>
<pre><code class="language-lean">inductive Weekday where
  | sunday
  | monday
  | tuesday
  | wednesday
  | thursday
  | friday
  | saturday
</code></pre>
<p>Think of <code>sunday</code>, <code>monday</code>, ... , <code>saturday</code> as
being distinct elements of <code>Weekday</code>, with no other distinguishing
properties. The elimination principle, <code>Weekday.rec</code>, is defined
along with the type <code>Weekday</code> and its constructors. It is also known
as a <em>recursor</em>, and it is what makes the type &quot;inductive&quot;: it allows
us to define a function on <code>Weekday</code> by assigning values
corresponding to each constructor. The intuition is that an inductive
type is exhaustively generated by the constructors, and has no
elements beyond those they construct.</p>
<p>We will use the <code>match</code> expression to define a function from <code>Weekday</code>
to the natural numbers:</p>
<pre><code class="language-lean"><span class="boring">inductive Weekday where
</span><span class="boring"> | sunday : Weekday
</span><span class="boring"> | monday : Weekday
</span><span class="boring"> | tuesday : Weekday
</span><span class="boring"> | wednesday : Weekday
</span><span class="boring"> | thursday : Weekday
</span><span class="boring"> | friday : Weekday
</span><span class="boring"> | saturday : Weekday
</span>open Weekday

def numberOfDay (d : Weekday) : Nat :=
  match d with
  | sunday    =&gt; 1
  | monday    =&gt; 2
  | tuesday   =&gt; 3
  | wednesday =&gt; 4
  | thursday  =&gt; 5
  | friday    =&gt; 6
  | saturday  =&gt; 7

#eval numberOfDay Weekday.sunday  -- 1
#eval numberOfDay Weekday.monday  -- 2
#eval numberOfDay Weekday.tuesday -- 3
</code></pre>
<p>Note that the <code>match</code> expression is compiled using the <em>recursor</em> <code>Weekday.rec</code> generated when
you declare the inductive type.</p>
<pre><code class="language-lean"><span class="boring">inductive Weekday where
</span><span class="boring"> | sunday : Weekday
</span><span class="boring"> | monday : Weekday
</span><span class="boring"> | tuesday : Weekday
</span><span class="boring"> | wednesday : Weekday
</span><span class="boring"> | thursday : Weekday
</span><span class="boring"> | friday : Weekday
</span><span class="boring"> | saturday : Weekday
</span>open Weekday

def numberOfDay (d : Weekday) : Nat :=
  match d with
  | sunday    =&gt; 1
  | monday    =&gt; 2
  | tuesday   =&gt; 3
  | wednesday =&gt; 4
  | thursday  =&gt; 5
  | friday    =&gt; 6
  | saturday  =&gt; 7

set_option pp.all true
#print numberOfDay
-- ... numberOfDay.match_1
#print numberOfDay.match_1
-- ... Weekday.casesOn ...
#print Weekday.casesOn
-- ... Weekday.rec ...
#check @Weekday.rec
/-
@Weekday.rec.{u}
 : {motive : Weekday → Sort u} →
    motive Weekday.sunday →
    motive Weekday.monday →
    motive Weekday.tuesday →
    motive Weekday.wednesday →
    motive Weekday.thursday →
    motive Weekday.friday →
    motive Weekday.saturday →
    (t : Weekday) → motive t
-/
</code></pre>
<p>When declaring an inductive datatype, you can use <code>deriving Repr</code> to instruct
Lean to generate a function that converts <code>Weekday</code> objects into text.
This function is used by the <code>#eval</code> command to display <code>Weekday</code> objects.</p>
<pre><code class="language-lean">inductive Weekday where
  | sunday
  | monday
  | tuesday
  | wednesday
  | thursday
  | friday
  | saturday
  deriving Repr

open Weekday

#eval tuesday   -- Weekday.tuesday
</code></pre>
<p>It is often useful to group definitions and theorems related to a
structure in a namespace with the same name. For example, we can put
the <code>numberOfDay</code> function in the <code>Weekday</code> namespace. We are
then allowed to use the shorter name when we open the namespace.</p>
<p>We can define functions from <code>Weekday</code> to <code>Weekday</code>:</p>
<pre><code class="language-lean"><span class="boring">inductive Weekday where
</span><span class="boring"> | sunday : Weekday
</span><span class="boring"> | monday : Weekday
</span><span class="boring"> | tuesday : Weekday
</span><span class="boring"> | wednesday : Weekday
</span><span class="boring"> | thursday : Weekday
</span><span class="boring"> | friday : Weekday
</span><span class="boring"> | saturday : Weekday
</span><span class="boring"> deriving Repr
</span>namespace Weekday
def next (d : Weekday) : Weekday :=
  match d with
  | sunday    =&gt; monday
  | monday    =&gt; tuesday
  | tuesday   =&gt; wednesday
  | wednesday =&gt; thursday
  | thursday  =&gt; friday
  | friday    =&gt; saturday
  | saturday  =&gt; sunday

def previous (d : Weekday) : Weekday :=
  match d with
  | sunday    =&gt; saturday
  | monday    =&gt; sunday
  | tuesday   =&gt; monday
  | wednesday =&gt; tuesday
  | thursday  =&gt; wednesday
  | friday    =&gt; thursday
  | saturday  =&gt; friday

#eval next (next tuesday)      -- Weekday.thursday
#eval next (previous tuesday)  -- Weekday.tuesday

example : next (previous tuesday) = tuesday :=
  rfl

end Weekday
</code></pre>
<p>How can we prove the general theorem that <code>next (previous d) = d</code>
for any Weekday <code>d</code>? You can use <code>match</code> to provide a proof of the claim for each
constructor:</p>
<pre><code class="language-lean"><span class="boring">inductive Weekday where
</span><span class="boring"> | sunday : Weekday
</span><span class="boring"> | monday : Weekday
</span><span class="boring"> | tuesday : Weekday
</span><span class="boring"> | wednesday : Weekday
</span><span class="boring"> | thursday : Weekday
</span><span class="boring"> | friday : Weekday
</span><span class="boring"> | saturday : Weekday
</span><span class="boring"> deriving Repr
</span><span class="boring">namespace Weekday
</span><span class="boring">def next (d : Weekday) : Weekday :=
</span><span class="boring"> match d with
</span><span class="boring"> | sunday    =&gt; monday
</span><span class="boring"> | monday    =&gt; tuesday
</span><span class="boring"> | tuesday   =&gt; wednesday
</span><span class="boring"> | wednesday =&gt; thursday
</span><span class="boring"> | thursday  =&gt; friday
</span><span class="boring"> | friday    =&gt; saturday
</span><span class="boring"> | saturday  =&gt; sunday
</span><span class="boring">def previous (d : Weekday) : Weekday :=
</span><span class="boring"> match d with
</span><span class="boring"> | sunday    =&gt; saturday
</span><span class="boring"> | monday    =&gt; sunday
</span><span class="boring"> | tuesday   =&gt; monday
</span><span class="boring"> | wednesday =&gt; tuesday
</span><span class="boring"> | thursday  =&gt; wednesday
</span><span class="boring"> | friday    =&gt; thursday
</span><span class="boring"> | saturday  =&gt; friday
</span>def next_previous (d : Weekday) : next (previous d) = d :=
  match d with
  | sunday    =&gt; rfl
  | monday    =&gt; rfl
  | tuesday   =&gt; rfl
  | wednesday =&gt; rfl
  | thursday  =&gt; rfl
  | friday    =&gt; rfl
  | saturday  =&gt; rfl
</code></pre>
<p>Using a tactic proof, we can be even more concise:</p>
<pre><code class="language-lean"><span class="boring">inductive Weekday where
</span><span class="boring"> | sunday : Weekday
</span><span class="boring"> | monday : Weekday
</span><span class="boring"> | tuesday : Weekday
</span><span class="boring"> | wednesday : Weekday
</span><span class="boring"> | thursday : Weekday
</span><span class="boring"> | friday : Weekday
</span><span class="boring"> | saturday : Weekday
</span><span class="boring"> deriving Repr
</span><span class="boring">namespace Weekday
</span><span class="boring">def next (d : Weekday) : Weekday :=
</span><span class="boring"> match d with
</span><span class="boring"> | sunday    =&gt; monday
</span><span class="boring"> | monday    =&gt; tuesday
</span><span class="boring"> | tuesday   =&gt; wednesday
</span><span class="boring"> | wednesday =&gt; thursday
</span><span class="boring"> | thursday  =&gt; friday
</span><span class="boring"> | friday    =&gt; saturday
</span><span class="boring"> | saturday  =&gt; sunday
</span><span class="boring">def previous (d : Weekday) : Weekday :=
</span><span class="boring"> match d with
</span><span class="boring"> | sunday    =&gt; saturday
</span><span class="boring"> | monday    =&gt; sunday
</span><span class="boring"> | tuesday   =&gt; monday
</span><span class="boring"> | wednesday =&gt; tuesday
</span><span class="boring"> | thursday  =&gt; wednesday
</span><span class="boring"> | friday    =&gt; thursday
</span><span class="boring"> | saturday  =&gt; friday
</span>def next_previous (d : Weekday) : next (previous d) = d := by
  cases d &lt;;&gt; rfl
</code></pre>
<p><a href="inductive_types.html#tactics-for-inductive-types">Tactics for Inductive Types</a> below will introduce additional
tactics that are specifically designed to make use of inductive types.</p>
<p>Notice that, under the propositions-as-types correspondence, we can
use <code>match</code> to prove theorems as well as define functions.  In other
words, under the propositions-as-types correspondence, the proof by
cases is a kind of definition by cases, where what is being &quot;defined&quot;
is a proof instead of a piece of data.</p>
<p>The <code>Bool</code> type in the Lean library is an instance of
enumerated type.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive Bool where
  | false : Bool
  | true  : Bool
<span class="boring">end Hidden
</span></code></pre>
<p>(To run these examples, we put them in a namespace called <code>Hidden</code>,
so that a name like <code>Bool</code> does not conflict with the <code>Bool</code> in
the standard library. This is necessary because these types are part
of the Lean &quot;prelude&quot; that is automatically imported when the system
is started.)</p>
<p>As an exercise, you should think about what the introduction and
elimination rules for these types do. As a further exercise, we
suggest defining boolean operations <code>and</code>, <code>or</code>, <code>not</code> on the
<code>Bool</code> type, and verifying common identities. Note that you can define a
binary operation like <code>and</code> using <code>match</code>:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>def and (a b : Bool) : Bool :=
  match a with
  | true  =&gt; b
  | false =&gt; false
<span class="boring">end Hidden
</span></code></pre>
<p>Similarly, most identities can be proved by introducing suitable <code>match</code>, and then using <code>rfl</code>.</p>
<h2><a class="header" href="#constructors-with-arguments" id="constructors-with-arguments">Constructors with Arguments</a></h2>
<p>Enumerated types are a very special case of inductive types, in which
the constructors take no arguments at all. In general, a
&quot;construction&quot; can depend on data, which is then represented in the
constructed argument. Consider the definitions of the product type and
sum type in the library:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive Prod (α : Type u) (β : Type v)
  | mk : α → β → Prod α β

inductive Sum (α : Type u) (β : Type v) where
  | inl : α → Sum α β
  | inr : β → Sum α β
<span class="boring">end Hidden
</span></code></pre>
<p>Consider what is going on in these examples.
The product type has one constructor, <code>Prod.mk</code>,
which takes two arguments. To define a function on <code>Prod α β</code>, we
can assume the input is of the form <code>Prod.mk a b</code>, and we have to
specify the output, in terms of <code>a</code> and <code>b</code>. We can use this to
define the two projections for <code>Prod</code>. Remember that the standard
library defines notation <code>α × β</code> for <code>Prod α β</code> and <code>(a, b)</code> for
<code>Prod.mk a b</code>.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span><span class="boring">inductive Prod (α : Type u) (β : Type v)
</span><span class="boring">  | mk : α → β → Prod α β
</span>def fst {α : Type u} {β : Type v} (p : Prod α β) : α :=
  match p with
  | Prod.mk a b =&gt; a

def snd {α : Type u} {β : Type v} (p : Prod α β) : β :=
  match p with
  | Prod.mk a b =&gt; b
<span class="boring">end Hidden
</span></code></pre>
<p>The function <code>fst</code> takes a pair, <code>p</code>. The <code>match</code> interprets
<code>p</code> as a pair, <code>Prod.mk a b</code>. Recall also from <a href="./dependent_type_theory.html">Dependent Type Theory</a>
that to give these definitions the greatest generality possible, we allow
the types <code>α</code> and <code>β</code> to belong to any universe.</p>
<p>Here is another example where we use the recursor <code>Prod.casesOn</code> instead
of <code>match</code>.</p>
<pre><code class="language-lean">def prod_example (p : Bool × Nat) : Nat :=
  Prod.casesOn (motive := fun _ =&gt; Nat) p (fun b n =&gt; cond b (2 * n) (2 * n + 1))

#eval prod_example (true, 3)
#eval prod_example (false, 3)
</code></pre>
<p>The argument <code>motive</code> is used to specify the type of the object you want to
construct, and it is a function because it may depend on the pair.
The <code>cond</code> function is a boolean conditional: <code>cond b t1 t2</code>
returns <code>t1</code> if <code>b</code> is true, and <code>t2</code> otherwise.
The function <code>prod_example</code> takes a pair consisting of a boolean,
<code>b</code>, and a number, <code>n</code>, and returns either <code>2 * n</code> or <code>2 * n + 1</code>
according to whether <code>b</code> is true or false.</p>
<p>In contrast, the sum type has <em>two</em> constructors, <code>inl</code> and <code>inr</code>
(for &quot;insert left&quot; and &quot;insert right&quot;), each of which takes <em>one</em>
(explicit) argument. To define a function on <code>Sum α β</code>, we have to
handle two cases: either the input is of the form <code>inl a</code>, in which
case we have to specify an output value in terms of <code>a</code>, or the
input is of the form <code>inr b</code>, in which case we have to specify an
output value in terms of <code>b</code>.</p>
<pre><code class="language-lean">def sum_example (s : Sum Nat Nat) : Nat :=
  Sum.casesOn (motive := fun _ =&gt; Nat) s
    (fun n =&gt; 2 * n)
    (fun n =&gt; 2 * n + 1)

#eval sum_example (Sum.inl 3)
#eval sum_example (Sum.inr 3)
</code></pre>
<p>This example is similar to the previous one, but now an input to
<code>sum_example</code> is implicitly either of the form <code>inl n</code> or <code>inr n</code>.
In the first case, the function returns <code>2 * n</code>, and the second
case, it returns <code>2 * n + 1</code>.</p>
<p>Notice that the product type depends on parameters <code>α β : Type</code>
which are arguments to the constructors as well as <code>Prod</code>. Lean
detects when these arguments can be inferred from later arguments to a
constructor or the return type, and makes them implicit in that case.</p>
<p>In <a href="inductive_types.html#defining-the-natural-numbers">Section Defining the Natural Numbers</a>
we will see what happens when the
constructor of an inductive type takes arguments from the inductive
type itself. What characterizes the examples we consider in this
section is that each constructor relies only on previously specified types.</p>
<p>Notice that a type with multiple constructors is disjunctive: an
element of <code>Sum α β</code> is either of the form <code>inl a</code> <em>or</em> of the
form <code>inl b</code>. A constructor with multiple arguments introduces
conjunctive information: from an element <code>Prod.mk a b</code> of
<code>Prod α β</code> we can extract <code>a</code> <em>and</em> <code>b</code>. An arbitrary inductive type can
include both features, by having any number of constructors, each of
which takes any number of arguments.</p>
<p>As with function definitions, Lean's inductive definition syntax will
let you put named arguments to the constructors before the colon:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive Prod (α : Type u) (β : Type v) where
  | mk (fst : α) (snd : β) : Prod α β

inductive Sum (α : Type u) (β : Type v) where
  | inl (a : α) : Sum α β
  | inr (b : β) : Sum α β
<span class="boring">end Hidden
</span></code></pre>
<p>The results of these definitions are essentially the same as the ones given earlier in this section.</p>
<p>A type, like <code>Prod</code>, that has only one constructor is purely
conjunctive: the constructor simply packs the list of arguments into a
single piece of data, essentially a tuple where the type of subsequent
arguments can depend on the type of the initial argument. We can also
think of such a type as a &quot;record&quot; or a &quot;structure&quot;. In Lean, the
keyword <code>structure</code> can be used to define such an inductive type as
well as its projections, at the same time.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>structure Prod (α : Type u) (β : Type v) where
  mk :: (fst : α) (snd : β)
<span class="boring">end Hidden
</span></code></pre>
<p>This example simultaneously introduces the inductive type, <code>Prod</code>,
its constructor, <code>mk</code>, the usual eliminators (<code>rec</code> and
<code>recOn</code>), as well as the projections, <code>fst</code> and <code>snd</code>, as
defined above.</p>
<p>If you do not name the constructor, Lean uses <code>mk</code> as a default. For
example, the following defines a record to store a color as a triple
of RGB values:</p>
<pre><code class="language-lean">structure Color where
  (red : Nat) (green : Nat) (blue : Nat)
  deriving Repr

def yellow := Color.mk 255 255 0

#eval Color.red yellow
</code></pre>
<p>The definition of <code>yellow</code> forms the record with the three values
shown, and the projection <code>Color.red</code> returns the red component.</p>
<p>You can avoid the parentheses if you add a line break between each field.</p>
<pre><code class="language-lean">structure Color where
  red : Nat
  green : Nat
  blue : Nat
  deriving Repr
</code></pre>
<p>The <code>structure</code> command is especially useful for defining algebraic
structures, and Lean provides substantial infrastructure to support
working with them. Here, for example, is the definition of a
semigroup:</p>
<pre><code class="language-lean">structure Semigroup where
  carrier : Type u
  mul : carrier → carrier → carrier
  mul_assoc : ∀ a b c, mul (mul a b) c = mul a (mul b c)
</code></pre>
<p>We will see more examples in <a href="./structures_and_records.html">Chapter Structures and Records</a>.</p>
<p>We have already discussed the dependent product type <code>Sigma</code>:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive Sigma {α : Type u} (β : α → Type v) where
  | mk : (a : α) → β a → Sigma β
<span class="boring">end Hidden
</span></code></pre>
<p>Two more examples of inductive types in the library are the following:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive Option (α : Type u) where
  | none : Option α
  | some : α → Option α

inductive Inhabited (α : Type u) where
  | mk : α → Inhabited α
<span class="boring">end Hidden
</span></code></pre>
<p>In the semantics of dependent type theory, there is no built-in notion
of a partial function. Every element of a function type <code>α → β</code> or a
dependent function type <code>(a : α) → β</code> is assumed to have a value
at every input. The <code>Option</code> type provides a way of representing partial functions. An
element of <code>Option β</code> is either <code>none</code> or of the form <code>some b</code>,
for some value <code>b : β</code>. Thus we can think of an element <code>f</code> of the
type <code>α → Option β</code> as being a partial function from <code>α</code> to <code>β</code>:
for every <code>a : α</code>, <code>f a</code> either returns <code>none</code>, indicating
<code>f a</code> is &quot;undefined&quot;, or <code>some b</code>.</p>
<p>An element of <code>Inhabited α</code> is simply a witness to the fact that
there is an element of <code>α</code>. Later, we will see that <code>Inhabited</code> is
an example of a <em>type class</em> in Lean: Lean can be instructed that
suitable base types are inhabited, and can automatically infer that
other constructed types are inhabited on that basis.</p>
<p>As exercises, we encourage you to develop a notion of composition for
partial functions from <code>α</code> to <code>β</code> and <code>β</code> to <code>γ</code>, and show
that it behaves as expected. We also encourage you to show that
<code>Bool</code> and <code>Nat</code> are inhabited, that the product of two inhabited
types is inhabited, and that the type of functions to an inhabited
type is inhabited.</p>
<h2><a class="header" href="#inductively-defined-propositions" id="inductively-defined-propositions">Inductively Defined Propositions</a></h2>
<p>Inductively defined types can live in any type universe, including the
bottom-most one, <code>Prop</code>. In fact, this is exactly how the logical
connectives are defined.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive False : Prop

inductive True : Prop where
  | intro : True

inductive And (a b : Prop) : Prop where
  | intro : a → b → And a b

inductive Or (a b : Prop) : Prop where
  | inl : a → Or a b
  | inr : b → Or a b
<span class="boring">end Hidden
</span></code></pre>
<p>You should think about how these give rise to the introduction and
elimination rules that you have already seen. There are rules that
govern what the eliminator of an inductive type can eliminate <em>to</em>,
that is, what kinds of types can be the target of a recursor. Roughly
speaking, what characterizes inductive types in <code>Prop</code> is that one
can only eliminate to other types in <code>Prop</code>. This is consistent with
the understanding that if <code>p : Prop</code>, an element <code>hp : p</code> carries
no data. There is a small exception to this rule, however, which we
will discuss below, in <a href="inductive_types.html#inductive-families">Section Inductive Families</a>.</p>
<p>Even the existential quantifier is inductively defined:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive Exists {α : Sort u} (p : α → Prop) : Prop where
  | intro (w : α) (h : p w) : Exists p
<span class="boring">end Hidden
</span></code></pre>
<p>Keep in mind that the notation <code>∃ x : α, p</code> is syntactic sugar for <code>Exists (fun x : α =&gt; p)</code>.</p>
<p>The definitions of <code>False</code>, <code>True</code>, <code>And</code>, and <code>Or</code> are
perfectly analogous to the definitions of <code>Empty</code>, <code>Unit</code>,
<code>Prod</code>, and <code>Sum</code>. The difference is that the first group yields
elements of <code>Prop</code>, and the second yields elements of <code>Type u</code> for
some <code>u</code>. In a similar way, <code>∃ x : α, p</code> is a <code>Prop</code>-valued
variant of <code>Σ x : α, p</code>.</p>
<p>This is a good place to mention another inductive type, denoted
<code>{x : α // p}</code>, which is sort of a hybrid between
<code>∃ x : α, P</code> and <code>Σ x : α, P</code>.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive Subtype {α : Type u} (p : α → Prop) where
  | mk : (x : α) → p x → Subtype p
<span class="boring">end Hidden
</span></code></pre>
<p>In fact, in Lean, <code>Subtype</code> is defined using the structure command:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>structure Subtype {α : Sort u} (p : α → Prop) where
  val : α
  property : p val
<span class="boring">end Hidden
</span></code></pre>
<p>The notation <code>{x : α // p x}</code> is syntactic sugar for <code>Subtype (fun x : α =&gt; p x)</code>.
It is modeled after subset notation in set theory: the idea is that <code>{x : α // p x}</code>
denotes the collection of elements of <code>α</code> that have property <code>p</code>.</p>
<h2><a class="header" href="#defining-the-natural-numbers" id="defining-the-natural-numbers">Defining the Natural Numbers</a></h2>
<p>The inductively defined types we have seen so far are &quot;flat&quot;:
constructors wrap data and insert it into a type, and the
corresponding recursor unpacks the data and acts on it. Things get
much more interesting when the constructors act on elements of the
very type being defined. A canonical example is the type <code>Nat</code> of
natural numbers:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive Nat where
  | zero : Nat
  | succ : Nat → Nat
<span class="boring">end Hidden
</span></code></pre>
<p>There are two constructors. We start with <code>zero : Nat</code>; it takes
no arguments, so we have it from the start. In contrast, the
constructor <code>succ</code> can only be applied to a previously constructed
<code>Nat</code>. Applying it to <code>zero</code> yields <code>succ zero : Nat</code>. Applying
it again yields <code>succ (succ zero) : Nat</code>, and so on. Intuitively,
<code>Nat</code> is the &quot;smallest&quot; type with these constructors, meaning that
it is exhaustively (and freely) generated by starting with <code>zero</code>
and applying <code>succ</code> repeatedly.</p>
<p>As before, the recursor for <code>Nat</code> is designed to define a dependent
function <code>f</code> from <code>Nat</code> to any domain, that is, an element <code>f</code>
of <code>(n : Nat) → motive n</code> for some <code>motive : Nat → Sort u</code>.
It has to handle two cases: the case where the input is <code>zero</code>, and the case where
the input is of the form <code>succ n</code> for some <code>n : Nat</code>. In the first
case, we simply specify a target value with the appropriate type, as
before. In the second case, however, the recursor can assume that a
value of <code>f</code> at <code>n</code> has already been computed. As a result, the
next argument to the recursor specifies a value for <code>f (succ n)</code> in
terms of <code>n</code> and <code>f n</code>. If we check the type of the recursor,</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span><span class="boring">inductive Nat where
</span><span class="boring"> | zero : Nat
</span><span class="boring"> | succ : Nat → Nat
</span>#check @Nat.rec
<span class="boring">end Hidden
</span></code></pre>
<p>you find the following:</p>
<pre><code>  {motive : Nat → Sort u}
  → motive Nat.zero
  → ((n : Nat) → motive n → motive (Nat.succ n))
  → (t : Nat) → motive t
</code></pre>
<p>The implicit argument, <code>motive</code>, is the codomain of the function being defined.
In type theory it is common to say <code>motive</code> is the <em>motive</em> for the elimination/recursion,
since it describes the kind of object we wish to construct.
The next two arguments specify how to compute the zero and successor cases, as described above.
They are also known as the <em>minor premises</em>.
Finally, the <code>t : Nat</code>, is the input to the function. It is also known as the <em>major premise</em>.</p>
<p>The <code>Nat.recOn</code> is similar to <code>Nat.rec</code> but the major premise occurs before the minor premises.</p>
<pre><code>@Nat.recOn :
  {motive : Nat → Sort u}
  → (t : Nat)
  → motive Nat.zero
  → ((n : Nat) → motive n → motive (Nat.succ n))
  → motive t
</code></pre>
<p>Consider, for example, the addition function <code>add m n</code> on the
natural numbers. Fixing <code>m</code>, we can define addition by recursion on
<code>n</code>. In the base case, we set <code>add m zero</code> to <code>m</code>. In the
successor step, assuming the value <code>add m n</code> is already determined,
we define <code>add m (succ n)</code> to be <code>succ (add m n)</code>.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive Nat where
  | zero : Nat
  | succ : Nat → Nat
  deriving Repr

def add (m n : Nat) : Nat :=
  match n with
  | Nat.zero   =&gt; m
  | Nat.succ n =&gt; Nat.succ (add m n)

open Nat

#eval add (succ (succ zero)) (succ zero)
<span class="boring">end Hidden
</span></code></pre>
<p>It is useful to put such definitions into a namespace, <code>Nat</code>. We can
then go on to define familiar notation in that namespace. The two
defining equations for addition now hold definitionally:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span><span class="boring">inductive Nat where
</span><span class="boring"> | zero : Nat
</span><span class="boring"> | succ : Nat → Nat
</span><span class="boring"> deriving Repr
</span>namespace Nat

def add (m n : Nat) : Nat :=
  match n with
  | Nat.zero   =&gt; m
  | Nat.succ n =&gt; Nat.succ (add m n)

instance : Add Nat where
  add := add

theorem add_zero (m : Nat) : m + zero = m := rfl
theorem add_succ (m n : Nat) : m + succ n = succ (m + n) := rfl

end Nat
<span class="boring">end Hidden
</span></code></pre>
<p>We will explain how the <code>instance</code> command works in
<a href="./type_classes.html">Chapter Type Classes</a>. In the examples below, we will use
Lean's version of the natural numbers.</p>
<p>Proving a fact like <code>zero + m = m</code>, however, requires a proof by induction.
As observed above, the induction principle is just a special case of the recursion principle,
when the codomain <code>motive n</code> is an element of <code>Prop</code>. It represents the familiar
pattern of an inductive proof: to prove <code>∀ n, motive n</code>, first prove <code>motive 0</code>,
and then, for arbitrary <code>n</code>, assume <code>ih : motive n</code> and prove <code>motive (succ n)</code>.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>open Nat

theorem zero_add (n : Nat) : 0 + n = n :=
  Nat.recOn (motive := fun x =&gt; 0 + x = x)
   n
   (show 0 + 0 = 0 from rfl)
   (fun (n : Nat) (ih : 0 + n = n) =&gt;
    show 0 + succ n = succ n from
    calc 0 + succ n
      _ = succ (0 + n) := rfl
      _ = succ n       := by rw [ih])
<span class="boring">end Hidden
</span></code></pre>
<p>Notice that, once again, when <code>Nat.recOn</code> is used in the context of
a proof, it is really the induction principle in disguise. The
<code>rewrite</code> and <code>simp</code> tactics tend to be very effective in proofs
like these. In this case, each can be used to reduce the proof to:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>open Nat

theorem zero_add (n : Nat) : 0 + n = n :=
  Nat.recOn (motive := fun x =&gt; 0 + x = x) n
    rfl
    (fun n ih =&gt; by simp [add_succ, ih])
<span class="boring">end Hidden
</span></code></pre>
<p>As another example, let us prove the associativity of addition,
<code>∀ m n k, m + n + k = m + (n + k)</code>.
(The notation <code>+</code>, as we have defined it, associates to the left, so <code>m + n + k</code> is really <code>(m + n) + k</code>.)
The hardest part is figuring out which variable to do the induction on. Since addition is defined by recursion on the second argument,
<code>k</code> is a good guess, and once we make that choice the proof almost writes itself:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>open Nat
theorem add_assoc (m n k : Nat) : m + n + k = m + (n + k) :=
  Nat.recOn (motive := fun k =&gt; m + n + k = m + (n + k)) k
    (show m + n + 0 = m + (n + 0) from rfl)
    (fun k (ih : m + n + k = m + (n + k)) =&gt;
      show m + n + succ k = m + (n + succ k) from
      calc m + n + succ k
        _ = succ (m + n + k)   := rfl
        _ = succ (m + (n + k)) := by rw [ih]
        _ = m + succ (n + k)   := rfl
        _ = m + (n + succ k)   := rfl)
<span class="boring">end Hidden
</span></code></pre>
<p>Once again, you can reduce the proof to:</p>
<pre><code class="language-lean">open Nat
theorem add_assoc (m n k : Nat) : m + n + k = m + (n + k) :=
  Nat.recOn (motive := fun k =&gt; m + n + k = m + (n + k)) k
    rfl
    (fun k ih =&gt; by simp [Nat.add_succ, ih])
</code></pre>
<p>Suppose we try to prove the commutativity of addition. Choosing induction on the second argument, we might begin as follows:</p>
<pre><code class="language-lean">open Nat
theorem add_comm (m n : Nat) : m + n = n + m :=
  Nat.recOn (motive := fun x =&gt; m + x = x + m) n
   (show m + 0 = 0 + m by rw [Nat.zero_add, Nat.add_zero])
   (fun (n : Nat) (ih : m + n = n + m) =&gt;
    show m + succ n = succ n + m from
    calc m + succ n
      _ = succ (m + n) := rfl
      _ = succ (n + m) := by rw [ih]
      _ = succ n + m   := sorry)
</code></pre>
<p>At this point, we see that we need another supporting fact, namely, that <code>succ (n + m) = succ n + m</code>.
You can prove this by induction on <code>m</code>:</p>
<pre><code class="language-lean">open Nat

theorem succ_add (n m : Nat) : succ n + m = succ (n + m) :=
  Nat.recOn (motive := fun x =&gt; succ n + x = succ (n + x)) m
    (show succ n + 0 = succ (n + 0) from rfl)
    (fun (m : Nat) (ih : succ n + m = succ (n + m)) =&gt;
     show succ n + succ m = succ (n + succ m) from
     calc succ n + succ m
       _ = succ (succ n + m)   := rfl
       _ = succ (succ (n + m)) := by rw [ih]
       _ = succ (n + succ m)   := rfl)
</code></pre>
<p>You can then replace the <code>sorry</code> in the previous proof with <code>succ_add</code>. Yet again, the proofs can be compressed:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>open Nat
theorem succ_add (n m : Nat) : succ n + m = succ (n + m) :=
  Nat.recOn (motive := fun x =&gt; succ n + x = succ (n + x)) m
    rfl
    (fun m ih =&gt; by simp only [add_succ, ih])

theorem add_comm (m n : Nat) : m + n = n + m :=
  Nat.recOn (motive := fun x =&gt; m + x = x + m) n
    (by simp)
    (fun m ih =&gt; by simp [add_succ, succ_add, ih])
<span class="boring">end Hidden
</span></code></pre>
<h2><a class="header" href="#other-recursive-data-types" id="other-recursive-data-types">Other Recursive Data Types</a></h2>
<p>Let us consider some more examples of inductively defined types. For
any type, <code>α</code>, the type <code>List α</code> of lists of elements of <code>α</code> is
defined in the library.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive List (α : Type u) where
  | nil  : List α
  | cons : α → List α → List α

namespace List

def append (as bs : List α) : List α :=
  match as with
  | nil       =&gt; bs
  | cons a as =&gt; cons a (append as bs)

theorem nil_append (as : List α) : append nil as = as :=
  rfl

theorem cons_append (a : α) (as bs : List α)
                    : append (cons a as) bs = cons a (append as bs) :=
  rfl

end List
<span class="boring">end Hidden
</span></code></pre>
<p>A list of elements of type <code>α</code> is either the empty list, <code>nil</code>, or
an element <code>h : α</code> followed by a list <code>t : List α</code>.
The first element, <code>h</code>, is commonly known as the &quot;head&quot; of the list,
and the remainder, <code>t</code>, is known as the &quot;tail.&quot;</p>
<p>As an exercise, prove the following:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span><span class="boring">inductive List (α : Type u) where
</span><span class="boring">| nil  : List α
</span><span class="boring">| cons : α → List α → List α
</span><span class="boring">namespace List
</span><span class="boring">def append (as bs : List α) : List α :=
</span><span class="boring"> match as with
</span><span class="boring"> | nil       =&gt; bs
</span><span class="boring"> | cons a as =&gt; cons a (append as bs)
</span><span class="boring">theorem nil_append (as : List α) : append nil as = as :=
</span><span class="boring"> rfl
</span><span class="boring">theorem cons_append (a : α) (as bs : List α)
</span><span class="boring">                    : append (cons a as) bs = cons a (append as bs) :=
</span><span class="boring"> rfl
</span>theorem append_nil (as : List α) : append as nil = as :=
  sorry

theorem append_assoc (as bs cs : List α)
        : append (append as bs) cs = append as (append bs cs) :=
  sorry
<span class="boring">end List
</span><span class="boring">end Hidden
</span></code></pre>
<p>Try also defining the function <code>length : {α : Type u} → List α → Nat</code> that returns the length of a list,
and prove that it behaves as expected (for example, <code>length (append as bs) = length as + length bs</code>).</p>
<p>For another example, we can define the type of binary trees:</p>
<pre><code class="language-lean">inductive BinaryTree where
  | leaf : BinaryTree
  | node : BinaryTree → BinaryTree → BinaryTree
</code></pre>
<p>In fact, we can even define the type of countably branching trees:</p>
<pre><code class="language-lean">inductive CBTree where
  | leaf : CBTree
  | sup : (Nat → CBTree) → CBTree

namespace CBTree

def succ (t : CBTree) : CBTree :=
  sup (fun _ =&gt; t)

def toCBTree : Nat → CBTree
  | 0 =&gt; leaf
  | n+1 =&gt; succ (toCBTree n)

def omega : CBTree :=
  sup toCBTree

end CBTree
</code></pre>
<h2><a class="header" href="#tactics-for-inductive-types" id="tactics-for-inductive-types">Tactics for Inductive Types</a></h2>
<p>Given the fundamental importance of inductive types in Lean, it should
not be surprising that there are a number of tactics designed to work
with them effectively. We describe some of them here.</p>
<p>The <code>cases</code> tactic works on elements of an inductively defined type,
and does what the name suggests: it decomposes the element according
to each of the possible constructors. In its most basic form, it is
applied to an element <code>x</code> in the local context. It then reduces the
goal to cases in which <code>x</code> is replaced by each of the constructions.</p>
<pre><code class="language-lean">example (p : Nat → Prop) (hz : p 0) (hs : ∀ n, p (Nat.succ n)) : ∀ n, p n := by
  intro n
  cases n
  . exact hz  -- goal is p 0
  . apply hs  -- goal is a : Nat ⊢ p (succ a)
</code></pre>
<p>There are extra bells and whistles. For one thing, <code>cases</code> allows
you to choose the names for each alternative using a
<code>with</code> clause. In the next example, for example, we choose the name
<code>m</code> for the argument to <code>succ</code>, so that the second case refers to
<code>succ m</code>. More importantly, the cases tactic will detect any items
in the local context that depend on the target variable. It reverts
these elements, does the split, and reintroduces them. In the example
below, notice that the hypothesis <code>h : n ≠ 0</code> becomes <code>h : 0 ≠ 0</code>
in the first branch, and <code>h : succ m ≠ 0</code> in the second.</p>
<pre><code class="language-lean">open Nat

example (n : Nat) (h : n ≠ 0) : succ (pred n) = n := by
  cases n with
  | zero =&gt;
    -- goal: h : 0 ≠ 0 ⊢ succ (pred 0) = 0
    apply absurd rfl h
  | succ m =&gt;
    -- second goal: h : succ m ≠ 0 ⊢ succ (pred (succ m)) = succ m
    rfl
</code></pre>
<p>Notice that <code>cases</code> can be used to produce data as well as prove propositions.</p>
<pre><code class="language-lean">def f (n : Nat) : Nat := by
  cases n; exact 3; exact 7

example : f 0 = 3 := rfl
example : f 5 = 7 := rfl
</code></pre>
<p>Once again, cases will revert, split, and then reintroduce dependencies in the context.</p>
<pre><code class="language-lean">def Tuple (α : Type) (n : Nat) :=
  { as : List α // as.length = n }

def f {n : Nat} (t : Tuple α n) : Nat := by
  cases n; exact 3; exact 7

def myTuple : Tuple Nat 3 :=
  ⟨[0, 1, 2], rfl⟩

example : f myTuple = 7 :=
  rfl
</code></pre>
<p>Here is an example of multiple constructors with arguments.</p>
<pre><code class="language-lean">inductive Foo where
  | bar1 : Nat → Nat → Foo
  | bar2 : Nat → Nat → Nat → Foo

def silly (x : Foo) : Nat := by
  cases x with
  | bar1 a b =&gt; exact b
  | bar2 c d e =&gt; exact e
</code></pre>
<p>The alternatives for each constructor don't need to be solved
in the order the constructors were declared.</p>
<pre><code class="language-lean"><span class="boring">inductive Foo where
</span><span class="boring">  | bar1 : Nat → Nat → Foo
</span><span class="boring">  | bar2 : Nat → Nat → Nat → Foo
</span>def silly (x : Foo) : Nat := by
  cases x with
  | bar2 c d e =&gt; exact e
  | bar1 a b =&gt; exact b
</code></pre>
<p>The syntax of the <code>with</code> is convenient for writing structured proofs.
Lean also provides a complementary <code>case</code> tactic, which allows you to focus on goal
assign variable names.</p>
<pre><code class="language-lean"><span class="boring">inductive Foo where
</span><span class="boring">  | bar1 : Nat → Nat → Foo
</span><span class="boring">  | bar2 : Nat → Nat → Nat → Foo
</span>def silly (x : Foo) : Nat := by
  cases x
  case bar1 a b =&gt; exact b
  case bar2 c d e =&gt; exact e
</code></pre>
<p>The <code>case</code> tactic is clever, in that it will match the constructor to the appropriate goal. For example, we can fill the goals above in the opposite order:</p>
<pre><code class="language-lean"><span class="boring">inductive Foo where
</span><span class="boring">  | bar1 : Nat → Nat → Foo
</span><span class="boring">  | bar2 : Nat → Nat → Nat → Foo
</span>def silly (x : Foo) : Nat := by
  cases x
  case bar2 c d e =&gt; exact e
  case bar1 a b =&gt; exact b
</code></pre>
<p>You can also use <code>cases</code> with an arbitrary expression. Assuming that
expression occurs in the goal, the cases tactic will generalize over
the expression, introduce the resulting universally quantified
variable, and case on that.</p>
<pre><code class="language-lean">open Nat

example (p : Nat → Prop) (hz : p 0) (hs : ∀ n, p (succ n)) (m k : Nat)
        : p (m + 3 * k) := by
  cases m + 3 * k
  exact hz   -- goal is p 0
  apply hs   -- goal is a : Nat ⊢ p (succ a)
</code></pre>
<p>Think of this as saying &quot;split on cases as to whether <code>m + 3 * k</code> is
zero or the successor of some number.&quot; The result is functionally
equivalent to the following:</p>
<pre><code class="language-lean">open Nat

example (p : Nat → Prop) (hz : p 0) (hs : ∀ n, p (succ n)) (m k : Nat)
        : p (m + 3 * k) := by
  generalize m + 3 * k = n
  cases n
  exact hz   -- goal is p 0
  apply hs   -- goal is a : Nat ⊢ p (succ a)
</code></pre>
<p>Notice that the expression <code>m + 3 * k</code> is erased by <code>generalize</code>; all
that matters is whether it is of the form <code>0</code> or <code>succ a</code>. This
form of <code>cases</code> will <em>not</em> revert any hypotheses that also mention
the expression in the equation (in this case, <code>m + 3 * k</code>). If such a
term appears in a hypothesis and you want to generalize over that as
well, you need to <code>revert</code> it explicitly.</p>
<p>If the expression you case on does not appear in the goal, the
<code>cases</code> tactic uses <code>have</code> to put the type of the expression into
the context. Here is an example:</p>
<pre><code class="language-lean">example (p : Prop) (m n : Nat)
        (h₁ : m &lt; n → p) (h₂ : m ≥ n → p) : p := by
  cases Nat.lt_or_ge m n
  case inl hlt =&gt; exact h₁ hlt
  case inr hge =&gt; exact h₂ hge
</code></pre>
<p>The theorem <code>Nat.lt_or_ge m n</code> says <code>m &lt; n ∨ m ≥ n</code>, and it is
natural to think of the proof above as splitting on these two
cases. In the first branch, we have the hypothesis <code>hlt : m &lt; n</code>, and
in the second we have the hypothesis <code>hge : m ≥ n</code>. The proof above
is functionally equivalent to the following:</p>
<pre><code class="language-lean">example (p : Prop) (m n : Nat)
        (h₁ : m &lt; n → p) (h₂ : m ≥ n → p) : p := by
  have h : m &lt; n ∨ m ≥ n := Nat.lt_or_ge m n
  cases h
  case inl hlt =&gt; exact h₁ hlt
  case inr hge =&gt; exact h₂ hge
</code></pre>
<p>After the first two lines, we have <code>h : m &lt; n ∨ m ≥ n</code> as a
hypothesis, and we simply do cases on that.</p>
<p>Here is another example, where we use the decidability of equality on
the natural numbers to split on the cases <code>m = n</code> and <code>m ≠ n</code>.</p>
<pre><code class="language-lean">#check Nat.sub_self

example (m n : Nat) : m - n = 0 ∨ m ≠ n := by
  cases Decidable.em (m = n) with
  | inl heq =&gt; rw [heq]; apply Or.inl; exact Nat.sub_self n
  | inr hne =&gt; apply Or.inr; exact hne
</code></pre>
<p>Remember that if you <code>open Classical</code>, you can use the law of the
excluded middle for any proposition at all. But using type class
inference (see <a href="./type_classes.html">Chapter Type Classes</a>), Lean can actually
find the relevant decision procedure, which means that you can use the
case split in a computable function.</p>
<p>Just as the <code>cases</code> tactic can be used to carry out proof by cases,
the <code>induction</code> tactic can be used to carry out proofs by
induction. The syntax is similar to that of <code>cases</code>, except that the
argument can only be a term in the local context. Here is an example:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>theorem zero_add (n : Nat) : 0 + n = n := by
  induction n with
  | zero =&gt; rfl
  | succ n ih =&gt; rw [Nat.add_succ, ih]
<span class="boring">end Hidden
</span></code></pre>
<p>As with <code>cases</code>, we can use the <code>case</code> tactic instead of <code>with</code>.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>theorem zero_add (n : Nat) : 0 + n = n := by
  induction n
  case zero =&gt; rfl
  case succ n ih =&gt; rw [Nat.add_succ, ih]
<span class="boring">end Hidden
</span></code></pre>
<p>Here are some additional examples:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span><span class="boring">theorem add_zero (n : Nat) : n + 0 = n := Nat.add_zero n
</span>open Nat

theorem zero_add (n : Nat) : 0 + n = n := by
  induction n &lt;;&gt; simp [*, add_zero, add_succ]

theorem succ_add (m n : Nat) : succ m + n = succ (m + n) := by
  induction n &lt;;&gt; simp [*, add_zero, add_succ]

theorem add_comm (m n : Nat) : m + n = n + m := by
  induction n &lt;;&gt; simp [*, add_zero, add_succ, succ_add, zero_add]

theorem add_assoc (m n k : Nat) : m + n + k = m + (n + k) := by
  induction k &lt;;&gt; simp [*, add_zero, add_succ]
<span class="boring">end Hidden
</span></code></pre>
<p>The <code>induction</code> tactic also supports user-defined induction principles with
multiple targets (aka major premises).</p>
<pre><code class="language-lean">/-
theorem Nat.mod.inductionOn
      {motive : Nat → Nat → Sort u}
      (x y  : Nat)
      (ind  : ∀ x y, 0 &lt; y ∧ y ≤ x → motive (x - y) y → motive x y)
      (base : ∀ x y, ¬(0 &lt; y ∧ y ≤ x) → motive x y)
      : motive x y :=
-/

example (x : Nat) {y : Nat} (h : y &gt; 0) : x % y &lt; y := by
  induction x, y using Nat.mod.inductionOn with
  | ind x y h₁ ih =&gt;
    rw [Nat.mod_eq_sub_mod h₁.2]
    exact ih h
  | base x y h₁ =&gt;
    have : ¬ 0 &lt; y ∨ ¬ y ≤ x := Iff.mp (Decidable.not_and_iff_or_not ..) h₁
    match this with
    | Or.inl h₁ =&gt; exact absurd h h₁
    | Or.inr h₁ =&gt;
      have hgt : y &gt; x := Nat.gt_of_not_le h₁
      rw [← Nat.mod_eq_of_lt hgt] at hgt
      assumption
</code></pre>
<p>You can use the <code>match</code> notation in tactics too:</p>
<pre><code class="language-lean">example : p ∨ q → q ∨ p := by
  intro h
  match h with
  | Or.inl _  =&gt; apply Or.inr; assumption
  | Or.inr h2 =&gt; apply Or.inl; exact h2
</code></pre>
<p>As a convenience, pattern-matching has been integrated into tactics such as <code>intro</code> and <code>funext</code>.</p>
<pre><code class="language-lean">example : s ∧ q ∧ r → p ∧ r → q ∧ p := by
  intro ⟨_, ⟨hq, _⟩⟩ ⟨hp, _⟩
  exact ⟨hq, hp⟩

example :
    (fun (x : Nat × Nat) (y : Nat × Nat) =&gt; x.1 + y.2)
    =
    (fun (x : Nat × Nat) (z : Nat × Nat) =&gt; z.2 + x.1) := by
  funext (a, b) (c, d)
  show a + d = d + a
  rw [Nat.add_comm]
</code></pre>
<p>We close this section with one last tactic that is designed to
facilitate working with inductive types, namely, the <code>injection</code>
tactic. By design, the elements of an inductive type are freely
generated, which is to say, the constructors are injective and have
disjoint ranges. The <code>injection</code> tactic is designed to make use of
this fact:</p>
<pre><code class="language-lean">open Nat

example (m n k : Nat) (h : succ (succ m) = succ (succ n))
        : n + k = m + k := by
  injection h with h'
  injection h' with h''
  rw [h'']
</code></pre>
<p>The first instance of the tactic adds <code>h' : succ m = succ n</code> to the
context, and the second adds <code>h'' : m = n</code>.</p>
<p>The <code>injection</code> tactic also detects contradictions that arise when different constructors
are set equal to one another, and uses them to close the goal.</p>
<pre><code class="language-lean">open Nat

example (m n : Nat) (h : succ m = 0) : n = n + 7 := by
  injection h

example (m n : Nat) (h : succ m = 0) : n = n + 7 := by
  contradiction

example (h : 7 = 4) : False := by
  contradiction
</code></pre>
<p>As the second example shows, the <code>contradiction</code> tactic also detects contradictions of this form.</p>
<h2><a class="header" href="#inductive-families" id="inductive-families">Inductive Families</a></h2>
<p>We are almost done describing the full range of inductive definitions
accepted by Lean. So far, you have seen that Lean allows you to
introduce inductive types with any number of recursive
constructors. In fact, a single inductive definition can introduce an
indexed <em>family</em> of inductive types, in a manner we now describe.</p>
<p>An inductive family is an indexed family of types defined by a
simultaneous induction of the following form:</p>
<pre><code>inductive foo : ... → Sort u where
  | constructor₁ : ... → foo ...
  | constructor₂ : ... → foo ...
  ...
  | constructorₙ : ... → foo ...
</code></pre>
<p>In contrast to an ordinary inductive definition, which constructs an
element of some <code>Sort u</code>, the more general version constructs a
function <code>... → Sort u</code>, where &quot;<code>...</code>&quot; denotes a sequence of
argument types, also known as <em>indices</em>. Each constructor then
constructs an element of some member of the family. One example is the
definition of <code>Vector α n</code>, the type of vectors of elements of <code>α</code>
of length <code>n</code>:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive Vector (α : Type u) : Nat → Type u where
  | nil  : Vector α 0
  | cons : α → {n : Nat} → Vector α n → Vector α (n+1)
<span class="boring">end Hidden
</span></code></pre>
<p>Notice that the <code>cons</code> constructor takes an element of
<code>Vector α n</code> and returns an element of <code>Vector α (n+1)</code>, thereby using an
element of one member of the family to build an element of another.</p>
<p>A more exotic example is given by the definition of the equality type in Lean:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>inductive Eq {α : Sort u} (a : α) : α → Prop where
  | refl : Eq a a
<span class="boring">end Hidden
</span></code></pre>
<p>For each fixed <code>α : Sort u</code> and <code>a : α</code>, this definition
constructs a family of types <code>Eq a x</code>, indexed by <code>x : α</code>.
Notably, however, there is only one constructor, <code>refl</code>, which
is an element of <code>Eq a a</code>.
Intuitively, the only way to construct a proof of <code>Eq a x</code>
is to use reflexivity, in the case where <code>x</code> is <code>a</code>.
Note that <code>Eq a a</code> is the only inhabited type in the family of types
<code>Eq a x</code>. The elimination principle generated by Lean is as follows:</p>
<pre><code class="language-lean">universe u v

#check (@Eq.rec : {α : Sort u} → {a : α} → {motive : (x : α) → a = x → Sort v}
                  → motive a rfl → {b : α} → (h : a = b) → motive b h)
</code></pre>
<p>It is a remarkable fact that all the basic axioms for equality follow
from the constructor, <code>refl</code>, and the eliminator, <code>Eq.rec</code>. The
definition of equality is atypical, however; see the discussion in <a href="inductive_types.html#axiomatic-details">Section Axiomatic Details</a>.</p>
<p>The recursor <code>Eq.rec</code> is also used to define substitution:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>theorem subst {α : Type u} {a b : α} {p : α → Prop} (h₁ : Eq a b) (h₂ : p a) : p b :=
  Eq.rec (motive := fun x _ =&gt; p x) h₂ h₁
<span class="boring">end Hidden
</span></code></pre>
<p>You can also define <code>subst</code> using <code>match</code>.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>theorem subst {α : Type u} {a b : α} {p : α → Prop} (h₁ : Eq a b) (h₂ : p a) : p b :=
  match h₁ with
  | rfl =&gt; h₂
<span class="boring">end Hidden
</span></code></pre>
<p>Actually, Lean compiles the <code>match</code> expressions using a definition based on
<code>Eq.rec</code>.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>theorem subst {α : Type u} {a b : α} {p : α → Prop} (h₁ : Eq a b) (h₂ : p a) : p b :=
  match h₁ with
  | rfl =&gt; h₂

set_option pp.all true
#print subst
  -- ... subst.match_1 ...
#print subst.match_1
  -- ... Eq.casesOn ...
#print Eq.casesOn
  -- ... Eq.rec ...
<span class="boring">end Hidden
</span></code></pre>
<p>Using the recursor or <code>match</code> with <code>h₁ : a = b</code>, we may assume <code>a</code> and <code>b</code> are the same,
in which case, <code>p b</code> and <code>p a</code> are the same.</p>
<p>It is not hard to prove that <code>Eq</code> is symmetric and transitive.
In the following example, we prove <code>symm</code> and leave as exercises the theorems <code>trans</code> and <code>congr</code> (congruence).</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>theorem symm {α : Type u} {a b : α} (h : Eq a b) : Eq b a :=
  match h with
  | rfl =&gt; rfl

theorem trans {α : Type u} {a b c : α} (h₁ : Eq a b) (h₂ : Eq b c) : Eq a c :=
  sorry

theorem congr {α β : Type u} {a b : α} (f : α → β) (h : Eq a b) : Eq (f a) (f b) :=
  sorry
<span class="boring">end Hidden
</span></code></pre>
<p>In the type theory literature, there are further generalizations of
inductive definitions, for example, the principles of
<em>induction-recursion</em> and <em>induction-induction</em>. These are not
supported by Lean.</p>
<h2><a class="header" href="#axiomatic-details" id="axiomatic-details">Axiomatic Details</a></h2>
<p>We have described inductive types and their syntax through
examples. This section provides additional information for those
interested in the axiomatic foundations.</p>
<p>We have seen that the constructor to an inductive type takes
<em>parameters</em> --- intuitively, the arguments that remain fixed
throughout the inductive construction --- and <em>indices</em>, the arguments
parameterizing the family of types that is simultaneously under
construction. Each constructor should have a type, where the
argument types are built up from previously defined types, the
parameter and index types, and the inductive family currently being
defined. The requirement is that if the latter is present at all, it
occurs only <em>strictly positively</em>. This means simply that any argument
to the constructor in which it occurs is a dependent arrow type in which the
inductive type under definition occurs only as the resulting type,
where the indices are given in terms of constants and previous
arguments.</p>
<p>Since an inductive type lives in <code>Sort u</code> for some <code>u</code>, it is
reasonable to ask <em>which</em> universe levels <code>u</code> can be instantiated
to. Each constructor <code>c</code> in the definition of a family <code>C</code> of
inductive types is of the form</p>
<pre><code>  c : (a : α) → (b : β[a]) → C a p[a,b]
</code></pre>
<p>where <code>a</code> is a sequence of data type parameters, <code>b</code> is the
sequence of arguments to the constructors, and <code>p[a, b]</code> are the
indices, which determine which element of the inductive family the
construction inhabits. (Note that this description is somewhat
misleading, in that the arguments to the constructor can appear in any
order as long as the dependencies make sense.) The constraints on the
universe level of <code>C</code> fall into two cases, depending on whether or
not the inductive type is specified to land in <code>Prop</code> (that is,
<code>Sort 0</code>).</p>
<p>Let us first consider the case where the inductive type is <em>not</em>
specified to land in <code>Prop</code>. Then the universe level <code>u</code> is
constrained to satisfy the following:</p>
<blockquote>
<p>For each constructor <code>c</code> as above, and each <code>βk[a]</code> in the sequence <code>β[a]</code>, if <code>βk[a] : Sort v</code>, we have <code>u</code> ≥ <code>v</code>.</p>
</blockquote>
<p>In other words, the universe level <code>u</code> is required to be at least as
large as the universe level of each type that represents an argument
to a constructor.</p>
<p>When the inductive type is specified to land in <code>Prop</code>, there are no
constraints on the universe levels of the constructor arguments. But
these universe levels do have a bearing on the elimination
rule. Generally speaking, for an inductive type in <code>Prop</code>, the
motive of the elimination rule is required to be in <code>Prop</code>.</p>
<p>There is an exception to this last rule: we are allowed to eliminate
from an inductively defined <code>Prop</code> to an arbitrary <code>Sort</code> when
there is only one constructor and each constructor argument is either
in <code>Prop</code> or an index. The intuition is that in this case the
elimination does not make use of any information that is not already
given by the mere fact that the type of argument is inhabited. This
special case is known as <em>singleton elimination</em>.</p>
<p>We have already seen singleton elimination at play in applications of
<code>Eq.rec</code>, the eliminator for the inductively defined equality
type. We can use an element <code>h : Eq a b</code> to cast an element
<code>t' : p a</code> to <code>p b</code> even when <code>p a</code> and <code>p b</code> are arbitrary types,
because the cast does not produce new data; it only reinterprets the
data we already have. Singleton elimination is also used with
heterogeneous equality and well-founded recursion, which will be
discussed in a <a href="./induction_and_recursion.html#well-founded-recursion-and-induction">Chapter Induction and Recursion</a>.</p>
<h2><a class="header" href="#mutual-and-nested-inductive-types" id="mutual-and-nested-inductive-types">Mutual and Nested Inductive Types</a></h2>
<p>We now consider two generalizations of inductive types that are often
useful, which Lean supports by &quot;compiling&quot; them down to the more
primitive kinds of inductive types described above. In other words,
Lean parses the more general definitions, defines auxiliary inductive
types based on them, and then uses the auxiliary types to define the
ones we really want. Lean's equation compiler, described in the next
chapter, is needed to make use of these types
effectively. Nonetheless, it makes sense to describe the declarations
here, because they are straightforward variations on ordinary
inductive definitions.</p>
<p>First, Lean supports <em>mutually defined</em> inductive types. The idea is
that we can define two (or more) inductive types at the same time,
where each one refers to the other(s).</p>
<pre><code class="language-lean">mutual
  inductive Even : Nat → Prop where
    | even_zero : Even 0
    | even_succ : (n : Nat) → Odd n → Even (n + 1)

  inductive Odd : Nat → Prop where
    | odd_succ : (n : Nat) → Even n → Odd (n + 1)
end
</code></pre>
<p>In this example, two types are defined simultaneously: a natural
number <code>n</code> is <code>Even</code> if it is <code>0</code> or one more than an <code>Odd</code>
number, and <code>Odd</code> if it is one more than an <code>Even</code> number.
In the exercises below, you are asked to spell out the details.</p>
<p>A mutual inductive definition can also be used to define the notation
of a finite tree with nodes labelled by elements of <code>α</code>:</p>
<pre><code class="language-lean">mutual
    inductive Tree (α : Type u) where
      | node : α → TreeList α → Tree α

    inductive TreeList (α : Type u) where
      | nil  : TreeList α
      | cons : Tree α → TreeList α → TreeList α
end
</code></pre>
<p>With this definition, one can construct an element of <code>Tree α</code> by
giving an element of <code>α</code> together with a list of subtrees, possibly
empty. The list of subtrees is represented by the type <code>TreeList α</code>,
which is defined to be either the empty list, <code>nil</code>, or the
<code>cons</code> of a tree and an element of <code>TreeList α</code>.</p>
<p>This definition is inconvenient to work with, however. It would be
much nicer if the list of subtrees were given by the type
<code>List (Tree α)</code>, especially since Lean's library contains a number of functions
and theorems for working with lists. One can show that the type
<code>TreeList α</code> is <em>isomorphic</em> to <code>List (Tree α)</code>, but translating
results back and forth along this isomorphism is tedious.</p>
<p>In fact, Lean allows us to define the inductive type we really want:</p>
<pre><code class="language-lean">inductive Tree (α : Type u) where
  | mk : α → List (Tree α) → Tree α
</code></pre>
<p>This is known as a <em>nested</em> inductive type. It falls outside the
strict specification of an inductive type given in the last section
because <code>Tree</code> does not occur strictly positively among the
arguments to <code>mk</code>, but, rather, nested inside the <code>List</code> type
constructor. Lean then automatically builds the
isomorphism between <code>TreeList α</code> and <code>List (Tree α)</code> in its kernel,
and defines the constructors for <code>Tree</code> in terms of the isomorphism.</p>
<h2><a class="header" href="#exercises-3" id="exercises-3">Exercises</a></h2>
<ol>
<li>
<p>Try defining other operations on the natural numbers, such as
multiplication, the predecessor function (with <code>pred 0 = 0</code>),
truncated subtraction (with <code>n - m = 0</code> when <code>m</code> is greater
than or equal to <code>n</code>), and exponentiation. Then try proving some
of their basic properties, building on the theorems we have already
proved.</p>
<p>Since many of these are already defined in Lean's core library, you
should work within a namespace named <code>Hidden</code>, or something like
that, in order to avoid name clashes.</p>
</li>
<li>
<p>Define some operations on lists, like a <code>length</code> function or the
<code>reverse</code> function. Prove some properties, such as the following:</p>
<p>a. <code>length (s ++ t) = length s + length t</code></p>
<p>b. <code>length (reverse t) = length t</code></p>
<p>c. <code>reverse (reverse t) = t</code></p>
</li>
<li>
<p>Define an inductive data type consisting of terms built up from the following constructors:</p>
<ul>
<li><code>const n</code>, a constant denoting the natural number <code>n</code></li>
<li><code>var n</code>, a variable, numbered <code>n</code></li>
<li><code>plus s t</code>, denoting the sum of <code>s</code> and <code>t</code></li>
<li><code>times s t</code>, denoting the product of <code>s</code> and <code>t</code></li>
</ul>
<p>Recursively define a function that evaluates any such term with respect to an assignment of values to the variables.</p>
</li>
<li>
<p>Similarly, define the type of propositional formulas, as well as
functions on the type of such formulas: an evaluation function,
functions that measure the complexity of a formula, and a function
that substitutes another formula for a given variable.</p>
</li>
</ol>
<h1><a class="header" href="#induction-and-recursion" id="induction-and-recursion">Induction and Recursion</a></h1>
<p>In the previous chapter, we saw that inductive definitions provide a
powerful means of introducing new types in Lean. Moreover, the
constructors and the recursors provide the only means of defining
functions on these types. By the propositions-as-types correspondence,
this means that induction is the fundamental method of proof.</p>
<p>Lean provides natural ways of defining recursive functions, performing
pattern matching, and writing inductive proofs. It allows you to
define a function by specifying equations that it should satisfy, and
it allows you to prove a theorem by specifying how to handle various
cases that can arise. Behind the scenes, these descriptions are
&quot;compiled&quot; down to primitive recursors, using a procedure that we
refer to as the &quot;equation compiler.&quot; The equation compiler is not part
of the trusted code base; its output consists of terms that are
checked independently by the kernel.</p>
<h2><a class="header" href="#pattern-matching" id="pattern-matching">Pattern Matching</a></h2>
<p>The interpretation of schematic patterns is the first step of the
compilation process. We have seen that the <code>casesOn</code> recursor can
be used to define functions and prove theorems by cases, according to
the constructors involved in an inductively defined type. But
complicated definitions may use several nested <code>casesOn</code>
applications, and may be hard to read and understand. Pattern matching
provides an approach that is more convenient, and familiar to users of
functional programming languages.</p>
<p>Consider the inductively defined type of natural numbers. Every
natural number is either <code>zero</code> or <code>succ x</code>, and so you can define
a function from the natural numbers to an arbitrary type by specifying
a value in each of those cases:</p>
<pre><code class="language-lean">open Nat

def sub1 : Nat → Nat
  | zero   =&gt; zero
  | succ x =&gt; x

def isZero : Nat → Bool
  | zero   =&gt; true
  | succ x =&gt; false
</code></pre>
<p>The equations used to define these functions hold definitionally:</p>
<pre><code class="language-lean"><span class="boring">open Nat
</span><span class="boring">def sub1 : Nat → Nat
</span><span class="boring">  | zero   =&gt; zero
</span><span class="boring">  | succ x =&gt; x
</span><span class="boring">def isZero : Nat → Bool
</span><span class="boring">  | zero   =&gt; true
</span><span class="boring">  | succ x =&gt; false
</span>example : sub1 0 = 0 := rfl
example (x : Nat) : sub1 (succ x) = x := rfl

example : isZero 0 = true := rfl
example (x : Nat) : isZero (succ x) = false := rfl

example : sub1 7 = 6 := rfl
example (x : Nat) : isZero (x + 3) = false := rfl
</code></pre>
<p>Instead of <code>zero</code> and <code>succ</code>, we can use more familiar notation:</p>
<pre><code class="language-lean">def sub1 : Nat → Nat
  | 0   =&gt; 0
  | x+1 =&gt; x

def isZero : Nat → Bool
  | 0   =&gt; true
  | x+1 =&gt; false
</code></pre>
<p>Because addition and the zero notation have been assigned the
<code>[match_pattern]</code> attribute, they can be used in pattern matching. Lean
simply normalizes these expressions until the constructors <code>zero</code>
and <code>succ</code> are exposed.</p>
<p>Pattern matching works with any inductive type, such as products and option types:</p>
<pre><code class="language-lean">def swap : α × β → β × α
  | (a, b) =&gt; (b, a)

def foo : Nat × Nat → Nat
  | (m, n) =&gt; m + n

def bar : Option Nat → Nat
  | some n =&gt; n + 1
  | none   =&gt; 0
</code></pre>
<p>Here we use it not only to define a function, but also to carry out a
proof by cases:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>def not : Bool → Bool
  | true  =&gt; false
  | false =&gt; true

theorem not_not : ∀ (b : Bool), not (not b) = b
  | true  =&gt; rfl  -- proof that not (not true) = true
  | false =&gt; rfl  -- proof that not (not false) = false
<span class="boring">end Hidden
</span></code></pre>
<p>Pattern matching can also be used to destruct inductively defined propositions:</p>
<pre><code class="language-lean">example (p q : Prop) : p ∧ q → q ∧ p
  | And.intro h₁ h₂ =&gt; And.intro h₂ h₁

example (p q : Prop) : p ∨ q → q ∨ p
  | Or.inl hp =&gt; Or.inr hp
  | Or.inr hq =&gt; Or.inl hq
</code></pre>
<p>This provides a compact way of unpacking hypotheses that make use of logical connectives.</p>
<p>In all these examples, pattern matching was used to carry out a single
case distinction. More interestingly, patterns can involve nested
constructors, as in the following examples.</p>
<pre><code class="language-lean">def sub2 : Nat → Nat
  | 0   =&gt; 0
  | 1   =&gt; 0
  | x+2 =&gt; x
</code></pre>
<p>The equation compiler first splits on cases as to whether the input is
<code>zero</code> or of the form <code>succ x</code>.  It then does a case split on
whether <code>x</code> is of the form <code>zero</code> or <code>succ x</code>.  It determines
the necessary case splits from the patterns that are presented to it,
and raises an error if the patterns fail to exhaust the cases. Once
again, we can use arithmetic notation, as in the version below. In
either case, the defining equations hold definitionally.</p>
<pre><code class="language-lean"><span class="boring">def sub2 : Nat → Nat
</span><span class="boring">  | 0   =&gt; 0
</span><span class="boring">  | 1   =&gt; 0
</span><span class="boring">  | x+2 =&gt; x
</span>example : sub2 0 = 0 := rfl
example : sub2 1 = 0 := rfl
example : sub2 (x+2) = x := rfl

example : sub2 5 = 3 := rfl
</code></pre>
<p>You can write <code>#print sub2</code> to see how the function was compiled to
recursors. (Lean will tell you that <code>sub2</code> has been defined in terms
of an internal auxiliary function, <code>sub2.match_1</code>, but you can print
that out too.) Lean uses these auxiliary functions to compile <code>match</code> expressions.
Actually, the definition above is expanded to</p>
<pre><code class="language-lean">def sub2 : Nat → Nat :=
  fun x =&gt;
    match x with
    | 0   =&gt; 0
    | 1   =&gt; 0
    | x+2 =&gt; x
</code></pre>
<p>Here are some more examples of nested pattern matching:</p>
<pre><code class="language-lean">example (p q : α → Prop)
        : (∃ x, p x ∨ q x) → (∃ x, p x) ∨ (∃ x, q x)
  | Exists.intro x (Or.inl px) =&gt; Or.inl (Exists.intro x px)
  | Exists.intro x (Or.inr qx) =&gt; Or.inr (Exists.intro x qx)

def foo : Nat × Nat → Nat
  | (0, n)     =&gt; 0
  | (m+1, 0)   =&gt; 1
  | (m+1, n+1) =&gt; 2
</code></pre>
<p>The equation compiler can process multiple arguments sequentially. For
example, it would be more natural to define the previous example as a
function of two arguments:</p>
<pre><code class="language-lean">def foo : Nat → Nat → Nat
  | 0,   n   =&gt; 0
  | m+1, 0   =&gt; 1
  | m+1, n+1 =&gt; 2
</code></pre>
<p>Here is another example:</p>
<pre><code class="language-lean">def bar : List Nat → List Nat → Nat
  | [],      []      =&gt; 0
  | a :: as, []      =&gt; a
  | [],      b :: bs =&gt; b
  | a :: as, b :: bs =&gt; a + b
</code></pre>
<p>Note that the patterns are separated by commas.</p>
<p>In each of the following examples, splitting occurs on only the first
argument, even though the others are included among the list of
patterns.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>def and : Bool → Bool → Bool
  | true,  a =&gt; a
  | false, _ =&gt; false

def or : Bool → Bool → Bool
  | true,  _ =&gt; true
  | false, a =&gt; a

def cond : Bool → α → α → α
  | true,  x, y =&gt; x
  | false, x, y =&gt; y
<span class="boring">end Hidden
</span></code></pre>
<p>Notice also that, when the value of an argument is not needed in the
definition, you can use an underscore instead. This underscore is
known as a <em>wildcard pattern</em>, or an <em>anonymous variable</em>. In contrast
to usage outside the equation compiler, here the underscore does <em>not</em>
indicate an implicit argument. The use of underscores for wildcards is
common in functional programming languages, and so Lean adopts that
notation. <a href="induction_and_recursion.html#wildcards-and-overlapping-patterns">Section Wildcards and Overlapping Patterns</a>
expands on the notion of a wildcard, and <a href="induction_and_recursion.html#inaccessible-patterns">Section Inaccessible Patterns</a> explains how
you can use implicit arguments in patterns as well.</p>
<p>As described in <a href="./inductive_types.html">Chapter Inductive Types</a>,
inductive data types can depend on parameters. The following example defines
the <code>tail</code> function using pattern matching. The argument <code>α : Type u</code>
is a parameter and occurs before the colon to indicate it does not participate in the pattern matching.
Lean also allows parameters to occur after <code>:</code>, but it cannot pattern match on them.</p>
<pre><code class="language-lean">def tail1 {α : Type u} : List α → List α
  | []      =&gt; []
  | a :: as =&gt; as

def tail2 : {α : Type u} → List α → List α
  | α, []      =&gt; []
  | α, a :: as =&gt; as
</code></pre>
<p>Despite the different placement of the parameter <code>α</code> in these two
examples, in both cases it is treated in the same way, in that it does
not participate in a case split.</p>
<p>Lean can also handle more complex forms of pattern matching, in which
arguments to dependent types pose additional constraints on the
various cases. Such examples of <em>dependent pattern matching</em> are
considered in the <a href="induction_and_recursion.html#dependent-pattern-matching">Section Dependent Pattern Matching</a>.</p>
<h2><a class="header" href="#wildcards-and-overlapping-patterns" id="wildcards-and-overlapping-patterns">Wildcards and Overlapping Patterns</a></h2>
<p>Consider one of the examples from the last section:</p>
<pre><code class="language-lean">def foo : Nat → Nat → Nat
  | 0,   n   =&gt; 0
  | m+1, 0   =&gt; 1
  | m+1, n+1 =&gt; 2
</code></pre>
<p>An alternative presentation is:</p>
<pre><code class="language-lean">def foo : Nat → Nat → Nat
  | 0, n =&gt; 0
  | m, 0 =&gt; 1
  | m, n =&gt; 2
</code></pre>
<p>In the second presentation, the patterns overlap; for example, the
pair of arguments <code>0 0</code> matches all three cases. But Lean handles
the ambiguity by using the first applicable equation, so in this example
the net result is the same. In particular, the following equations hold
definitionally:</p>
<pre><code class="language-lean"><span class="boring">def foo : Nat → Nat → Nat
</span><span class="boring">  | 0, n =&gt; 0
</span><span class="boring">  | m, 0 =&gt; 1
</span><span class="boring">  | m, n =&gt; 2
</span>example : foo 0     0     = 0 := rfl
example : foo 0     (n+1) = 0 := rfl
example : foo (m+1) 0     = 1 := rfl
example : foo (m+1) (n+1) = 2 := rfl
</code></pre>
<p>Since the values of <code>m</code> and <code>n</code> are not needed, we can just as well use wildcard patterns instead.</p>
<pre><code class="language-lean">def foo : Nat → Nat → Nat
  | 0, _ =&gt; 0
  | _, 0 =&gt; 1
  | _, _ =&gt; 2
</code></pre>
<p>You can check that this definition of <code>foo</code> satisfies the same
definitional identities as before.</p>
<p>Some functional programming languages support <em>incomplete
patterns</em>. In these languages, the interpreter produces an exception
or returns an arbitrary value for incomplete cases. We can simulate
the arbitrary value approach using the <code>Inhabited</code> type
class. Roughly, an element of <code>Inhabited α</code> is a witness to the fact
that there is an element of <code>α</code>; in the <a href="./type_classes.html">Chapter Type Classes</a>
we will see that Lean can be instructed that suitable
base types are inhabited, and can automatically infer that other
constructed types are inhabited. On this basis, the
standard library provides a default element, <code>default</code>, of
any inhabited type.</p>
<p>We can also use the type <code>Option α</code> to simulate incomplete patterns.
The idea is to return <code>some a</code> for the provided patterns, and use
<code>none</code> for the incomplete cases. The following example demonstrates
both approaches.</p>
<pre><code class="language-lean">def f1 : Nat → Nat → Nat
  | 0, _  =&gt; 1
  | _, 0  =&gt; 2
  | _, _  =&gt; default  -- the &quot;incomplete&quot; case

example : f1 0     0     = 1       := rfl
example : f1 0     (a+1) = 1       := rfl
example : f1 (a+1) 0     = 2       := rfl
example : f1 (a+1) (b+1) = default := rfl

def f2 : Nat → Nat → Option Nat
  | 0, _  =&gt; some 1
  | _, 0  =&gt; some 2
  | _, _  =&gt; none     -- the &quot;incomplete&quot; case

example : f2 0     0     = some 1 := rfl
example : f2 0     (a+1) = some 1 := rfl
example : f2 (a+1) 0     = some 2 := rfl
example : f2 (a+1) (b+1) = none   := rfl
</code></pre>
<p>The equation compiler is clever. If you leave out any of the cases in
the following definition, the error message will let you know what has
not been covered.</p>
<pre><code class="language-lean">def bar : Nat → List Nat → Bool → Nat
  | 0,   _,      false =&gt; 0
  | 0,   b :: _, _     =&gt; b
  | 0,   [],     true  =&gt; 7
  | a+1, [],     false =&gt; a
  | a+1, [],     true  =&gt; a + 1
  | a+1, b :: _, _     =&gt; a + b
</code></pre>
<p>It will also use an &quot;if ... then ... else&quot; instead of a <code>casesOn</code> in appropriate situations.</p>
<pre><code class="language-lean">def foo : Char → Nat
  | 'A' =&gt; 1
  | 'B' =&gt; 2
  | _   =&gt; 3

#print foo.match_1
</code></pre>
<h2><a class="header" href="#structural-recursion-and-induction" id="structural-recursion-and-induction">Structural Recursion and Induction</a></h2>
<p>What makes the equation compiler powerful is that it also supports
recursive definitions. In the next three sections, we will describe,
respectively:</p>
<ul>
<li>structurally recursive definitions</li>
<li>well-founded recursive definitions</li>
<li>mutually recursive definitions</li>
</ul>
<p>Generally speaking, the equation compiler processes input of the following form:</p>
<pre><code>def foo (a : α) : (b : β) → γ
  | [patterns₁] =&gt; t₁
  ...
  | [patternsₙ] =&gt; tₙ
</code></pre>
<p>Here <code>(a : α)</code> is a sequence of parameters, <code>(b : β)</code> is the
sequence of arguments on which pattern matching takes place, and <code>γ</code>
is any type, which can depend on <code>a</code> and <code>b</code>. Each line should
contain the same number of patterns, one for each element of <code>β</code>. As we
have seen, a pattern is either a variable, a constructor applied to
other patterns, or an expression that normalizes to something of that
form (where the non-constructors are marked with the <code>[match_pattern]</code>
attribute). The appearances of constructors prompt case splits, with
the arguments to the constructors represented by the given
variables. In <a href="induction_and_recursion.html#dependent-pattern-matching">Section Dependent Pattern Matching</a>,
we will see that it is sometimes necessary to include explicit terms in patterns that
are needed to make an expression type check, though they do not play a
role in pattern matching. These are called &quot;inaccessible patterns&quot; for
that reason. But we will not need to use such inaccessible patterns
before <a href="induction_and_recursion.html#dependent-pattern-matching">Section Dependent Pattern Matching</a>.</p>
<p>As we saw in the last section, the terms <code>t₁, ..., tₙ</code> can make use
of any of the parameters <code>a</code>, as well as any of the variables that
are introduced in the corresponding patterns. What makes recursion and
induction possible is that they can also involve recursive calls to
<code>foo</code>. In this section, we will deal with <em>structural recursion</em>, in
which the arguments to <code>foo</code> occurring on the right-hand side of the
<code>=&gt;</code> are subterms of the patterns on the left-hand side. The idea is
that they are structurally smaller, and hence appear in the inductive
type at an earlier stage. Here are some examples of structural
recursion from the last chapter, now defined using the equation
compiler:</p>
<pre><code class="language-lean">open Nat
def add : Nat → Nat → Nat
  | m, zero   =&gt; m
  | m, succ n =&gt; succ (add m n)

theorem add_zero (m : Nat)   : add m zero = m := rfl
theorem add_succ (m n : Nat) : add m (succ n) = succ (add m n) := rfl

theorem zero_add : ∀ n, add zero n = n
  | zero   =&gt; rfl
  | succ n =&gt; congrArg succ (zero_add n)

def mul : Nat → Nat → Nat
  | n, zero   =&gt; zero
  | n, succ m =&gt; add (mul n m) n
</code></pre>
<p>The proof of <code>zero_add</code> makes it clear that proof by induction is
really a form of recursion in Lean.</p>
<p>The example above shows that the defining equations for <code>add</code> hold
definitionally, and the same is true of <code>mul</code>. The equation compiler
tries to ensure that this holds whenever possible, as is the case with
straightforward structural induction. In other situations, however,
reductions hold only <em>propositionally</em>, which is to say, they are
equational theorems that must be applied explicitly. The equation
compiler generates such theorems internally. They are not meant to be
used directly by the user; rather, the <code>simp</code> tactic
is configured to use them when necessary. Thus both of the following
proofs of <code>zero_add</code> work:</p>
<pre><code class="language-lean">open Nat
<span class="boring">def add : Nat → Nat → Nat
</span><span class="boring">  | m, zero   =&gt; m
</span><span class="boring">  | m, succ n =&gt; succ (add m n)
</span>theorem zero_add : ∀ n, add zero n = n
  | zero   =&gt; by simp [add]
  | succ n =&gt; by simp [add, zero_add]
</code></pre>
<p>As with definition by pattern matching, parameters to a structural
recursion or induction may appear before the colon. Such parameters
are simply added to the local context before the definition is
processed. For example, the definition of addition may also be written
as follows:</p>
<pre><code class="language-lean">open Nat
def add (m : Nat) : Nat → Nat
  | zero   =&gt; m
  | succ n =&gt; succ (add m n)
</code></pre>
<p>You can also write the example above using <code>match</code>.</p>
<pre><code class="language-lean">open Nat
def add (m n : Nat) : Nat :=
  match n with
  | zero   =&gt; m
  | succ n =&gt; succ (add m n)
</code></pre>
<p>A more interesting example of structural recursion is given by the Fibonacci function <code>fib</code>.</p>
<pre><code class="language-lean">def fib : Nat → Nat
  | 0   =&gt; 1
  | 1   =&gt; 1
  | n+2 =&gt; fib (n+1) + fib n

example : fib 0 = 1 := rfl
example : fib 1 = 1 := rfl
example : fib (n + 2) = fib (n + 1) + fib n := rfl

example : fib 7 = 21 := rfl
</code></pre>
<p>Here, the value of the <code>fib</code> function at <code>n + 2</code> (which is
definitionally equal to <code>succ (succ n)</code>) is defined in terms of the
values at <code>n + 1</code> (which is definitionally equivalent to <code>succ n</code>)
and the value at <code>n</code>. This is a notoriously inefficient way of
computing the Fibonacci function, however, with an execution time that
is exponential in <code>n</code>. Here is a better way:</p>
<pre><code class="language-lean">def fibFast (n : Nat) : Nat :=
  (loop n).2
where
  loop : Nat → Nat × Nat
    | 0   =&gt; (0, 1)
    | n+1 =&gt; let p := loop n; (p.2, p.1 + p.2)

#eval fibFast 100
</code></pre>
<p>Here is the same definition using a <code>let rec</code> instead of a <code>where</code>.</p>
<pre><code class="language-lean">def fibFast (n : Nat) : Nat :=
  let rec loop : Nat → Nat × Nat
    | 0   =&gt; (0, 1)
    | n+1 =&gt; let p := loop n; (p.2, p.1 + p.2)
  (loop n).2
</code></pre>
<p>In both cases, Lean generates the auxiliary function <code>fibFast.loop</code>.</p>
<p>To handle structural recursion, the equation compiler uses
<em>course-of-values</em> recursion, using constants <code>below</code> and <code>brecOn</code>
that are automatically generated with each inductively defined
type. You can get a sense of how it works by looking at the types of
<code>Nat.below</code> and <code>Nat.brecOn</code>:</p>
<pre><code class="language-lean">variable (C : Nat → Type u)

#check (@Nat.below C : Nat → Type u)

#reduce @Nat.below C (3 : Nat)

#check (@Nat.brecOn C : (n : Nat) → ((n : Nat) → @Nat.below C n → C n) → C n)
</code></pre>
<p>The type <code>@Nat.below C (3 : nat)</code> is a data structure that stores elements of <code>C 0</code>, <code>C 1</code>, and <code>C 2</code>.
The course-of-values recursion is implemented by <code>Nat.brecOn</code>. It enables us to define the value of a dependent
function of type <code>(n : Nat) → C n</code> at a particular input <code>n</code> in terms of all the previous values of the function,
presented as an element of <code>@Nat.below C n</code>.</p>
<p>The use of course-of-values recursion is one of the techniques the equation compiler uses to justify to
the Lean kernel that a function terminates. It does not affect the code generator which compiles recursive
functions as other functional programming language compilers. Recall that <code>#eval fib &lt;n&gt;</code> is exponential on <code>&lt;n&gt;</code>.
On the other hand, <code>#reduce fib &lt;n&gt;</code> is efficient because it uses the definition sent to the kernel that
is based on the <code>brecOn</code> construction.</p>
<pre><code class="language-lean">def fib : Nat → Nat
  | 0   =&gt; 1
  | 1   =&gt; 1
  | n+2 =&gt; fib (n+1) + fib n

-- #eval fib 50 -- slow
#reduce fib 50  -- fast

#print fib
</code></pre>
<p>Another good example of a recursive definition is the list <code>append</code> function.</p>
<pre><code class="language-lean">def append : List α → List α → List α
  | [],    bs =&gt; bs
  | a::as, bs =&gt; a :: append as bs

example : append [1, 2, 3] [4, 5] = [1, 2, 3, 4, 5] := rfl
</code></pre>
<p>Here is another: it adds elements of the first list to elements of the second list, until one of the two lists runs out.</p>
<pre><code class="language-lean">def listAdd [Add α] : List α → List α → List α
  | [],      _       =&gt; []
  | _,       []      =&gt; []
  | a :: as, b :: bs =&gt; (a + b) :: listAdd as bs

#eval listAdd [1, 2, 3] [4, 5, 6, 6, 9, 10]
-- [5, 7, 9]
</code></pre>
<p>You are encouraged to experiment with similar examples in the exercises below.</p>
<h2><a class="header" href="#local-recursive-declarations" id="local-recursive-declarations">Local recursive declarations</a></h2>
<p>You can define local recursive declarations using the <code>let rec</code> keyword.</p>
<pre><code class="language-lean">def replicate (n : Nat) (a : α) : List α :=
  let rec loop : Nat → List α → List α
    | 0,   as =&gt; as
    | n+1, as =&gt; loop n (a::as)
  loop n []

#check @replicate.loop
-- {α : Type} → α → Nat → List α → List α
</code></pre>
<p>Lean creates an auxiliary declaration for each <code>let rec</code>. In the example above,
it created the declaration <code>replicate.loop</code> for the <code>let rec loop</code> occurring at <code>replicate</code>.
Note that, Lean &quot;closes&quot; the declaration by adding any local variable occurring in the
<code>let rec</code> declaration as additional parameters. For example, the local variable <code>a</code> occurs
at <code>let rec loop</code>.</p>
<p>You can also use <code>let rec</code> in tactic mode and for creating proofs by induction.</p>
<pre><code class="language-lean"><span class="boring">def replicate (n : Nat) (a : α) : List α :=
</span><span class="boring"> let rec loop : Nat → List α → List α
</span><span class="boring">   | 0,   as =&gt; as
</span><span class="boring">   | n+1, as =&gt; loop n (a::as)
</span><span class="boring"> loop n []
</span>theorem length_replicate (n : Nat) (a : α) : (replicate n a).length = n := by
  let rec aux (n : Nat) (as : List α)
              : (replicate.loop a n as).length = n + as.length := by
    match n with
    | 0   =&gt; simp [replicate.loop]
    | n+1 =&gt; simp [replicate.loop, aux n, Nat.add_succ, Nat.succ_add]
  exact aux n []
</code></pre>
<p>You can also introduce auxiliary recursive declarations using <code>where</code> clause after your definition.
Lean converts them into a <code>let rec</code>.</p>
<pre><code class="language-lean">def replicate (n : Nat) (a : α) : List α :=
  loop n []
where
  loop : Nat → List α → List α
    | 0,   as =&gt; as
    | n+1, as =&gt; loop n (a::as)

theorem length_replicate (n : Nat) (a : α) : (replicate n a).length = n := by
  exact aux n []
where
  aux (n : Nat) (as : List α)
      : (replicate.loop a n as).length = n + as.length := by
    match n with
    | 0   =&gt; simp [replicate.loop]
    | n+1 =&gt; simp [replicate.loop, aux n, Nat.add_succ, Nat.succ_add]
</code></pre>
<h2><a class="header" href="#well-founded-recursion-and-induction" id="well-founded-recursion-and-induction">Well-Founded Recursion and Induction</a></h2>
<p>When structural recursion cannot be used, we can prove termination using well-founded recursion.
We need a well-founded relation and a proof that each recursive application is decreasing with respect to
this relation. Dependent type theory is powerful enough to encode and justify
well-founded recursion. Let us start with the logical background that
is needed to understand how it works.</p>
<p>Lean's standard library defines two predicates, <code>Acc r a</code> and
<code>WellFounded r</code>, where <code>r</code> is a binary relation on a type <code>α</code>,
and <code>a</code> is an element of type <code>α</code>.</p>
<pre><code class="language-lean">variable (α : Sort u)
variable (r : α → α → Prop)

#check (Acc r : α → Prop)
#check (WellFounded r : Prop)
</code></pre>
<p>The first, <code>Acc</code>, is an inductively defined predicate. According to
its definition, <code>Acc r x</code> is equivalent to
<code>∀ y, r y x → Acc r y</code>. If you think of <code>r y x</code> as denoting a kind of order relation
<code>y ≺ x</code>, then <code>Acc r x</code> says that <code>x</code> is accessible from below,
in the sense that all its predecessors are accessible. In particular,
if <code>x</code> has no predecessors, it is accessible. Given any type <code>α</code>,
we should be able to assign a value to each accessible element of
<code>α</code>, recursively, by assigning values to all its predecessors first.</p>
<p>The statement that <code>r</code> is well founded, denoted <code>WellFounded r</code>,
is exactly the statement that every element of the type is
accessible. By the above considerations, if <code>r</code> is a well-founded
relation on a type <code>α</code>, we should have a principle of well-founded
recursion on <code>α</code>, with respect to the relation <code>r</code>. And, indeed,
we do: the standard library defines <code>WellFounded.fix</code>, which serves
exactly that purpose.</p>
<pre><code class="language-lean">noncomputable def f {α : Sort u}
      (r : α → α → Prop)
      (h : WellFounded r)
      (C : α → Sort v)
      (F : (x : α) → ((y : α) → r y x → C y) → C x)
      : (x : α) → C x := WellFounded.fix h F
</code></pre>
<p>There is a long cast of characters here, but the first block we have
already seen: the type, <code>α</code>, the relation, <code>r</code>, and the
assumption, <code>h</code>, that <code>r</code> is well founded. The variable <code>C</code>
represents the motive of the recursive definition: for each element
<code>x : α</code>, we would like to construct an element of <code>C x</code>. The
function <code>F</code> provides the inductive recipe for doing that: it tells
us how to construct an element <code>C x</code>, given elements of <code>C y</code> for
each predecessor <code>y</code> of <code>x</code>.</p>
<p>Note that <code>WellFounded.fix</code> works equally well as an induction
principle. It says that if <code>≺</code> is well founded and you want to prove
<code>∀ x, C x</code>, it suffices to show that for an arbitrary <code>x</code>, if we
have <code>∀ y ≺ x, C y</code>, then we have <code>C x</code>.</p>
<p>In the example above we use the modifier <code>noncomputable</code> because the code
generator currently does not support <code>WellFounded.fix</code>. The function
<code>WellFounded.fix</code> is another tool Lean uses to justify that a function
terminates.</p>
<p>Lean knows that the usual order <code>&lt;</code> on the natural numbers is well
founded. It also knows a number of ways of constructing new well
founded orders from others, for example, using lexicographic order.</p>
<p>Here is essentially the definition of division on the natural numbers that is found in the standard library.</p>
<pre><code class="language-lean">open Nat

theorem div_lemma {x y : Nat} : 0 &lt; y ∧ y ≤ x → x - y &lt; x :=
  fun h =&gt; sub_lt (Nat.lt_of_lt_of_le h.left h.right) h.left

def div.F (x : Nat) (f : (x₁ : Nat) → x₁ &lt; x → Nat → Nat) (y : Nat) : Nat :=
  if h : 0 &lt; y ∧ y ≤ x then
    f (x - y) (div_lemma h) y + 1
  else
    zero

noncomputable def div := WellFounded.fix (measure id).wf div.F

#reduce div 8 2 -- 4
</code></pre>
<p>The definition is somewhat inscrutable. Here the recursion is on
<code>x</code>, and <code>div.F x f : Nat → Nat</code> returns the &quot;divide by <code>y</code>&quot;
function for that fixed <code>x</code>. You have to remember that the second
argument to <code>div.F</code>, the recipe for the recursion, is a function
that is supposed to return the divide by <code>y</code> function for all values
<code>x₁</code> smaller than <code>x</code>.</p>
<p>The elaborator is designed to make definitions like this more
convenient. It accepts the following:</p>
<pre><code class="language-lean">def div (x y : Nat) : Nat :=
  if h : 0 &lt; y ∧ y ≤ x then
    have : x - y &lt; x := Nat.sub_lt (Nat.lt_of_lt_of_le h.1 h.2) h.1
    div (x - y) y + 1
  else
    0
</code></pre>
<p>When Lean encounters a recursive definition, it first
tries structural recursion, and only when that fails, does it fall
back on well-founded recursion. Lean uses the tactic <code>decreasing_tactic</code>
to show that the recursive applications are smaller. The auxiliary
proposition <code>x - y &lt; x</code> in the example above should be viewed as a hint
for this tactic.</p>
<p>The defining equation for <code>div</code> does <em>not</em> hold definitionally, but
we can unfold <code>div</code> using the <code>unfold</code> tactic. We use <a href="./conv.html"><code>conv</code></a> to select which
<code>div</code> application we want to unfold.</p>
<pre><code class="language-lean"><span class="boring">def div (x y : Nat) : Nat :=
</span><span class="boring"> if h : 0 &lt; y ∧ y ≤ x then
</span><span class="boring">   have : x - y &lt; x := Nat.sub_lt (Nat.lt_of_lt_of_le h.1 h.2) h.1
</span><span class="boring">   div (x - y) y + 1
</span><span class="boring"> else
</span><span class="boring">   0
</span>example (x y : Nat) : div x y = if 0 &lt; y ∧ y ≤ x then div (x - y) y + 1 else 0 := by
  conv =&gt; lhs; unfold div -- unfold occurrence in the left-hand-side of the equation

example (x y : Nat) (h : 0 &lt; y ∧ y ≤ x) : div x y = div (x - y) y + 1 := by
  conv =&gt; lhs; unfold div
  simp [h]
</code></pre>
<p>The following example is similar: it converts any natural number to a
binary expression, represented as a list of 0's and 1's. We have to
provide evidence that the recursive call is
decreasing, which we do here with a <code>sorry</code>. The <code>sorry</code> does not
prevent the interpreter from evaluating the function successfully.</p>
<pre><code class="language-lean">def natToBin : Nat → List Nat
  | 0     =&gt; [0]
  | 1     =&gt; [1]
  | n + 2 =&gt;
    have : (n + 2) / 2 &lt; n + 2 := sorry
    natToBin ((n + 2) / 2) ++ [n % 2]

#eval natToBin 1234567
</code></pre>
<p>As a final example, we observe that Ackermann's function can be
defined directly, because it is justified by the well foundedness of
the lexicographic order on the natural numbers. The <code>termination_by</code> clause
instructs Lean to use a lexicographic order. This clause is actually mapping
the function arguments to elements of type <code>Nat × Nat</code>. Then, Lean uses typeclass
resolution to synthesize an element of type <code>WellFoundedRelation (Nat × Nat)</code>.</p>
<pre><code class="language-lean">def ack : Nat → Nat → Nat
  | 0,   y   =&gt; y+1
  | x+1, 0   =&gt; ack x 1
  | x+1, y+1 =&gt; ack x (ack (x+1) y)
termination_by x y =&gt; (x, y)
</code></pre>
<p>Note that a lexicographic order is used in the example above because the instance
<code>WellFoundedRelation (α × β)</code> uses a lexicographic order. Lean also defines the instance</p>
<pre><code class="language-lean">instance (priority := low) [SizeOf α] : WellFoundedRelation α :=
  sizeOfWFRel
</code></pre>
<p>In the following example, we prove termination by showing that <code>as.size - i</code> is decreasing
in the recursive application.</p>
<pre><code class="language-lean">def takeWhile (p : α → Bool) (as : Array α) : Array α :=
  go 0 #[]
where
  go (i : Nat) (r : Array α) : Array α :=
    if h : i &lt; as.size then
      let a := as.get ⟨i, h⟩
      if p a then
        go (i+1) (r.push a)
      else
        r
    else
      r
  termination_by as.size - i
</code></pre>
<p>Note that, auxiliary function <code>go</code> is recursive in this example, but <code>takeWhile</code> is not.</p>
<p>By default, Lean uses the tactic <code>decreasing_tactic</code> to prove recursive applications are decreasing. The modifier <code>decreasing_by</code> allows us to provide our own tactic. Here is an example.</p>
<pre><code class="language-lean">theorem div_lemma {x y : Nat} : 0 &lt; y ∧ y ≤ x → x - y &lt; x :=
  fun ⟨ypos, ylex⟩ =&gt; Nat.sub_lt (Nat.lt_of_lt_of_le ypos ylex) ypos

def div (x y : Nat) : Nat :=
  if h : 0 &lt; y ∧ y ≤ x then
    div (x - y) y + 1
  else
    0
decreasing_by apply div_lemma; assumption
</code></pre>
<p>Note that <code>decreasing_by</code> is not replacement for <code>termination_by</code>, they complement each other. <code>termination_by</code> is used to specify a well-founded relation, and <code>decreasing_by</code> for providing our own tactic for showing recursive applications are decreasing. In the following example, we use both of them.</p>
<pre><code class="language-lean">def ack : Nat → Nat → Nat
  | 0,   y   =&gt; y+1
  | x+1, 0   =&gt; ack x 1
  | x+1, y+1 =&gt; ack x (ack (x+1) y)
termination_by x y =&gt; (x, y)
decreasing_by
  all_goals simp_wf -- unfolds well-founded recursion auxiliary definitions
  · apply Prod.Lex.left; simp_arith
  · apply Prod.Lex.right; simp_arith
  · apply Prod.Lex.left; simp_arith
</code></pre>
<p>We can use <code>decreasing_by sorry</code> to instruct Lean to &quot;trust&quot; us that the function terminates.</p>
<pre><code class="language-lean">def natToBin : Nat → List Nat
  | 0     =&gt; [0]
  | 1     =&gt; [1]
  | n + 2 =&gt; natToBin ((n + 2) / 2) ++ [n % 2]
decreasing_by sorry

#eval natToBin 1234567
</code></pre>
<p>Recall that using <code>sorry</code> is equivalent to using a new axiom, and should be avoided. In the following example, we used the <code>sorry</code> to prove <code>False</code>. The command <code>#print axioms</code> shows that <code>unsound</code> depends on the unsound axiom <code>sorryAx</code> used to implement <code>sorry</code>.</p>
<pre><code class="language-lean">def unsound (x : Nat) : False :=
  unsound (x + 1)
decreasing_by sorry

#check unsound 0
-- `unsound 0` is a proof of `False`

#print axioms unsound
-- 'unsound' depends on axioms: [sorryAx]
</code></pre>
<p>Summary:</p>
<ul>
<li>
<p>If there is no <code>termination_by</code>, a well-founded relation is derived (if possible) by selecting an argument and then using typeclass resolution to synthesize a well-founded relation for this argument's type.</p>
</li>
<li>
<p>If <code>termination_by</code> is specified, it maps the arguments of the function to a type <code>α</code> and type class resolution is again used. Recall that, the default instance for <code>β × γ</code> is a lexicographic order based on the well-founded relations for <code>β</code> and <code>γ</code>.</p>
</li>
<li>
<p>The default well-founded relation instance for <code>Nat</code> is <code>&lt;</code>.</p>
</li>
<li>
<p>By default, the tactic <code>decreasing_tactic</code> is used to show that recursive applications are smaller with respect to the selected well-founded relation. If <code>decreasing_tactic</code> fails, the error message includes the remaining goal <code>... |- G</code>. Note that, the <code>decreasing_tactic</code> uses <code>assumption</code>. So, you can include a <code>have</code>-expression to prove goal <code>G</code>. You can also provide your own tactic using <code>decreasing_by</code>.</p>
</li>
</ul>
<h2><a class="header" href="#mutual-recursion" id="mutual-recursion">Mutual Recursion</a></h2>
<p>Lean also supports mutual recursive definitions. The syntax is similar to that for mutual inductive types. Here is an example:</p>
<pre><code class="language-lean">mutual
  def even : Nat → Bool
    | 0   =&gt; true
    | n+1 =&gt; odd n

  def odd : Nat → Bool
    | 0   =&gt; false
    | n+1 =&gt; even n
end

example : even (a + 1) = odd a := by
  simp [even]

example : odd (a + 1) = even a := by
  simp [odd]

theorem even_eq_not_odd : ∀ a, even a = not (odd a) := by
  intro a; induction a
  . simp [even, odd]
  . simp [even, odd, *]
</code></pre>
<p>What makes this a mutual definition is that <code>even</code> is defined recursively in terms of <code>odd</code>, while <code>odd</code> is defined recursively in terms of <code>even</code>. Under the hood, this is compiled as a single recursive definition. The internally defined function takes, as argument, an element of a sum type, either an input to <code>even</code>, or an input to <code>odd</code>. It then returns an output appropriate to the input. To define that function, Lean uses a suitable well-founded measure. The internals are meant to be hidden from users; the canonical way to make use of such definitions is to use <code>simp</code> (or <code>unfold</code>), as we did above.</p>
<p>Mutual recursive definitions also provide natural ways of working with mutual and nested inductive types. Recall the definition of <code>Even</code> and <code>Odd</code> as mutual inductive predicates as presented before.</p>
<pre><code class="language-lean">mutual
  inductive Even : Nat → Prop where
    | even_zero : Even 0
    | even_succ : ∀ n, Odd n → Even (n + 1)

  inductive Odd : Nat → Prop where
    | odd_succ : ∀ n, Even n → Odd (n + 1)
end
</code></pre>
<p>The constructors, <code>even_zero</code>, <code>even_succ</code>, and <code>odd_succ</code> provide positive means for showing that a number is even or odd. We need to use the fact that the inductive type is generated by these constructors to know that zero is not odd, and that the latter two implications reverse. As usual, the constructors are kept in a namespace that is named after the type being defined, and the command <code>open Even Odd</code> allows us to access them more conveniently.</p>
<pre><code class="language-lean"><span class="boring">mutual
</span><span class="boring"> inductive Even : Nat → Prop where
</span><span class="boring">   | even_zero : Even 0
</span><span class="boring">   | even_succ : ∀ n, Odd n → Even (n + 1)
</span><span class="boring"> inductive Odd : Nat → Prop where
</span><span class="boring">   | odd_succ : ∀ n, Even n → Odd (n + 1)
</span><span class="boring">end
</span>open Even Odd

theorem not_odd_zero : ¬ Odd 0 :=
  fun h =&gt; nomatch h

theorem even_of_odd_succ : ∀ n, Odd (n + 1) → Even n
  | _, odd_succ n h =&gt; h

theorem odd_of_even_succ : ∀ n, Even (n + 1) → Odd n
  | _, even_succ n h =&gt; h
</code></pre>
<p>For another example, suppose we use a nested inductive type to define a set of terms inductively, so that a term is either a constant (with a name given by a string), or the result of applying a constant to a list of constants.</p>
<pre><code class="language-lean">inductive Term where
  | const : String → Term
  | app   : String → List Term → Term
</code></pre>
<p>We can then use a mutual recursive definition to count the number of constants occurring in a term, as well as the number occurring in a list of terms.</p>
<pre><code class="language-lean"><span class="boring">inductive Term where
</span><span class="boring"> | const : String → Term
</span><span class="boring"> | app   : String → List Term → Term
</span>namespace Term

mutual
  def numConsts : Term → Nat
    | const _ =&gt; 1
    | app _ cs =&gt; numConstsLst cs

  def numConstsLst : List Term → Nat
    | [] =&gt; 0
    | c :: cs =&gt; numConsts c + numConstsLst cs
end

def sample := app &quot;f&quot; [app &quot;g&quot; [const &quot;x&quot;], const &quot;y&quot;]

#eval numConsts sample

end Term
</code></pre>
<p>As a final example, we define a function <code>replaceConst a b e</code> that replaces a constant <code>a</code> with <code>b</code> in a term <code>e</code>, and then prove the number of constants is the same. Note that, our proof uses mutual recursion (aka induction).</p>
<pre><code class="language-lean"><span class="boring">inductive Term where
</span><span class="boring"> | const : String → Term
</span><span class="boring"> | app   : String → List Term → Term
</span><span class="boring">namespace Term
</span><span class="boring">mutual
</span><span class="boring"> def numConsts : Term → Nat
</span><span class="boring">   | const _ =&gt; 1
</span><span class="boring">   | app _ cs =&gt; numConstsLst cs
</span><span class="boring">  def numConstsLst : List Term → Nat
</span><span class="boring">   | [] =&gt; 0
</span><span class="boring">   | c :: cs =&gt; numConsts c + numConstsLst cs
</span><span class="boring">end
</span>mutual
  def replaceConst (a b : String) : Term → Term
    | const c =&gt; if a == c then const b else const c
    | app f cs =&gt; app f (replaceConstLst a b cs)

  def replaceConstLst (a b : String) : List Term → List Term
    | [] =&gt; []
    | c :: cs =&gt; replaceConst a b c :: replaceConstLst a b cs
end

mutual
  theorem numConsts_replaceConst (a b : String) (e : Term)
            : numConsts (replaceConst a b e) = numConsts e := by
    match e with
    | const c =&gt; simp [replaceConst]; split &lt;;&gt; simp [numConsts]
    | app f cs =&gt; simp [replaceConst, numConsts, numConsts_replaceConstLst a b cs]

  theorem numConsts_replaceConstLst (a b : String) (es : List Term)
            : numConstsLst (replaceConstLst a b es) = numConstsLst es := by
    match es with
    | [] =&gt; simp [replaceConstLst, numConstsLst]
    | c :: cs =&gt;
      simp [replaceConstLst, numConstsLst, numConsts_replaceConst a b c,
            numConsts_replaceConstLst a b cs]
end
</code></pre>
<h2><a class="header" href="#dependent-pattern-matching" id="dependent-pattern-matching">Dependent Pattern Matching</a></h2>
<p>All the examples of pattern matching we considered in
<a href="induction_and_recursion.html#pattern-matching">Section Pattern Matching</a> can easily be written using <code>casesOn</code>
and <code>recOn</code>. However, this is often not the case with indexed
inductive families such as <code>Vector α n</code>, since case splits impose
constraints on the values of the indices. Without the equation
compiler, we would need a lot of boilerplate code to define very
simple functions such as <code>map</code>, <code>zip</code>, and <code>unzip</code> using
recursors. To understand the difficulty, consider what it would take
to define a function <code>tail</code> which takes a vector
<code>v : Vector α (succ n)</code> and deletes the first element. A first thought might be to
use the <code>casesOn</code> function:</p>
<pre><code class="language-lean">inductive Vector (α : Type u) : Nat → Type u
  | nil  : Vector α 0
  | cons : α → {n : Nat} → Vector α n → Vector α (n+1)

namespace Vector

#check @Vector.casesOn
/-
  {α : Type u}
  → {motive : (a : Nat) → Vector α a → Sort v} →
  → {a : Nat} → (t : Vector α a)
  → motive 0 nil
  → ((a : α) → {n : Nat} → (a_1 : Vector α n) → motive (n + 1) (cons a a_1))
  → motive a t
-/

end Vector
</code></pre>
<p>But what value should we return in the <code>nil</code> case? Something funny
is going on: if <code>v</code> has type <code>Vector α (succ n)</code>, it <em>can't</em> be
nil, but it is not clear how to tell that to <code>casesOn</code>.</p>
<p>One solution is to define an auxiliary function:</p>
<pre><code class="language-lean"><span class="boring">inductive Vector (α : Type u) : Nat → Type u
</span><span class="boring">  | nil  : Vector α 0
</span><span class="boring">  | cons : α → {n : Nat} → Vector α n → Vector α (n+1)
</span><span class="boring">namespace Vector
</span>def tailAux (v : Vector α m) : m = n + 1 → Vector α n :=
  Vector.casesOn (motive := fun x _ =&gt; x = n + 1 → Vector α n) v
    (fun h : 0 = n + 1 =&gt; Nat.noConfusion h)
    (fun (a : α) (m : Nat) (as : Vector α m) =&gt;
     fun (h : m + 1 = n + 1) =&gt;
       Nat.noConfusion h (fun h1 : m = n =&gt; h1 ▸ as))

def tail (v : Vector α (n+1)) : Vector α n :=
  tailAux v rfl
<span class="boring">end Vector
</span></code></pre>
<p>In the <code>nil</code> case, <code>m</code> is instantiated to <code>0</code>, and
<code>noConfusion</code> makes use of the fact that <code>0 = succ n</code> cannot
occur.  Otherwise, <code>v</code> is of the form <code>a :: w</code>, and we can simply
return <code>w</code>, after casting it from a vector of length <code>m</code> to a
vector of length <code>n</code>.</p>
<p>The difficulty in defining <code>tail</code> is to maintain the relationships between the indices.
The hypothesis <code>e : m = n + 1</code> in <code>tailAux</code> is used to communicate the relationship
between <code>n</code> and the index associated with the minor premise.
Moreover, the <code>zero = n + 1</code> case is unreachable, and the canonical way to discard such
a case is to use <code>noConfusion</code>.</p>
<p>The <code>tail</code> function is, however, easy to define using recursive
equations, and the equation compiler generates all the boilerplate
code automatically for us. Here are a number of similar examples:</p>
<pre><code class="language-lean"><span class="boring">inductive Vector (α : Type u) : Nat → Type u
</span><span class="boring">  | nil  : Vector α 0
</span><span class="boring">  | cons : α → {n : Nat} → Vector α n → Vector α (n+1)
</span><span class="boring">namespace Vector
</span>def head : {n : Nat} → Vector α (n+1) → α
  | n, cons a as =&gt; a

def tail : {n : Nat} → Vector α (n+1) → Vector α n
  | n, cons a as =&gt; as

theorem eta : ∀ {n : Nat} (v : Vector α (n+1)), cons (head v) (tail v) = v
  | n, cons a as =&gt; rfl

def map (f : α → β → γ) : {n : Nat} → Vector α n → Vector β n → Vector γ n
  | 0,   nil,       nil       =&gt; nil
  | n+1, cons a as, cons b bs =&gt; cons (f a b) (map f as bs)

def zip : {n : Nat} → Vector α n → Vector β n → Vector (α × β) n
  | 0,   nil,       nil       =&gt; nil
  | n+1, cons a as, cons b bs =&gt; cons (a, b) (zip as bs)
<span class="boring">end Vector
</span></code></pre>
<p>Note that we can omit recursive equations for &quot;unreachable&quot; cases such
as <code>head nil</code>. The automatically generated definitions for indexed
families are far from straightforward. For example:</p>
<pre><code class="language-lean"><span class="boring">inductive Vector (α : Type u) : Nat → Type u
</span><span class="boring">  | nil  : Vector α 0
</span><span class="boring">  | cons : α → {n : Nat} → Vector α n → Vector α (n+1)
</span><span class="boring">namespace Vector
</span>def map (f : α → β → γ) : {n : Nat} → Vector α n → Vector β n → Vector γ n
  | 0,   nil,       nil       =&gt; nil
  | n+1, cons a as, cons b bs =&gt; cons (f a b) (map f as bs)

#print map
#print map.match_1
<span class="boring">end Vector
</span></code></pre>
<p>The <code>map</code> function is even more tedious to define by hand than the
<code>tail</code> function. We encourage you to try it, using <code>recOn</code>,
<code>casesOn</code> and <code>noConfusion</code>.</p>
<h2><a class="header" href="#inaccessible-patterns" id="inaccessible-patterns">Inaccessible Patterns</a></h2>
<p>Sometimes an argument in a dependent matching pattern is not essential
to the definition, but nonetheless has to be included to specialize
the type of the expression appropriately. Lean allows users to mark
such subterms as <em>inaccessible</em> for pattern matching. These
annotations are essential, for example, when a term occurring in the
left-hand side is neither a variable nor a constructor application,
because these are not suitable targets for pattern matching. We can
view such inaccessible patterns as &quot;don't care&quot; components of the
patterns. You can declare a subterm inaccessible by writing
<code>.(t)</code>. If the inaccessible pattern can be inferred, you can also write
<code>_</code>.</p>
<p>The following example, we declare an inductive type that defines the
property of &quot;being in the image of <code>f</code>&quot;. You can view an element of
the type <code>ImageOf f b</code> as evidence that <code>b</code> is in the image of
<code>f</code>, whereby the constructor <code>imf</code> is used to build such
evidence. We can then define any function <code>f</code> with an &quot;inverse&quot;
which takes anything in the image of <code>f</code> to an element that is
mapped to it. The typing rules forces us to write <code>f a</code> for the
first argument, but this term is neither a variable nor a constructor
application, and plays no role in the pattern-matching definition. To
define the function <code>inverse</code> below, we <em>have to</em> mark <code>f a</code>
inaccessible.</p>
<pre><code class="language-lean">inductive ImageOf {α β : Type u} (f : α → β) : β → Type u where
  | imf : (a : α) → ImageOf f (f a)

open ImageOf

def inverse {f : α → β} : (b : β) → ImageOf f b → α
  | .(f a), imf a =&gt; a

def inverse' {f : α → β} : (b : β) → ImageOf f b → α
  | _, imf a =&gt; a
</code></pre>
<p>In the example above, the inaccessible annotation makes it clear that
<code>f</code> is <em>not</em> a pattern matching variable.</p>
<p>Inaccessible patterns can be used to clarify and control definitions that
make use of dependent pattern matching. Consider the following
definition of the function <code>Vector.add</code>, which adds two vectors of
elements of a type, assuming that type has an associated addition
function:</p>
<pre><code class="language-lean">inductive Vector (α : Type u) : Nat → Type u
  | nil  : Vector α 0
  | cons : α → {n : Nat} → Vector α n → Vector α (n+1)

namespace Vector

def add [Add α] : {n : Nat} → Vector α n → Vector α n → Vector α n
  | 0,   nil,       nil       =&gt; nil
  | n+1, cons a as, cons b bs =&gt; cons (a + b) (add as bs)

end Vector
</code></pre>
<p>The argument <code>{n : Nat}</code> appear after the colon, because it cannot
be held fixed throughout the definition.  When implementing this
definition, the equation compiler starts with a case distinction as to
whether the first argument is <code>0</code> or of the form <code>n+1</code>.  This is
followed by nested case splits on the next two arguments, and in each
case the equation compiler rules out the cases are not compatible with
the first pattern.</p>
<p>But, in fact, a case split is not required on the first argument; the
<code>casesOn</code> eliminator for <code>Vector</code> automatically abstracts this
argument and replaces it by <code>0</code> and <code>n + 1</code> when we do a case
split on the second argument. Using inaccessible patterns, we can prompt
the equation compiler to avoid the case split on <code>n</code></p>
<pre><code class="language-lean"><span class="boring">inductive Vector (α : Type u) : Nat → Type u
</span><span class="boring">  | nil  : Vector α 0
</span><span class="boring">  | cons : α → {n : Nat} → Vector α n → Vector α (n+1)
</span><span class="boring">namespace Vector
</span>def add [Add α] : {n : Nat} → Vector α n → Vector α n → Vector α n
  | .(_), nil,       nil       =&gt; nil
  | .(_), cons a as, cons b bs =&gt; cons (a + b) (add as bs)
<span class="boring">end Vector
</span></code></pre>
<p>Marking the position as an inaccessible pattern tells the
equation compiler first, that the form of the argument should be
inferred from the constraints posed by the other arguments, and,
second, that the first argument should <em>not</em> participate in pattern
matching.</p>
<p>The inaccessible pattern <code>.(_)</code> can be written as <code>_</code> for convenience.</p>
<pre><code class="language-lean"><span class="boring">inductive Vector (α : Type u) : Nat → Type u
</span><span class="boring">  | nil  : Vector α 0
</span><span class="boring">  | cons : α → {n : Nat} → Vector α n → Vector α (n+1)
</span><span class="boring">namespace Vector
</span>def add [Add α] : {n : Nat} → Vector α n → Vector α n → Vector α n
  | _, nil,       nil       =&gt; nil
  | _, cons a as, cons b bs =&gt; cons (a + b) (add as bs)
<span class="boring">end Vector
</span></code></pre>
<p>As we mentioned above, the argument <code>{n : Nat}</code> is part of the
pattern matching, because it cannot be held fixed throughout the
definition. In previous Lean versions, users often found it cumbersome
to have to include these extra discriminants. Thus, Lean 4
implements a new feature, <em>discriminant refinement</em>, which includes
these extra discriminants automatically for us.</p>
<pre><code class="language-lean"><span class="boring">inductive Vector (α : Type u) : Nat → Type u
</span><span class="boring">  | nil  : Vector α 0
</span><span class="boring">  | cons : α → {n : Nat} → Vector α n → Vector α (n+1)
</span><span class="boring">namespace Vector
</span>def add [Add α] {n : Nat} : Vector α n → Vector α n → Vector α n
  | nil,       nil       =&gt; nil
  | cons a as, cons b bs =&gt; cons (a + b) (add as bs)
<span class="boring">end Vector
</span></code></pre>
<p>When combined with the <em>auto bound implicits</em> feature, you can simplify
the declare further and write:</p>
<pre><code class="language-lean"><span class="boring">inductive Vector (α : Type u) : Nat → Type u
</span><span class="boring">  | nil  : Vector α 0
</span><span class="boring">  | cons : α → {n : Nat} → Vector α n → Vector α (n+1)
</span><span class="boring">namespace Vector
</span>def add [Add α] : Vector α n → Vector α n → Vector α n
  | nil,       nil       =&gt; nil
  | cons a as, cons b bs =&gt; cons (a + b) (add as bs)
<span class="boring">end Vector
</span></code></pre>
<p>Using these new features, you can write the other vector functions defined
in the previous sections more compactly as follows:</p>
<pre><code class="language-lean"><span class="boring">inductive Vector (α : Type u) : Nat → Type u
</span><span class="boring">  | nil  : Vector α 0
</span><span class="boring">  | cons : α → {n : Nat} → Vector α n → Vector α (n+1)
</span><span class="boring">namespace Vector
</span>def head : Vector α (n+1) → α
  | cons a as =&gt; a

def tail : Vector α (n+1) → Vector α n
  | cons a as =&gt; as

theorem eta : (v : Vector α (n+1)) → cons (head v) (tail v) = v
  | cons a as =&gt; rfl

def map (f : α → β → γ) : Vector α n → Vector β n → Vector γ n
  | nil,       nil       =&gt; nil
  | cons a as, cons b bs =&gt; cons (f a b) (map f as bs)

def zip : Vector α n → Vector β n → Vector (α × β) n
  | nil,       nil       =&gt; nil
  | cons a as, cons b bs =&gt; cons (a, b) (zip as bs)
<span class="boring">end Vector
</span></code></pre>
<h2><a class="header" href="#match-expressions" id="match-expressions">Match Expressions</a></h2>
<p>Lean also provides a compiler for <em>match-with</em> expressions found in
many functional languages.</p>
<pre><code class="language-lean">def isNotZero (m : Nat) : Bool :=
  match m with
  | 0   =&gt; false
  | n+1 =&gt; true
</code></pre>
<p>This does not look very different from an ordinary pattern matching
definition, but the point is that a <code>match</code> can be used anywhere in
an expression, and with arbitrary arguments.</p>
<pre><code class="language-lean">def isNotZero (m : Nat) : Bool :=
  match m with
  | 0   =&gt; false
  | n+1 =&gt; true

def filter (p : α → Bool) : List α → List α
  | []      =&gt; []
  | a :: as =&gt;
    match p a with
    | true =&gt; a :: filter p as
    | false =&gt; filter p as

example : filter isNotZero [1, 0, 0, 3, 0] = [1, 3] := rfl
</code></pre>
<p>Here is another example:</p>
<pre><code class="language-lean">def foo (n : Nat) (b c : Bool) :=
  5 + match n - 5, b &amp;&amp; c with
      | 0,   true  =&gt; 0
      | m+1, true  =&gt; m + 7
      | 0,   false =&gt; 5
      | m+1, false =&gt; m + 3

#eval foo 7 true false

example : foo 7 true false = 9 := rfl
</code></pre>
<p>Lean uses the <code>match</code> construct internally to implement pattern-matching in all parts of the system.
Thus, all four of these definitions have the same net effect.</p>
<pre><code class="language-lean">def bar₁ : Nat × Nat → Nat
  | (m, n) =&gt; m + n

def bar₂ (p : Nat × Nat) : Nat :=
  match p with
  | (m, n) =&gt; m + n

def bar₃ : Nat × Nat → Nat :=
  fun (m, n) =&gt; m + n

def bar₄ (p : Nat × Nat) : Nat :=
  let (m, n) := p; m + n
</code></pre>
<p>These variations are equally useful for destructing propositions:</p>
<pre><code class="language-lean">variable (p q : Nat → Prop)

example : (∃ x, p x) → (∃ y, q y) → ∃ x y, p x ∧ q y
  | ⟨x, px⟩, ⟨y, qy⟩ =&gt; ⟨x, y, px, qy⟩

example (h₀ : ∃ x, p x) (h₁ : ∃ y, q y)
        : ∃ x y, p x ∧ q y :=
  match h₀, h₁ with
  | ⟨x, px⟩, ⟨y, qy⟩ =&gt; ⟨x, y, px, qy⟩

example : (∃ x, p x) → (∃ y, q y) → ∃ x y, p x ∧ q y :=
  fun ⟨x, px⟩ ⟨y, qy⟩ =&gt; ⟨x, y, px, qy⟩

example (h₀ : ∃ x, p x) (h₁ : ∃ y, q y)
        : ∃ x y, p x ∧ q y :=
  let ⟨x, px⟩ := h₀
  let ⟨y, qy⟩ := h₁
  ⟨x, y, px, qy⟩
</code></pre>
<h2><a class="header" href="#local-recursive-declarations-1" id="local-recursive-declarations-1">Local Recursive Declarations</a></h2>
<p>You can define local recursive declarations using the <code>let rec</code> keyword.</p>
<pre><code class="language-lean">def replicate (n : Nat) (a : α) : List α :=
  let rec loop : Nat → List α → List α
    | 0,   as =&gt; as
    | n+1, as =&gt; loop n (a::as)
  loop n []

#check @replicate.loop
-- {α : Type} → α → Nat → List α → List α
</code></pre>
<p>Lean creates an auxiliary declaration for each <code>let rec</code>. In the example above,
it created the declaration <code>replicate.loop</code> for the <code>let rec loop</code> occurring at <code>replicate</code>.
Note that, Lean &quot;closes&quot; the declaration by adding any local variable occurring in the
<code>let rec</code> declaration as additional parameters. For example, the local variable <code>a</code> occurs
at <code>let rec loop</code>.</p>
<p>You can also use <code>let rec</code> in tactic mode and for creating proofs by induction.</p>
<pre><code class="language-lean"><span class="boring">def replicate (n : Nat) (a : α) : List α :=
</span><span class="boring"> let rec loop : Nat → List α → List α
</span><span class="boring">   | 0,   as =&gt; as
</span><span class="boring">   | n+1, as =&gt; loop n (a::as)
</span><span class="boring"> loop n []
</span>theorem length_replicate (n : Nat) (a : α) : (replicate n a).length = n := by
  let rec aux (n : Nat) (as : List α)
              : (replicate.loop a n as).length = n + as.length := by
    match n with
    | 0   =&gt; simp [replicate.loop]
    | n+1 =&gt; simp [replicate.loop, aux n, Nat.add_succ, Nat.succ_add]
  exact aux n []
</code></pre>
<p>You can also introduce auxiliary recursive declarations using a <code>where</code> clause after your definition.
Lean converts them into a <code>let rec</code>.</p>
<pre><code class="language-lean">def replicate (n : Nat) (a : α) : List α :=
  loop n []
where
  loop : Nat → List α → List α
    | 0,   as =&gt; as
    | n+1, as =&gt; loop n (a::as)

theorem length_replicate (n : Nat) (a : α) : (replicate n a).length = n := by
  exact aux n []
where
  aux (n : Nat) (as : List α)
      : (replicate.loop a n as).length = n + as.length := by
    match n with
    | 0   =&gt; simp [replicate.loop]
    | n+1 =&gt; simp [replicate.loop, aux n, Nat.add_succ, Nat.succ_add]
</code></pre>
<h2><a class="header" href="#exercises-4" id="exercises-4">Exercises</a></h2>
<ol>
<li>
<p>Open a namespace <code>Hidden</code> to avoid naming conflicts, and use the
equation compiler to define addition, multiplication, and
exponentiation on the natural numbers. Then use the equation
compiler to derive some of their basic properties.</p>
</li>
<li>
<p>Similarly, use the equation compiler to define some basic
operations on lists (like the <code>reverse</code> function) and prove
theorems about lists by induction (such as the fact that
<code>reverse (reverse xs) = xs</code> for any list <code>xs</code>).</p>
</li>
<li>
<p>Define your own function to carry out course-of-value recursion on
the natural numbers. Similarly, see if you can figure out how to
define <code>WellFounded.fix</code> on your own.</p>
</li>
<li>
<p>Following the examples in <a href="induction_and_recursion.html#dependent-pattern-matching">Section Dependent Pattern Matching</a>,
define a function that will append two vectors.
This is tricky; you will have to define an auxiliary function.</p>
</li>
<li>
<p>Consider the following type of arithmetic expressions. The idea is
that <code>var n</code> is a variable, <code>vₙ</code>, and <code>const n</code> is the
constant whose value is <code>n</code>.</p>
</li>
</ol>
<pre><code class="language-lean">inductive Expr where
  | const : Nat → Expr
  | var : Nat → Expr
  | plus : Expr → Expr → Expr
  | times : Expr → Expr → Expr
  deriving Repr

open Expr

def sampleExpr : Expr :=
  plus (times (var 0) (const 7)) (times (const 2) (var 1))
</code></pre>
<p>Here <code>sampleExpr</code> represents <code>(v₀ * 7) + (2 * v₁)</code>.</p>
<p>Write a function that evaluates such an expression, evaluating each <code>var n</code> to <code>v n</code>.</p>
<pre><code class="language-lean"><span class="boring">inductive Expr where
</span><span class="boring">  | const : Nat → Expr
</span><span class="boring">  | var : Nat → Expr
</span><span class="boring">  | plus : Expr → Expr → Expr
</span><span class="boring">  | times : Expr → Expr → Expr
</span><span class="boring">  deriving Repr
</span><span class="boring">open Expr
</span><span class="boring">def sampleExpr : Expr :=
</span><span class="boring">  plus (times (var 0) (const 7)) (times (const 2) (var 1))
</span>def eval (v : Nat → Nat) : Expr → Nat
  | const n     =&gt; sorry
  | var n       =&gt; v n
  | plus e₁ e₂  =&gt; sorry
  | times e₁ e₂ =&gt; sorry

def sampleVal : Nat → Nat
  | 0 =&gt; 5
  | 1 =&gt; 6
  | _ =&gt; 0

-- Try it out. You should get 47 here.
-- #eval eval sampleVal sampleExpr
</code></pre>
<p>Implement &quot;constant fusion,&quot; a procedure that simplifies subterms like
<code>5 + 7</code> to <code>12</code>. Using the auxiliary function <code>simpConst</code>,
define a function &quot;fuse&quot;: to simplify a plus or a times, first
simplify the arguments recursively, and then apply <code>simpConst</code> to
try to simplify the result.</p>
<pre><code class="language-lean"><span class="boring">inductive Expr where
</span><span class="boring">  | const : Nat → Expr
</span><span class="boring">  | var : Nat → Expr
</span><span class="boring">  | plus : Expr → Expr → Expr
</span><span class="boring">  | times : Expr → Expr → Expr
</span><span class="boring">  deriving Repr
</span><span class="boring">open Expr
</span><span class="boring">def eval (v : Nat → Nat) : Expr → Nat
</span><span class="boring">  | const n     =&gt; sorry
</span><span class="boring">  | var n       =&gt; v n
</span><span class="boring">  | plus e₁ e₂  =&gt; sorry
</span><span class="boring">  | times e₁ e₂ =&gt; sorry
</span>def simpConst : Expr → Expr
  | plus (const n₁) (const n₂)  =&gt; const (n₁ + n₂)
  | times (const n₁) (const n₂) =&gt; const (n₁ * n₂)
  | e                           =&gt; e

def fuse : Expr → Expr := sorry

theorem simpConst_eq (v : Nat → Nat)
        : ∀ e : Expr, eval v (simpConst e) = eval v e :=
  sorry

theorem fuse_eq (v : Nat → Nat)
        : ∀ e : Expr, eval v (fuse e) = eval v e :=
  sorry
</code></pre>
<p>The last two theorems show that the definitions preserve the value.</p>
<h1><a class="header" href="#structures-and-records" id="structures-and-records">Structures and Records</a></h1>
<p>We have seen that Lean's foundational system includes inductive
types. We have, moreover, noted that it is a remarkable fact that it
is possible to construct a substantial edifice of mathematics based on
nothing more than the type universes, dependent arrow types, and inductive types;
everything else follows from those. The Lean standard library contains
many instances of inductive types (e.g., <code>Nat</code>, <code>Prod</code>, <code>List</code>),
and even the logical connectives are defined using inductive types.</p>
<p>Recall that a non-recursive inductive type that contains only one
constructor is called a <em>structure</em> or <em>record</em>. The product type is a
structure, as is the dependent product (Sigma) type.
In general, whenever we define a structure <code>S</code>, we usually
define <em>projection</em> functions that allow us to &quot;destruct&quot; each
instance of <code>S</code> and retrieve the values that are stored in its
fields. The functions <code>prod.fst</code> and <code>prod.snd</code>, which return the
first and second elements of a pair, are examples of such projections.</p>
<p>When writing programs or formalizing mathematics, it is not uncommon
to define structures containing many fields. The <code>structure</code>
command, available in Lean, provides infrastructure to support this
process. When we define a structure using this command, Lean
automatically generates all the projection functions. The
<code>structure</code> command also allows us to define new structures based on
previously defined ones. Moreover, Lean provides convenient notation
for defining instances of a given structure.</p>
<h2><a class="header" href="#declaring-structures" id="declaring-structures">Declaring Structures</a></h2>
<p>The structure command is essentially a &quot;front end&quot; for defining
inductive data types. Every <code>structure</code> declaration introduces a
namespace with the same name. The general form is as follows:</p>
<pre><code>    structure &lt;name&gt; &lt;parameters&gt; &lt;parent-structures&gt; where
      &lt;constructor&gt; :: &lt;fields&gt;
</code></pre>
<p>Most parts are optional. Here is an example:</p>
<pre><code class="language-lean">structure Point (α : Type u) where
  mk :: (x : α) (y : α)
</code></pre>
<p>Values of type <code>Point</code> are created using <code>Point.mk a b</code>, and the
fields of a point <code>p</code> are accessed using <code>Point.x p</code> and
<code>Point.y p</code> (but <code>p.x</code> and <code>p.y</code> also work, see below).
The structure command also generates useful recursors and
theorems. Here are some of the constructions generated for the
declaration above.</p>
<pre><code class="language-lean"><span class="boring">structure Point (α : Type u) where
</span><span class="boring"> mk :: (x : α) (y : α)
</span>#check Point       -- a Type
#check @Point.rec  -- the eliminator
#check @Point.mk   -- the constructor
#check @Point.x    -- a projection
#check @Point.y    -- a projection
</code></pre>
<p>If the constructor name is not provided, then a constructor is named
<code>mk</code> by default.  You can also avoid the parentheses around field
names if you add a line break between each field.</p>
<pre><code class="language-lean">structure Point (α : Type u) where
  x : α
  y : α
</code></pre>
<p>Here are some simple theorems and expressions that use the generated
constructions. As usual, you can avoid the prefix <code>Point</code> by using
the command <code>open Point</code>.</p>
<pre><code class="language-lean"><span class="boring">structure Point (α : Type u) where
</span><span class="boring"> x : α
</span><span class="boring"> y : α
</span>#eval Point.x (Point.mk 10 20)
#eval Point.y (Point.mk 10 20)

open Point

example (a b : α) : x (mk a b) = a :=
  rfl

example (a b : α) : y (mk a b) = b :=
  rfl
</code></pre>
<p>Given <code>p : Point Nat</code>, the dot notation <code>p.x</code> is shorthand for
<code>Point.x p</code>. This provides a convenient way of accessing the fields
of a structure.</p>
<pre><code class="language-lean"><span class="boring">structure Point (α : Type u) where
</span><span class="boring"> x : α
</span><span class="boring"> y : α
</span>def p := Point.mk 10 20

#check p.x  -- Nat
#eval p.x   -- 10
#eval p.y   -- 20
</code></pre>
<p>The dot notation is convenient not just for accessing the projections
of a record, but also for applying functions defined in a namespace
with the same name. Recall from the <a href="./propositions_and_proofs.html#conjunction">Conjunction section</a> if <code>p</code>
has type <code>Point</code>, the expression <code>p.foo</code> is interpreted as
<code>Point.foo p</code>, assuming that the first non-implicit argument to
<code>foo</code> has type <code>Point</code>. The expression <code>p.add q</code> is therefore
shorthand for <code>Point.add p q</code> in the example below.</p>
<pre><code class="language-lean">structure Point (α : Type u) where
  x : α
  y : α
  deriving Repr

def Point.add (p q : Point Nat) :=
  mk (p.x + q.x) (p.y + q.y)

def p : Point Nat := Point.mk 1 2
def q : Point Nat := Point.mk 3 4

#eval p.add q  -- {x := 4, y := 6}
</code></pre>
<p>In the next chapter, you will learn how to define a function like
<code>add</code> so that it works generically for elements of <code>Point α</code>
rather than just <code>Point Nat</code>, assuming <code>α</code> has an associated
addition operation.</p>
<p>More generally, given an expression <code>p.foo x y z</code> where <code>p : Point</code>,
Lean will insert <code>p</code> at the first argument to <code>Point.foo</code> of type
<code>Point</code>. For example, with the definition of scalar multiplication
below, <code>p.smul 3</code> is interpreted as <code>Point.smul 3 p</code>.</p>
<pre><code class="language-lean"><span class="boring">structure Point (α : Type u) where
</span><span class="boring"> x : α
</span><span class="boring"> y : α
</span><span class="boring"> deriving Repr
</span>def Point.smul (n : Nat) (p : Point Nat) :=
  Point.mk (n * p.x) (n * p.y)

def p : Point Nat := Point.mk 1 2

#eval p.smul 3  -- {x := 3, y := 6}
</code></pre>
<p>It is common to use a similar trick with the <code>List.map</code> function,
which takes a list as its second non-implicit argument:</p>
<pre><code class="language-lean">#check @List.map

def xs : List Nat := [1, 2, 3]
def f : Nat → Nat := fun x =&gt; x * x

#eval xs.map f  -- [1, 4, 9]
</code></pre>
<p>Here <code>xs.map f</code> is interpreted as <code>List.map f xs</code>.</p>
<h2><a class="header" href="#objects" id="objects">Objects</a></h2>
<p>We have been using constructors to create elements of a structure
type. For structures containing many fields, this is often
inconvenient, because we have to remember the order in which the
fields were defined. Lean therefore provides the following alternative
notations for defining elements of a structure type.</p>
<pre><code>    { (&lt;field-name&gt; := &lt;expr&gt;)* : structure-type }
    or
    { (&lt;field-name&gt; := &lt;expr&gt;)* }
</code></pre>
<p>The suffix <code>: structure-type</code> can be omitted whenever the name of
the structure can be inferred from the expected type. For example, we
use this notation to define &quot;points.&quot; The order that the fields are
specified does not matter, so all the expressions below define the
same point.</p>
<pre><code class="language-lean">structure Point (α : Type u) where
  x : α
  y : α

#check { x := 10, y := 20 : Point Nat }  -- Point ℕ
#check { y := 20, x := 10 : Point _ }
#check ({ x := 10, y := 20 } : Point Nat)

example : Point Nat :=
  { y := 20, x := 10 }
</code></pre>
<p>If the value of a field is not specified, Lean tries to infer it. If
the unspecified fields cannot be inferred, Lean flags an error
indicating the corresponding placeholder could not be synthesized.</p>
<pre><code class="language-lean">structure MyStruct where
    {α : Type u}
    {β : Type v}
    a : α
    b : β

#check { a := 10, b := true : MyStruct }
</code></pre>
<p><em>Record update</em> is another common operation which amounts to creating
a new record object by modifying the value of one or more fields in an
old one. Lean allows you to specify that unassigned fields in the
specification of a record should be taken from a previously defined
structure object <code>s</code> by adding the annotation <code>s with</code> before the field
assignments. If more than one record object is provided, then they are
visited in order until Lean finds one that contains the unspecified
field. Lean raises an error if any of the field names remain
unspecified after all the objects are visited.</p>
<pre><code class="language-lean">structure Point (α : Type u) where
  x : α
  y : α
  deriving Repr

def p : Point Nat :=
  { x := 1, y := 2 }

#eval { p with y := 3 }  -- { x := 1, y := 3 }
#eval { p with x := 4 }  -- { x := 4, y := 2 }

structure Point3 (α : Type u) where
  x : α
  y : α
  z : α

def q : Point3 Nat :=
  { x := 5, y := 5, z := 5 }

def r : Point3 Nat :=
  { p, q with x := 6 }

example : r.x = 6 := rfl
example : r.y = 2 := rfl
example : r.z = 5 := rfl
</code></pre>
<h2><a class="header" href="#inheritance" id="inheritance">Inheritance</a></h2>
<p>We can <em>extend</em> existing structures by adding new fields. This feature
allows us to simulate a form of <em>inheritance</em>.</p>
<pre><code class="language-lean">structure Point (α : Type u) where
  x : α
  y : α

inductive Color where
  | red | green | blue

structure ColorPoint (α : Type u) extends Point α where
  c : Color
</code></pre>
<p>In the next example, we define a structure using multiple inheritance,
and then define an object using objects of the parent structures.</p>
<pre><code class="language-lean">structure Point (α : Type u) where
  x : α
  y : α
  z : α

structure RGBValue where
  red : Nat
  green : Nat
  blue : Nat

structure RedGreenPoint (α : Type u) extends Point α, RGBValue where
  no_blue : blue = 0

def p : Point Nat :=
  { x := 10, y := 10, z := 20 }

def rgp : RedGreenPoint Nat :=
  { p with red := 200, green := 40, blue := 0, no_blue := rfl }

example : rgp.x   = 10 := rfl
example : rgp.red = 200 := rfl
</code></pre>
<h1><a class="header" href="#type-classes" id="type-classes">Type Classes</a></h1>
<p>Type classes were introduced as a principled way of enabling
ad-hoc polymorphism in functional programming languages. We first observe that it
would be easy to implement an ad-hoc polymorphic function (such as addition) if the
function simply took the type-specific implementation of addition as an argument
and then called that implementation on the remaining arguments. For example,
suppose we declare a structure in Lean to hold implementations of addition.</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span>structure Add (a : Type) where
  add : a → a → a

#check @Add.add
-- Add.add : {a : Type} → Add a → a → a → a
<span class="boring">end Ex
</span></code></pre>
<p>In the above Lean code, the field <code>add</code> has type
<code>Add.add : {a : Type} → Add a → a → a → a</code>
where the curly braces around the type <code>a</code> mean that it is an implicit argument.
We could implement <code>double</code> by:</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span><span class="boring">structure Add (a : Type) where
</span><span class="boring"> add : a → a → a
</span>def double (s : Add a) (x : a) : a :=
  s.add x x

#eval double { add := Nat.add } 10
-- 20

#eval double { add := Nat.mul } 10
-- 100

#eval double { add := Int.add } 10
-- 20
<span class="boring">end Ex
</span></code></pre>
<p>Note that you can double a natural number <code>n</code> by <code>double { add := Nat.add } n</code>.
Of course, it would be highly cumbersome for users to manually pass the
implementations around in this way.
Indeed, it would defeat most of the potential benefits of ad-hoc
polymorphism.</p>
<p>The main idea behind type classes is to make arguments such as <code>Add a</code> implicit,
and to use a database of user-defined instances to synthesize the desired instances
automatically through a process known as typeclass resolution. In Lean, by changing
<code>structure</code> to <code>class</code> in the example above, the type of <code>Add.add</code> becomes:</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span>class Add (a : Type) where
  add : a → a → a

#check @Add.add
-- Add.add : {a : Type} → [self : Add a] → a → a → a
<span class="boring">end Ex
</span></code></pre>
<p>where the square brackets indicate that the argument of type <code>Add a</code> is <em>instance implicit</em>,
i.e. that it should be synthesized using typeclass resolution. This version of
<code>add</code> is the Lean analogue of the Haskell term <code>add :: Add a =&gt; a -&gt; a -&gt; a</code>.
Similarly, we can register instances by:</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span><span class="boring">class Add (a : Type) where
</span><span class="boring"> add : a → a → a
</span>instance : Add Nat where
  add := Nat.add

instance : Add Int where
  add := Int.add

instance : Add Float where
  add := Float.add
<span class="boring">end Ex
</span></code></pre>
<p>Then for <code>n : Nat</code> and <code>m : Nat</code>, the term <code>Add.add n m</code> triggers typeclass resolution with
the goal of <code>Add Nat</code>, and typeclass resolution will synthesize the instance for <code>Nat</code> above.
We can now reimplement <code>double</code> using an instance implicit by:</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span><span class="boring">class Add (a : Type) where
</span><span class="boring">  add : a → a → a
</span><span class="boring">instance : Add Nat where
</span><span class="boring"> add := Nat.add
</span><span class="boring">instance : Add Int where
</span><span class="boring"> add := Int.add
</span><span class="boring">instance : Add Float where
</span><span class="boring"> add := Float.add
</span>def double [Add a] (x : a) : a :=
  Add.add x x

#check @double
-- @double : {a : Type} → [inst : Add a] → a → a

#eval double 10
-- 20

#eval double (10 : Int)
-- 100

#eval double (7 : Float)
-- 14.000000

#eval double (239.0 + 2)
-- 482.000000

<span class="boring">end Ex
</span></code></pre>
<p>In general, instances may depend on other instances in complicated ways. For example,
you can declare an (anonymous) instance stating that if <code>a</code> has addition, then <code>Array a</code>
has addition:</p>
<pre><code class="language-lean">instance [Add a] : Add (Array a) where
  add x y := Array.zipWith x y (· + ·)

#eval Add.add #[1, 2] #[3, 4]
-- #[4, 6]

#eval #[1, 2] + #[3, 4]
-- #[4, 6]
</code></pre>
<p>Note that <code>(· + ·)</code> is notation for <code>fun x y =&gt; x + y</code> in Lean.</p>
<p>The example above demonstrates how type classes are used to overload notation.
Now, we explore another application. We often need an arbitrary element of a given type.
Recall that types may not have any elements in Lean.
It often happens that we would like a definition to return an arbitrary element in a &quot;corner case.&quot;
For example, we may like the expression <code>head xs</code> to be of type <code>a</code> when <code>xs</code> is of type <code>List a</code>.
Similarly, many theorems hold under the additional assumption that a type is not empty.
For example, if <code>a</code> is a type, <code>exists x : a, x = x</code> is true only if <code>a</code> is not empty.
The standard library defines a type class <code>Inhabited</code> to enable type class inference to infer a
&quot;default&quot; element of an inhabited type.
Let us start with the first step of the program above, declaring an appropriate class:</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span>class Inhabited (a : Type u) where
  default : a

#check @Inhabited.default
-- Inhabited.default : {a : Type u} → [self : Inhabited a] → a
<span class="boring">end Ex
</span></code></pre>
<p>Note <code>Inhabited.default</code> doesn't have any explicit arguments.</p>
<p>An element of the class <code>Inhabited a</code> is simply an expression of the form <code>Inhabited.mk x</code>, for some element <code>x : a</code>.
The projection <code>Inhabited.default</code> will allow us to &quot;extract&quot; such an element of <code>a</code> from an element of <code>Inhabited a</code>.
Now we populate the class with some instances:</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span><span class="boring">class Inhabited (a : Type _) where
</span><span class="boring"> default : a
</span>instance : Inhabited Bool where
  default := true

instance : Inhabited Nat where
  default := 0

instance : Inhabited Unit where
  default := ()

instance : Inhabited Prop where
  default := True

#eval (Inhabited.default : Nat)
-- 0

#eval (Inhabited.default : Bool)
-- true
<span class="boring">end Ex
</span></code></pre>
<p>You can use the command <code>export</code> to create the alias <code>default</code> for <code>Inhabited.default</code></p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span><span class="boring">class Inhabited (a : Type _) where
</span><span class="boring"> default : a
</span><span class="boring">instance : Inhabited Bool where
</span><span class="boring"> default := true
</span><span class="boring">instance : Inhabited Nat where
</span><span class="boring"> default := 0
</span><span class="boring">instance : Inhabited Unit where
</span><span class="boring"> default := ()
</span><span class="boring">instance : Inhabited Prop where
</span><span class="boring"> default := True
</span>export Inhabited (default)

#eval (default : Nat)
-- 0

#eval (default : Bool)
-- true
<span class="boring">end Ex
</span></code></pre>
<h2><a class="header" href="#chaining-instances" id="chaining-instances">Chaining Instances</a></h2>
<p>If that were the extent of type class inference, it would not be all that impressive;
it would be simply a mechanism of storing a list of instances for the elaborator to find in a lookup table.
What makes type class inference powerful is that one can <em>chain</em> instances. That is,
an instance declaration can in turn depend on an implicit instance of a type class.
This causes class inference to chain through instances recursively, backtracking when necessary, in a Prolog-like search.</p>
<p>For example, the following definition shows that if two types <code>a</code> and <code>b</code> are inhabited, then so is their product:</p>
<pre><code class="language-lean">instance [Inhabited a] [Inhabited b] : Inhabited (a × b) where
  default := (default, default)
</code></pre>
<p>With this added to the earlier instance declarations, type class instance can infer, for example, a default element of <code>Nat × Bool</code>:</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span><span class="boring">class Inhabited (a : Type u) where
</span><span class="boring"> default : a
</span><span class="boring">instance : Inhabited Bool where
</span><span class="boring"> default := true
</span><span class="boring">instance : Inhabited Nat where
</span><span class="boring"> default := 0
</span><span class="boring">opaque default [Inhabited a] : a :=
</span><span class="boring"> Inhabited.default
</span>instance [Inhabited a] [Inhabited b] : Inhabited (a × b) where
  default := (default, default)

#eval (default : Nat × Bool)
-- (0, true)
<span class="boring">end Ex
</span></code></pre>
<p>Similarly, we can inhabit type function with suitable constant functions:</p>
<pre><code class="language-lean">instance [Inhabited b] : Inhabited (a → b) where
  default := fun _ =&gt; default
</code></pre>
<p>As an exercise, try defining default instances for other types, such as <code>List</code> and <code>Sum</code> types.</p>
<p>The Lean standard library contains the definition <code>inferInstance</code>. It has type <code>{α : Sort u} → [i : α] → α</code>,
and is useful for triggering the type class resolution procedure when the expected type is an instance.</p>
<pre><code class="language-lean">#check (inferInstance : Inhabited Nat) -- Inhabited Nat

def foo : Inhabited (Nat × Nat) :=
  inferInstance

theorem ex : foo.default = (default, default) :=
  rfl
</code></pre>
<p>You can use the command <code>#print</code> to inspect how simple <code>inferInstance</code> is.</p>
<pre><code class="language-lean">#print inferInstance
</code></pre>
<h2><a class="header" href="#tostring" id="tostring">ToString</a></h2>
<p>The polymorphic method <code>toString</code> has type <code>{α : Type u} → [ToString α] → α → String</code>. You implement the instance
for your own types and use chaining to convert complex values into strings. Lean comes with <code>ToString</code> instances
for most builtin types.</p>
<pre><code class="language-lean">structure Person where
  name : String
  age  : Nat

instance : ToString Person where
  toString p := p.name ++ &quot;@&quot; ++ toString p.age

#eval toString { name := &quot;Leo&quot;, age := 542 : Person }
#eval toString ({ name := &quot;Daniel&quot;, age := 18 : Person }, &quot;hello&quot;)
</code></pre>
<h2><a class="header" href="#numerals" id="numerals">Numerals</a></h2>
<p>Numerals are polymorphic in Lean. You can use a numeral (e.g., <code>2</code>) to denote an element of any type that implements
the type class <code>OfNat</code>.</p>
<pre><code class="language-lean">structure Rational where
  num : Int
  den : Nat
  inv : den ≠ 0

instance : OfNat Rational n where
  ofNat := { num := n, den := 1, inv := by decide }

instance : ToString Rational where
  toString r := s!&quot;{r.num}/{r.den}&quot;

#eval (2 : Rational) -- 2/1

#check (2 : Rational) -- Rational
#check (2 : Nat)      -- Nat
</code></pre>
<p>Lean elaborates the terms <code>(2 : Nat)</code> and <code>(2 : Rational)</code> as
<code>OfNat.ofNat Nat 2 (instOfNatNat 2)</code> and
<code>OfNat.ofNat Rational 2 (instOfNatRational 2)</code> respectively.
We say the numerals <code>2</code> occurring in the elaborated terms are <em>raw</em> natural numbers.
You can input the raw natural number <code>2</code> using the macro <code>nat_lit 2</code>.</p>
<pre><code class="language-lean">#check nat_lit 2  -- Nat
</code></pre>
<p>Raw natural numbers are <em>not</em> polymorphic.</p>
<p>The <code>OfNat</code> instance is parametric on the numeral. So, you can define instances for particular numerals.
The second argument is often a variable as in the example above, or a <em>raw</em> natural number.</p>
<pre><code class="language-lean">class Monoid (α : Type u) where
  unit : α
  op   : α → α → α

instance [s : Monoid α] : OfNat α (nat_lit 1) where
  ofNat := s.unit

def getUnit [Monoid α] : α :=
  1
</code></pre>
<h2><a class="header" href="#output-parameters" id="output-parameters">Output Parameters</a></h2>
<p>By default, Lean only tries to synthesize an instance <code>Inhabited T</code> when the term <code>T</code> is known and does not
contain missing parts. The following command produces the error
&quot;typeclass instance problem is stuck, it is often due to metavariables <code>?m.7</code>&quot; because the type has a missing part (i.e., the <code>_</code>).</p>
<pre><code class="language-lean">#check_failure (inferInstance : Inhabited (Nat × _))
</code></pre>
<p>You can view the parameter of the type class <code>Inhabited</code> as an <em>input</em> value for the type class synthesizer.
When a type class has multiple parameters, you can mark some of them as output parameters.
Lean will start type class synthesizer even when these parameters have missing parts.
In the following example, we use output parameters to define a <em>heterogeneous</em> polymorphic
multiplication.</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span>class HMul (α : Type u) (β : Type v) (γ : outParam (Type w)) where
  hMul : α → β → γ

export HMul (hMul)

instance : HMul Nat Nat Nat where
  hMul := Nat.mul

instance : HMul Nat (Array Nat) (Array Nat) where
  hMul a bs := bs.map (fun b =&gt; hMul a b)

#eval hMul 4 3           -- 12
#eval hMul 4 #[2, 3, 4]  -- #[8, 12, 16]
<span class="boring">end Ex
</span></code></pre>
<p>The parameters <code>α</code> and <code>β</code> are considered input parameters and <code>γ</code> an output one.
Given an application <code>hMul a b</code>, after the types of <code>a</code> and <code>b</code> are known, the type class
synthesizer is invoked, and the resulting type is obtained from the output parameter <code>γ</code>.
In the example above, we defined two instances. The first one is the homogeneous
multiplication for natural numbers. The second is the scalar multiplication for arrays.
Note that you chain instances and generalize the second instance.</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span>class HMul (α : Type u) (β : Type v) (γ : outParam (Type w)) where
  hMul : α → β → γ

export HMul (hMul)

instance : HMul Nat Nat Nat where
  hMul := Nat.mul

instance : HMul Int Int Int where
  hMul := Int.mul

instance [HMul α β γ] : HMul α (Array β) (Array γ) where
  hMul a bs := bs.map (fun b =&gt; hMul a b)

#eval hMul 4 3                    -- 12
#eval hMul 4 #[2, 3, 4]           -- #[8, 12, 16]
#eval hMul (-2) #[3, -1, 4]       -- #[-6, 2, -8]
#eval hMul 2 #[#[2, 3], #[0, 4]]  -- #[#[4, 6], #[0, 8]]
<span class="boring">end Ex
</span></code></pre>
<p>You can use our new scalar array multiplication instance on arrays of type <code>Array β</code>
with a scalar of type <code>α</code> whenever you have an instance <code>HMul α β γ</code>.
In the last <code>#eval</code>, note that the instance was used twice on an array of arrays.</p>
<h2><a class="header" href="#default-instances" id="default-instances">Default Instances</a></h2>
<p>In the class <code>HMul</code>, the parameters <code>α</code> and <code>β</code> are treated as input values.
Thus, type class synthesis only starts after these two types are known. This may often
be too restrictive.</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span>class HMul (α : Type u) (β : Type v) (γ : outParam (Type w)) where
  hMul : α → β → γ

export HMul (hMul)

instance : HMul Int Int Int where
  hMul := Int.mul

def xs : List Int := [1, 2, 3]

-- Error &quot;typeclass instance problem is stuck, it is often due to metavariables HMul ?m.89 ?m.90 ?m.91&quot;
#check_failure fun y =&gt; xs.map (fun x =&gt; hMul x y)
<span class="boring">end Ex
</span></code></pre>
<p>The instance <code>HMul</code> is not synthesized by Lean because the type of <code>y</code> has not been provided.
However, it is natural to assume that the type of <code>y</code> and <code>x</code> should be the same in
this kind of situation. We can achieve exactly that using <em>default instances</em>.</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span>class HMul (α : Type u) (β : Type v) (γ : outParam (Type w)) where
  hMul : α → β → γ

export HMul (hMul)

@[default_instance]
instance : HMul Int Int Int where
  hMul := Int.mul

def xs : List Int := [1, 2, 3]

#check fun y =&gt; xs.map (fun x =&gt; hMul x y)  -- Int → List Int
<span class="boring">end Ex
</span></code></pre>
<p>By tagging the instance above with the attribute <code>default_instance</code>, we are instructing Lean
to use this instance on pending type class synthesis problems.
The actual Lean implementation defines homogeneous and heterogeneous classes for arithmetical operators.
Moreover, <code>a+b</code>, <code>a*b</code>, <code>a-b</code>, <code>a/b</code>, and <code>a%b</code> are notations for the heterogeneous versions.
The instance <code>OfNat Nat n</code> is the default instance (with priority 100) for the <code>OfNat</code> class. This is why the numeral
<code>2</code> has type <code>Nat</code> when the expected type is not known. You can define default instances with higher
priority to override the builtin ones.</p>
<pre><code class="language-lean">structure Rational where
  num : Int
  den : Nat
  inv : den ≠ 0

@[default_instance 200]
instance : OfNat Rational n where
  ofNat := { num := n, den := 1, inv := by decide }

instance : ToString Rational where
  toString r := s!&quot;{r.num}/{r.den}&quot;

#check 2 -- Rational
</code></pre>
<p>Priorities are also useful to control the interaction between different default instances.
For example, suppose <code>xs</code> has type <code>List α</code>. When elaborating <code>xs.map (fun x =&gt; 2 * x)</code>, we want the homogeneous instance for multiplication
to have higher priority than the default instance for <code>OfNat</code>. This is particularly important when we have implemented only the instance
<code>HMul α α α</code>, and did not implement <code>HMul Nat α α</code>.
Now, we reveal how the notation <code>a*b</code> is defined in Lean.</p>
<pre><code class="language-lean"><span class="boring">namespace Ex
</span>class OfNat (α : Type u) (n : Nat) where
  ofNat : α

@[default_instance]
instance (n : Nat) : OfNat Nat n where
  ofNat := n

class HMul (α : Type u) (β : Type v) (γ : outParam (Type w)) where
  hMul : α → β → γ

class Mul (α : Type u) where
  mul : α → α → α

@[default_instance 10]
instance [Mul α] : HMul α α α where
  hMul a b := Mul.mul a b

infixl:70 &quot; * &quot; =&gt; HMul.hMul
<span class="boring">end Ex
</span></code></pre>
<p>The <code>Mul</code> class is convenient for types that only implement the homogeneous multiplication.</p>
<h2><a class="header" href="#local-instances" id="local-instances">Local Instances</a></h2>
<p>Type classes are implemented using attributes in Lean. Thus, you can
use the <code>local</code> modifier to indicate that they only have effect until
the current <code>section</code> or <code>namespace</code> is closed, or until the end
of the current file.</p>
<pre><code class="language-lean">structure Point where
  x : Nat
  y : Nat

section

local instance : Add Point where
  add a b := { x := a.x + b.x, y := a.y + b.y }

def double (p : Point) :=
  p + p

end -- instance `Add Point` is not active anymore

-- def triple (p : Point) :=
--  p + p + p  -- Error: failed to synthesize instance
</code></pre>
<p>You can also temporarily disable an instance using the <code>attribute</code> command
until the current <code>section</code> or <code>namespace</code> is closed, or until the end
of the current file.</p>
<pre><code class="language-lean">structure Point where
  x : Nat
  y : Nat

instance addPoint : Add Point where
  add a b := { x := a.x + b.x, y := a.y + b.y }

def double (p : Point) :=
  p + p

attribute [-instance] addPoint

-- def triple (p : Point) :=
--  p + p + p  -- Error: failed to synthesize instance
</code></pre>
<p>We recommend you only use this command to diagnose problems.</p>
<h2><a class="header" href="#scoped-instances" id="scoped-instances">Scoped Instances</a></h2>
<p>You can also declare scoped instances in namespaces. This kind of instance is
only active when you are inside of the namespace or open the namespace.</p>
<pre><code class="language-lean">structure Point where
  x : Nat
  y : Nat

namespace Point

scoped instance : Add Point where
  add a b := { x := a.x + b.x, y := a.y + b.y }

def double (p : Point) :=
  p + p

end Point
-- instance `Add Point` is not active anymore

-- #check fun (p : Point) =&gt; p + p + p  -- Error

namespace Point
-- instance `Add Point` is active again
#check fun (p : Point) =&gt; p + p + p

end Point

open Point -- activates instance `Add Point`
#check fun (p : Point) =&gt; p + p + p
</code></pre>
<p>You can use the command <code>open scoped &lt;namespace&gt;</code> to activate scoped attributes but will
not &quot;open&quot; the names from the namespace.</p>
<pre><code class="language-lean">structure Point where
  x : Nat
  y : Nat

namespace Point

scoped instance : Add Point where
  add a b := { x := a.x + b.x, y := a.y + b.y }

def double (p : Point) :=
  p + p

end Point

open scoped Point -- activates instance `Add Point`
#check fun (p : Point) =&gt; p + p + p

-- #check fun (p : Point) =&gt; double p -- Error: unknown identifier 'double'
</code></pre>
<h2><a class="header" href="#decidable-propositions" id="decidable-propositions">Decidable Propositions</a></h2>
<p>Let us consider another example of a type class defined in the
standard library, namely the type class of <code>Decidable</code>
propositions. Roughly speaking, an element of <code>Prop</code> is said to be
decidable if we can decide whether it is true or false. The
distinction is only useful in constructive mathematics; classically,
every proposition is decidable. But if we use the classical principle,
say, to define a function by cases, that function will not be
computable. Algorithmically speaking, the <code>Decidable</code> type class can
be used to infer a procedure that effectively determines whether or
not the proposition is true. As a result, the type class supports such
computational definitions when they are possible while at the same
time allowing a smooth transition to the use of classical definitions
and classical reasoning.</p>
<p>In the standard library, <code>Decidable</code> is defined formally as follows:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>class inductive Decidable (p : Prop) where
  | isFalse (h : ¬p) : Decidable p
  | isTrue  (h : p)  : Decidable p
<span class="boring">end Hidden
</span></code></pre>
<p>Logically speaking, having an element <code>t : Decidable p</code> is stronger
than having an element <code>t : p ∨ ¬p</code>; it enables us to define values
of an arbitrary type depending on the truth value of <code>p</code>. For
example, for the expression <code>if p then a else b</code> to make sense, we
need to know that <code>p</code> is decidable. That expression is syntactic
sugar for <code>ite p a b</code>, where <code>ite</code> is defined as follows:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>def ite {α : Sort u} (c : Prop) [h : Decidable c] (t e : α) : α :=
  Decidable.casesOn (motive := fun _ =&gt; α) h (fun _ =&gt; e) (fun _ =&gt; t)
<span class="boring">end Hidden
</span></code></pre>
<p>The standard library also contains a variant of <code>ite</code> called
<code>dite</code>, the dependent if-then-else expression. It is defined as
follows:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>def dite {α : Sort u} (c : Prop) [h : Decidable c] (t : c → α) (e : Not c → α) : α :=
  Decidable.casesOn (motive := fun _ =&gt; α) h e t
<span class="boring">end Hidden
</span></code></pre>
<p>That is, in <code>dite c t e</code>, we can assume <code>hc : c</code> in the &quot;then&quot;
branch, and <code>hnc : ¬ c</code> in the &quot;else&quot; branch. To make <code>dite</code> more
convenient to use, Lean allows us to write <code>if h : c then t else e</code>
instead of <code>dite c (λ h : c =&gt; t) (λ h : ¬ c =&gt; e)</code>.</p>
<p>Without classical logic, we cannot prove that every proposition is
decidable. But we can prove that <em>certain</em> propositions are
decidable. For example, we can prove the decidability of basic
operations like equality and comparisons on the natural numbers and
the integers. Moreover, decidability is preserved under propositional
connectives:</p>
<pre><code class="language-lean">#check @instDecidableAnd
  -- {p q : Prop} → [Decidable p] → [Decidable q] → Decidable (And p q)

#check @instDecidableOr
#check @instDecidableNot
</code></pre>
<p>Thus we can carry out definitions by cases on decidable predicates on
the natural numbers:</p>
<pre><code class="language-lean">def step (a b x : Nat) : Nat :=
  if x &lt; a ∨ x &gt; b then 0 else 1

set_option pp.explicit true
#print step
</code></pre>
<p>Turning on implicit arguments shows that the elaborator has inferred
the decidability of the proposition <code>x &lt; a ∨ x &gt; b</code>, simply by
applying appropriate instances.</p>
<p>With the classical axioms, we can prove that every proposition is
decidable. You can import the classical axioms and make the generic
instance of decidability available by opening the <code>Classical</code> namespace.</p>
<pre><code class="language-lean">open Classical
</code></pre>
<p>Thereafter <code>Decidable p</code> has an instance for every <code>p</code>.
Thus all theorems in the library
that rely on decidability assumptions are freely available when you
want to reason classically. In <a href="./axioms_and_computation.html">Chapter Axioms and Computation</a>,
we will see that using the law of the
excluded middle to define functions can prevent them from being used
computationally. Thus, the standard library assigns a low priority to
the <code>propDecidable</code> instance.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>open Classical
noncomputable scoped
instance (priority := low) propDecidable (a : Prop) : Decidable a :=
  choice &lt;| match em a with
    | Or.inl h =&gt; ⟨isTrue h⟩
    | Or.inr h =&gt; ⟨isFalse h⟩
<span class="boring">end Hidden
</span></code></pre>
<p>This guarantees that Lean will favor other instances and fall back on
<code>propDecidable</code> only after other attempts to infer decidability have
failed.</p>
<p>The <code>Decidable</code> type class also provides a bit of small-scale
automation for proving theorems. The standard library introduces the
tactic <code>decide</code> that uses the <code>Decidable</code> instance to solve simple goals.</p>
<pre><code class="language-lean">example : 10 &lt; 5 ∨ 1 &gt; 0 := by
  decide

example : ¬ (True ∧ False) := by
  decide

example : 10 * 20 = 200 := by
  decide

theorem ex : True ∧ 2 = 1+1 := by
  decide

#print ex
-- theorem ex : True ∧ 2 = 1 + 1 :=
-- of_decide_eq_true (Eq.refl true)

#check @of_decide_eq_true
-- ∀ {p : Prop} [Decidable p], decide p = true → p

#check @decide
-- (p : Prop) → [Decidable p] → Bool
</code></pre>
<p>They work as follows. The expression <code>decide p</code> tries to infer a
decision procedure for <code>p</code>, and, if it is successful, evaluates to
either <code>true</code> or <code>false</code>. In particular, if <code>p</code> is a true closed
expression, <code>decide p</code> will reduce definitionally to the Boolean <code>true</code>.
On the assumption that <code>decide p = true</code> holds, <code>of_decide_eq_true</code>
produces a proof of <code>p</code>. The tactic <code>decide</code> puts it all together to
prove a target <code>p</code>. By the previous observations,
<code>decide</code> will succeed any time the inferred decision procedure
for <code>c</code> has enough information to evaluate, definitionally, to the <code>isTrue</code> case.</p>
<h2><a class="header" href="#managing-type-class-inference" id="managing-type-class-inference">Managing Type Class Inference</a></h2>
<p>If you are ever in a situation where you need to supply an expression
that Lean can infer by type class inference, you can ask Lean to carry
out the inference using <code>inferInstance</code>:</p>
<pre><code class="language-lean">def foo : Add Nat := inferInstance
def bar : Inhabited (Nat → Nat) := inferInstance

#check @inferInstance
-- {α : Sort u} → [α] → α
</code></pre>
<p>In fact, you can use Lean's <code>(t : T)</code> notation to specify the class whose instance you are looking for,
in a concise manner:</p>
<pre><code class="language-lean">#check (inferInstance : Add Nat)
</code></pre>
<p>You can also use the auxiliary definition <code>inferInstanceAs</code>:</p>
<pre><code class="language-lean">#check inferInstanceAs (Add Nat)

#check @inferInstanceAs
-- (α : Sort u) → [α] → α
</code></pre>
<p>Sometimes Lean can't find an instance because the class is buried
under a definition. For example, Lean cannot
find an instance of <code>Inhabited (Set α)</code>. We can declare one
explicitly:</p>
<pre><code class="language-lean">def Set (α : Type u) := α → Prop

-- fails
-- example : Inhabited (Set α) :=
--  inferInstance

instance : Inhabited (Set α) :=
  inferInstanceAs (Inhabited (α → Prop))
</code></pre>
<p>At times, you may find that the type class inference fails to find an
expected instance, or, worse, falls into an infinite loop and times
out. To help debug in these situations, Lean enables you to request a
trace of the search:</p>
<pre><code class="language-lean">set_option trace.Meta.synthInstance true
</code></pre>
<p>If you are using VS Code, you can read the results by hovering over
the relevant theorem or definition, or opening the messages window
with <code>Ctrl-Shift-Enter</code>. In Emacs, you can use <code>C-c C-x</code> to run an
independent Lean process on your file, and the output buffer will show
a trace every time the type class resolution procedure is subsequently
triggered.</p>
<p>You can also limit the search using the following options:</p>
<pre><code class="language-lean">set_option synthInstance.maxHeartbeats 10000
set_option synthInstance.maxSize 400
</code></pre>
<p>Option <code>synthInstance.maxHeartbeats</code> specifies the maximum amount of
heartbeats per typeclass resolution problem. A heartbeat is the number of
(small) memory allocations (in thousands), 0 means there is no limit.
Option <code>synthInstance.maxSize</code> is the maximum number of instances used
to construct a solution in the type class instance synthesis procedure.</p>
<p>Remember also that in both the VS Code and Emacs editor modes, tab
completion works in <code>set_option</code>, to help you find suitable options.</p>
<p>As noted above, the type class instances in a given context represent
a Prolog-like program, which gives rise to a backtracking search. Both
the efficiency of the program and the solutions that are found can
depend on the order in which the system tries the instance. Instances
which are declared last are tried first. Moreover, if instances are
declared in other modules, the order in which they are tried depends
on the order in which namespaces are opened. Instances declared in
namespaces which are opened later are tried earlier.</p>
<p>You can change the order that type class instances are tried by
assigning them a <em>priority</em>. When an instance is declared, it is
assigned a default priority value. You can assign other priorities
when defining an instance. The following example illustrates how this
is done:</p>
<pre><code class="language-lean">class Foo where
  a : Nat
  b : Nat

instance (priority := default+1) i1 : Foo where
  a := 1
  b := 1

instance i2 : Foo where
  a := 2
  b := 2

example : Foo.a = 1 :=
  rfl

instance (priority := default+2) i3 : Foo where
  a := 3
  b := 3

example : Foo.a = 3 :=
  rfl
</code></pre>
<h2><a class="header" href="#coercions-using-type-classes" id="coercions-using-type-classes">Coercions using Type Classes</a></h2>
<p>The most basic type of coercion maps elements of one type to another. For example, a coercion from <code>Nat</code> to <code>Int</code> allows us to view any element <code>n : Nat</code> as an element of <code>Int</code>. But some coercions depend on parameters; for example, for any type <code>α</code>, we can view any element <code>as : List α</code> as an element of <code>Set α</code>, namely, the set of elements occurring in the list. The corresponding coercion is defined on the &quot;family&quot; of types <code>List α</code>, parameterized by <code>α</code>.</p>
<p>Lean allows us to declare three kinds of coercions:</p>
<ul>
<li>from a family of types to another family of types</li>
<li>from a family of types to the class of sorts</li>
<li>from a family of types to the class of function types</li>
</ul>
<p>The first kind of coercion allows us to view any element of a member of the source family as an element of a corresponding member of the target family. The second kind of coercion allows us to view any element of a member of the source family as a type. The third kind of coercion allows us to view any element of the source family as a function. Let us consider each of these in turn.</p>
<p>In Lean, coercions are implemented on top of the type class resolution framework. We define a coercion from <code>α</code> to <code>β</code> by declaring an instance of <code>Coe α β</code>. For example, we can define a coercion from <code>Bool</code> to <code>Prop</code> as follows:</p>
<pre><code class="language-lean">instance : Coe Bool Prop where
  coe b := b = true
</code></pre>
<p>This enables us to use boolean terms in if-then-else expressions:</p>
<pre><code class="language-lean">#eval if true then 5 else 3
#eval if false then 5 else 3
</code></pre>
<p>We can define a coercion from <code>List α</code> to <code>Set α</code> as follows:</p>
<pre><code class="language-lean"><span class="boring">def Set (α : Type u) := α → Prop
</span><span class="boring">def Set.empty {α : Type u} : Set α := fun _ =&gt; False
</span><span class="boring">def Set.mem (a : α) (s : Set α) : Prop := s a
</span><span class="boring">def Set.singleton (a : α) : Set α := fun x =&gt; x = a
</span><span class="boring">def Set.union (a b : Set α) : Set α := fun x =&gt; a x ∨ b x
</span><span class="boring">notation &quot;{ &quot; a &quot; }&quot; =&gt; Set.singleton a
</span><span class="boring">infix:55 &quot; ∪ &quot; =&gt; Set.union
</span>def List.toSet : List α → Set α
  | []    =&gt; Set.empty
  | a::as =&gt; {a} ∪ as.toSet

instance : Coe (List α) (Set α) where
  coe a := a.toSet

def s : Set Nat := {1}
#check s ∪ [2, 3]
-- s ∪ List.toSet [2, 3] : Set Nat
</code></pre>
<p>We can use the notation <code>↑</code> to force a coercion to be introduced in a particular place. It is also helpful to make our intent clear, and work around limitations of the coercion resolution system.</p>
<pre><code class="language-lean"><span class="boring">def Set (α : Type u) := α → Prop
</span><span class="boring">def Set.empty {α : Type u} : Set α := fun _ =&gt; False
</span><span class="boring">def Set.mem (a : α) (s : Set α) : Prop := s a
</span><span class="boring">def Set.singleton (a : α) : Set α := fun x =&gt; x = a
</span><span class="boring">def Set.union (a b : Set α) : Set α := fun x =&gt; a x ∨ b x
</span><span class="boring">notation &quot;{ &quot; a &quot; }&quot; =&gt; Set.singleton a
</span><span class="boring">infix:55 &quot; ∪ &quot; =&gt; Set.union
</span><span class="boring">def List.toSet : List α → Set α
</span><span class="boring">  | []    =&gt; Set.empty
</span><span class="boring">  | a::as =&gt; {a} ∪ as.toSet
</span><span class="boring">instance : Coe (List α) (Set α) where
</span><span class="boring">  coe a := a.toSet
</span>def s : Set Nat := {1}

#check let x := ↑[2, 3]; s ∪ x
-- let x := List.toSet [2, 3]; s ∪ x : Set Nat
#check let x := [2, 3]; s ∪ x
-- let x := [2, 3]; s ∪ List.toSet x : Set Nat
</code></pre>
<p>Lean also supports dependent coercions using the type class <code>CoeDep</code>. For example, we cannot coerce arbitrary propositions to <code>Bool</code>, only the ones that implement the <code>Decidable</code> typeclass.</p>
<pre><code class="language-lean">instance (p : Prop) [Decidable p] : CoeDep Prop p Bool where
  coe := decide p
</code></pre>
<p>Lean will also chain (non-dependent) coercions as necessary. Actually, the type class <code>CoeT</code> is the transitive closure of <code>Coe</code>.</p>
<p>Let us now consider the second kind of coercion. By the <em>class of sorts</em>, we mean the collection of universes <code>Type u</code>. A coercion of the second kind is of the form:</p>
<pre><code>    c : (x1 : A1) → ... → (xn : An) → F x1 ... xn → Type u
</code></pre>
<p>where <code>F</code> is a family of types as above. This allows us to write <code>s : t</code> whenever <code>t</code> is of type <code>F a1 ... an</code>. In other words, the coercion allows us to view the elements of <code>F a1 ... an</code> as types. This is very useful when defining algebraic structures in which one component, the carrier of the structure, is a <code>Type</code>. For example, we can define a semigroup as follows:</p>
<pre><code class="language-lean">structure Semigroup where
  carrier : Type u
  mul : carrier → carrier → carrier
  mul_assoc (a b c : carrier) : mul (mul a b) c = mul a (mul b c)

instance (S : Semigroup) : Mul S.carrier where
  mul a b := S.mul a b
</code></pre>
<p>In other words, a semigroup consists of a type, <code>carrier</code>, and a multiplication, <code>mul</code>, with the property that the multiplication is associative. The <code>instance</code> command allows us to write <code>a * b</code> instead of <code>Semigroup.mul S a b</code> whenever we have <code>a b : S.carrier</code>; notice that Lean can infer the argument <code>S</code> from the types of <code>a</code> and <code>b</code>. The function <code>Semigroup.carrier</code> maps the class <code>Semigroup</code> to the sort <code>Type u</code>:</p>
<pre><code class="language-lean"><span class="boring">structure Semigroup where
</span><span class="boring">  carrier : Type u
</span><span class="boring">  mul : carrier → carrier → carrier
</span><span class="boring">  mul_assoc (a b c : carrier) : mul (mul a b) c = mul a (mul b c)
</span><span class="boring">instance (S : Semigroup) : Mul S.carrier where
</span><span class="boring">  mul a b := S.mul a b
</span>#check Semigroup.carrier
</code></pre>
<p>If we declare this function to be a coercion, then whenever we have a semigroup <code>S : Semigroup</code>, we can write <code>a : S</code> instead of <code>a : S.carrier</code>:</p>
<pre><code class="language-lean"><span class="boring">structure Semigroup where
</span><span class="boring">  carrier : Type u
</span><span class="boring">  mul : carrier → carrier → carrier
</span><span class="boring">  mul_assoc (a b c : carrier) : mul (mul a b) c = mul a (mul b c)
</span><span class="boring">instance (S : Semigroup) : Mul S.carrier where
</span><span class="boring">  mul a b := S.mul a b
</span>instance : CoeSort Semigroup (Type u) where
  coe s := s.carrier

example (S : Semigroup) (a b c : S) : (a * b) * c = a * (b * c) :=
  Semigroup.mul_assoc _ a b c
</code></pre>
<p>It is the coercion that makes it possible to write <code>(a b c : S)</code>. Note that, we define an instance of <code>CoeSort Semigroup (Type u)</code> instead of <code>Coe Semigroup (Type u)</code>.</p>
<p>By the <em>class of function types</em>, we mean the collection of Pi types <code>(z : B) → C</code>. The third kind of coercion has the form:</p>
<pre><code>    c : (x1 : A1) → ... → (xn : An) → (y : F x1 ... xn) → (z : B) → C
</code></pre>
<p>where <code>F</code> is again a family of types and <code>B</code> and <code>C</code> can depend on <code>x1, ..., xn, y</code>. This makes it possible to write <code>t s</code> whenever <code>t</code> is an element of <code>F a1 ... an</code>. In other words, the coercion enables us to view elements of <code>F a1 ... an</code> as functions. Continuing the example above, we can define the notion of a morphism between semigroups <code>S1</code> and <code>S2</code>. That is, a function from the carrier of <code>S1</code> to the carrier of <code>S2</code> (note the implicit coercion) that respects the multiplication. The projection <code>morphism.mor</code> takes a morphism to the underlying function:</p>
<pre><code class="language-lean"><span class="boring">structure Semigroup where
</span><span class="boring">  carrier : Type u
</span><span class="boring">  mul : carrier → carrier → carrier
</span><span class="boring">  mul_assoc (a b c : carrier) : mul (mul a b) c = mul a (mul b c)
</span><span class="boring">instance (S : Semigroup) : Mul S.carrier where
</span><span class="boring">  mul a b := S.mul a b
</span><span class="boring">instance : CoeSort Semigroup (Type u) where
</span><span class="boring">  coe s := s.carrier
</span>structure Morphism (S1 S2 : Semigroup) where
  mor : S1 → S2
  resp_mul : ∀ a b : S1, mor (a * b) = (mor a) * (mor b)

#check @Morphism.mor
</code></pre>
<p>As a result, it is a prime candidate for the third type of coercion.</p>
<pre><code class="language-lean"><span class="boring">structure Semigroup where
</span><span class="boring">  carrier : Type u
</span><span class="boring">  mul : carrier → carrier → carrier
</span><span class="boring">  mul_assoc (a b c : carrier) : mul (mul a b) c = mul a (mul b c)
</span><span class="boring">instance (S : Semigroup) : Mul S.carrier where
</span><span class="boring">  mul a b := S.mul a b
</span><span class="boring">instance : CoeSort Semigroup (Type u) where
</span><span class="boring">  coe s := s.carrier
</span><span class="boring">structure Morphism (S1 S2 : Semigroup) where
</span><span class="boring">  mor : S1 → S2
</span><span class="boring">  resp_mul : ∀ a b : S1, mor (a * b) = (mor a) * (mor b)
</span>instance (S1 S2 : Semigroup) : CoeFun (Morphism S1 S2) (fun _ =&gt; S1 → S2) where
  coe m := m.mor

theorem resp_mul {S1 S2 : Semigroup} (f : Morphism S1 S2) (a b : S1)
        : f (a * b) = f a * f b :=
  f.resp_mul a b

example (S1 S2 : Semigroup) (f : Morphism S1 S2) (a : S1) :
      f (a * a * a) = f a * f a * f a :=
  calc f (a * a * a)
    _ = f (a * a) * f a := by rw [resp_mul f]
    _ = f a * f a * f a := by rw [resp_mul f]
</code></pre>
<p>With the coercion in place, we can write <code>f (a * a * a)</code> instead of <code>f.mor (a * a * a)</code>. When the <code>Morphism</code>, <code>f</code>, is used where a function is expected, Lean inserts the coercion. Similar to <code>CoeSort</code>, we have yet another class <code>CoeFun</code> for this class of coercions. The field <code>F</code> is used to specify the function type we are coercing to. This type may depend on the type we are coercing from.</p>
<h1><a class="header" href="#the-conversion-tactic-mode" id="the-conversion-tactic-mode">The Conversion Tactic Mode</a></h1>
<p>Inside a tactic block, one can use the keyword <code>conv</code> to enter
conversion mode. This mode allows to travel inside assumptions and
goals, even inside function abstractions and dependent arrows, to apply rewriting or
simplifying steps.</p>
<h2><a class="header" href="#basic-navigation-and-rewriting" id="basic-navigation-and-rewriting">Basic navigation and rewriting</a></h2>
<p>As a first example, let us prove example
<code>(a b c : Nat) : a * (b * c) = a * (c * b)</code>
(examples in this file are somewhat artificial since
other tactics could finish them immediately). The naive
first attempt is to enter tactic mode and try <code>rw [Nat.mul_comm]</code>. But this
transforms the goal into <code>b * c * a = a * (c * b)</code>, after commuting the
very first multiplication appearing in the term. There are several
ways to fix this issue, and one way is to use a more precise tool:
the conversion mode. The following code block shows the current target
after each line.</p>
<pre><code class="language-lean">example (a b c : Nat) : a * (b * c) = a * (c * b) := by
  conv =&gt;
    -- ⊢ a * (b * c) = a * (c * b)
    lhs
    -- ⊢ a * (b * c)
    congr
    -- 2 goals: ⊢ a, ⊢ b * c
    rfl
    -- ⊢ b * c
    rw [Nat.mul_comm]
</code></pre>
<p>The above snippet shows three navigation commands:</p>
<ul>
<li><code>lhs</code> navigates to the left hand side of a relation (here equality), there is also a <code>rhs</code> navigating to the right hand side.</li>
<li><code>congr</code> creates as many targets as there are (nondependent and explicit) arguments to the current head function
(here the head function is multiplication).</li>
<li><code>rfl</code> closes target using reflexivity.</li>
</ul>
<p>Once arrived at the relevant target, we can use <code>rw</code> as in normal
tactic mode.</p>
<p>The second main reason to use conversion mode is to rewrite under
binders. Suppose we want to prove example
<code>(fun x : Nat =&gt; 0 + x) = (fun x =&gt; x)</code>.
The naive first attempt is to enter tactic mode and try
<code>rw [Nat.zero_add]</code>. But this fails with a frustrating</p>
<pre><code>error: tactic 'rewrite' failed, did not find instance of the pattern
       in the target expression
  0 + ?n
⊢ (fun x =&gt; 0 + x) = fun x =&gt; x
</code></pre>
<p>The solution is:</p>
<pre><code class="language-lean">example : (fun x : Nat =&gt; 0 + x) = (fun x =&gt; x) := by
  conv =&gt;
    lhs
    intro x
    rw [Nat.zero_add]
</code></pre>
<p>where <code>intro x</code> is the navigation command entering inside the <code>fun</code> binder.
Note that this example is somewhat artificial, one could also do:</p>
<pre><code class="language-lean">example : (fun x : Nat =&gt; 0 + x) = (fun x =&gt; x) := by
  funext x; rw [Nat.zero_add]
</code></pre>
<p>or just</p>
<pre><code class="language-lean">example : (fun x : Nat =&gt; 0 + x) = (fun x =&gt; x) := by
  simp
</code></pre>
<p><code>conv</code> can also rewrite a hypothesis <code>h</code> from the local context, using <code>conv at h</code>.</p>
<h2><a class="header" href="#pattern-matching-1" id="pattern-matching-1">Pattern matching</a></h2>
<p>Navigation using the above commands can be tedious. One can shortcut it using pattern matching as follows:</p>
<pre><code class="language-lean">example (a b c : Nat) : a * (b * c) = a * (c * b) := by
  conv in b * c =&gt; rw [Nat.mul_comm]
</code></pre>
<p>which is just syntax sugar for</p>
<pre><code class="language-lean">example (a b c : Nat) : a * (b * c) = a * (c * b) := by
  conv =&gt;
    pattern b * c
    rw [Nat.mul_comm]
</code></pre>
<p>Of course, wildcards are allowed:</p>
<pre><code class="language-lean">example (a b c : Nat) : a * (b * c) = a * (c * b) := by
  conv in _ * c =&gt; rw [Nat.mul_comm]
</code></pre>
<h2><a class="header" href="#structuring-conversion-tactics" id="structuring-conversion-tactics">Structuring conversion tactics</a></h2>
<p>Curly brackets and <code>.</code> can also be used in <code>conv</code> mode to structure tactics.</p>
<pre><code class="language-lean">example (a b c : Nat) : (0 + a) * (b * c) = a * (c * b) := by
  conv =&gt;
    lhs
    congr
    . rw [Nat.zero_add]
    . rw [Nat.mul_comm]
</code></pre>
<h2><a class="header" href="#other-tactics-inside-conversion-mode" id="other-tactics-inside-conversion-mode">Other tactics inside conversion mode</a></h2>
<ul>
<li><code>arg i</code> enter the <code>i</code>-th nondependent explicit argument of an application.</li>
</ul>
<pre><code class="language-lean">example (a b c : Nat) : a * (b * c) = a * (c * b) := by
  conv =&gt;
    -- ⊢ a * (b * c) = a * (c * b)
    lhs
    -- ⊢ a * (b * c)
    arg 2
    -- ⊢ b * c
    rw [Nat.mul_comm]
</code></pre>
<ul>
<li>
<p><code>args</code> alternative name for <code>congr</code>.</p>
</li>
<li>
<p><code>simp</code> applies the simplifier to the current goal. It supports the same options available in regular tactic mode.</p>
</li>
</ul>
<pre><code class="language-lean">def f (x : Nat) :=
  if x &gt; 0 then x + 1 else x + 2

example (g : Nat → Nat) (h₁ : g x = x + 1) (h₂ : x &gt; 0) : g x = f x := by
  conv =&gt;
    rhs
    simp [f, h₂]
  exact h₁
</code></pre>
<ul>
<li><code>enter [1, x, 2, y]</code> iterate <code>arg</code> and <code>intro</code> with the given arguments. It is just the macro:</li>
</ul>
<pre><code>syntax enterArg := ident &lt;|&gt; group(&quot;@&quot;? num)
syntax &quot;enter &quot; &quot;[&quot; (colGt enterArg),+ &quot;]&quot;: conv
macro_rules
  | `(conv| enter [$i:num]) =&gt; `(conv| arg $i)
  | `(conv| enter [@$i:num]) =&gt; `(conv| arg @$i)
  | `(conv| enter [$id:ident]) =&gt; `(conv| ext $id)
  | `(conv| enter [$arg:enterArg, $args,*]) =&gt; `(conv| (enter [$arg]; enter [$args,*]))
</code></pre>
<ul>
<li>
<p><code>done</code> fail if there are unsolved goals.</p>
</li>
<li>
<p><code>trace_state</code> display the current tactic state.</p>
</li>
<li>
<p><code>whnf</code> put term in weak head normal form.</p>
</li>
<li>
<p><code>tactic =&gt; &lt;tactic sequence&gt;</code> go back to regular tactic mode. This
is useful for discharging goals not supported by <code>conv</code> mode, and
applying custom congruence and extensionality lemmas.</p>
</li>
</ul>
<pre><code class="language-lean">example (g : Nat → Nat → Nat)
        (h₁ : ∀ x, x ≠ 0 → g x x = 1)
        (h₂ : x ≠ 0)
        : g x x + x = 1 + x := by
  conv =&gt;
    lhs
    -- ⊢ g x x + x
    arg 1
    -- ⊢ g x x
    rw [h₁]
    -- 2 goals: ⊢ 1, ⊢ x ≠ 0
    . skip
    . tactic =&gt; exact h₂
</code></pre>
<ul>
<li><code>apply &lt;term&gt;</code> is syntax sugar for <code>tactic =&gt; apply &lt;term&gt;</code></li>
</ul>
<pre><code class="language-lean">example (g : Nat → Nat → Nat)
        (h₁ : ∀ x, x ≠ 0 → g x x = 1)
        (h₂ : x ≠ 0)
        : g x x + x = 1 + x := by
  conv =&gt;
    lhs
    arg 1
    rw [h₁]
    . skip
    . apply h₂
</code></pre>
<h1><a class="header" href="#axioms-and-computation" id="axioms-and-computation">Axioms and Computation</a></h1>
<p>We have seen that the version of the Calculus of Constructions that
has been implemented in Lean includes dependent function types,
inductive types, and a hierarchy of universes that starts with an
impredicative, proof-irrelevant <code>Prop</code> at the bottom. In this
chapter, we consider ways of extending the CIC with additional axioms
and rules. Extending a foundational system in such a way is often
convenient; it can make it possible to prove more theorems, as well as
make it easier to prove theorems that could have been proved
otherwise. But there can be negative consequences of adding additional
axioms, consequences which may go beyond concerns about their
correctness. In particular, the use of axioms bears on the
computational content of definitions and theorems, in ways we will
explore here.</p>
<p>Lean is designed to support both computational and classical
reasoning. Users that are so inclined can stick to a &quot;computationally
pure&quot; fragment, which guarantees that closed expressions in the system
evaluate to canonical normal forms. In particular, any closed
computationally pure expression of type <code>Nat</code>, for example, will
reduce to a numeral.</p>
<p>Lean's standard library defines an additional axiom, propositional
extensionality, and a quotient construction which in turn implies the
principle of function extensionality. These extensions are used, for
example, to develop theories of sets and finite sets. We will see
below that using these theorems can block evaluation in Lean's kernel,
so that closed terms of type <code>Nat</code> no longer evaluate to numerals. But
Lean erases types and propositional information when compiling
definitions to bytecode for its virtual machine evaluator, and since
these axioms only add new propositions, they are compatible with that
computational interpretation. Even computationally inclined users may
wish to use the classical law of the excluded middle to reason about
computation. This also blocks evaluation in the kernel, but it is
compatible with compilation to bytecode.</p>
<p>The standard library also defines a choice principle that is entirely
antithetical to a computational interpretation, since it magically
produces &quot;data&quot; from a proposition asserting its existence. Its use is
essential to some classical constructions, and users can import it
when needed. But expressions that use this construction to produce
data do not have computational content, and in Lean we are required to
mark such definitions as <code>noncomputable</code> to flag that fact.</p>
<p>Using a clever trick (known as Diaconescu's theorem), one can use
propositional extensionality, function extensionality, and choice to
derive the law of the excluded middle. As noted above, however, use of
the law of the excluded middle is still compatible with bytecode
compilation and code extraction, as are other classical principles, as
long as they are not used to manufacture data.</p>
<p>To summarize, then, on top of the underlying framework of universes,
dependent function types, and inductive types, the standard library
adds three additional components:</p>
<ul>
<li>the axiom of propositional extensionality</li>
<li>a quotient construction, which implies function extensionality</li>
<li>a choice principle, which produces data from an existential proposition.</li>
</ul>
<p>The first two of these block normalization within Lean, but are
compatible with bytecode evaluation, whereas the third is not amenable
to computational interpretation. We will spell out the details more
precisely below.</p>
<h2><a class="header" href="#historical-and-philosophical-context" id="historical-and-philosophical-context">Historical and Philosophical Context</a></h2>
<p>For most of its history, mathematics was essentially computational:
geometry dealt with constructions of geometric objects, algebra was
concerned with algorithmic solutions to systems of equations, and
analysis provided means to compute the future behavior of systems
evolving over time. From the proof of a theorem to the effect that
&quot;for every <code>x</code>, there is a <code>y</code> such that ...&quot;, it was generally
straightforward to extract an algorithm to compute such a <code>y</code> given
<code>x</code>.</p>
<p>In the nineteenth century, however, increases in the complexity of
mathematical arguments pushed mathematicians to develop new styles of
reasoning that suppress algorithmic information and invoke
descriptions of mathematical objects that abstract away the details of
how those objects are represented. The goal was to obtain a powerful
&quot;conceptual&quot; understanding without getting bogged down in
computational details, but this had the effect of admitting
mathematical theorems that are simply <em>false</em> on a direct
computational reading.</p>
<p>There is still fairly uniform agreement today that computation is
important to mathematics. But there are different views as to how best
to address computational concerns. From a <em>constructive</em> point of
view, it is a mistake to separate mathematics from its computational
roots; every meaningful mathematical theorem should have a direct
computational interpretation. From a <em>classical</em> point of view, it is
more fruitful to maintain a separation of concerns: we can use one
language and body of methods to write computer programs, while
maintaining the freedom to use nonconstructive theories and methods
to reason about them. Lean is designed to support both of these
approaches. Core parts of the library are developed constructively,
but the system also provides support for carrying out classical
mathematical reasoning.</p>
<p>Computationally, the purest part of dependent type theory avoids the
use of <code>Prop</code> entirely. Inductive types and dependent function types
can be viewed as data types, and terms of these types can be
&quot;evaluated&quot; by applying reduction rules until no more rules can be
applied. In principle, any closed term (that is, term with no free
variables) of type <code>Nat</code> should evaluate to a numeral, <code>succ (... (succ zero)...)</code>.</p>
<p>Introducing a proof-irrelevant <code>Prop</code> and marking theorems
irreducible represents a first step towards separation of
concerns. The intention is that elements of a type <code>p : Prop</code> should
play no role in computation, and so the particular construction of a
term <code>t : p</code> is &quot;irrelevant&quot; in that sense. One can still define
computational objects that incorporate elements of type <code>Prop</code>; the
point is that these elements can help us reason about the effects of
the computation, but can be ignored when we extract &quot;code&quot; from the
term. Elements of type <code>Prop</code> are not entirely innocuous,
however. They include equations <code>s = t : α</code> for any type <code>α</code>, and
such equations can be used as casts, to type check terms. Below, we
will see examples of how such casts can block computation in the
system. However, computation is still possible under an evaluation
scheme that erases propositional content, ignores intermediate typing
constraints, and reduces terms until they reach a normal form. This is
precisely what Lean's virtual machine does.</p>
<p>Having adopted a proof-irrelevant <code>Prop</code>, one might consider it
legitimate to use, for example, the law of the excluded middle,
<code>p ∨ ¬p</code>, where <code>p</code> is any proposition. Of course, this, too, can block
computation according to the rules of CIC, but it does not block
bytecode evaluation, as described above. It is only the choice
principles discussed in :numref:<code>choice</code> that completely erase the
distinction between the proof-irrelevant and data-relevant parts of
the theory.</p>
<h2><a class="header" href="#propositional-extensionality" id="propositional-extensionality">Propositional Extensionality</a></h2>
<p>Propositional extensionality is the following axiom:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>axiom propext {a b : Prop} : (a ↔ b) → a = b
<span class="boring">end Hidden
</span></code></pre>
<p>It asserts that when two propositions imply one another, they are
actually equal. This is consistent with set-theoretic interpretations
in which any element <code>a : Prop</code> is either empty or the singleton set
<code>{*}</code>, for some distinguished element <code>*</code>. The axiom has the
effect that equivalent propositions can be substituted for one another
in any context:</p>
<pre><code class="language-lean">theorem thm₁ (a b c d e : Prop) (h : a ↔ b) : (c ∧ a ∧ d → e) ↔ (c ∧ b ∧ d → e) :=
  propext h ▸ Iff.refl _

theorem thm₂ (a b : Prop) (p : Prop → Prop) (h : a ↔ b) (h₁ : p a) : p b :=
  propext h ▸ h₁
</code></pre>
<!--
The first example could be proved more laboriously without ``propext``
using the fact that the propositional connectives respect
propositional equivalence. The second example represents a more
essential use of ``propext``. In fact, it is equivalent to ``propext``
itself, a fact which we encourage you to prove.

Given any definition or theorem in Lean, you can use the ``#print
axioms`` command to display the axioms it depends on.

.. code-block:: lean

    variables a b c d e : Prop
    variable p : Prop → Prop

    theorem thm₁ (h : a ↔ b) : (c ∧ a ∧ d → e) ↔ (c ∧ b ∧ d → e) :=
    propext h ▸ iff.refl _

    theorem thm₂ (h : a ↔ b) (h₁ : p a) : p b :=
    propext h ▸ h₁

    -- BEGIN
    #print axioms thm₁  -- propext
    #print axioms thm₂  -- propext
    -- END
-->
<h2><a class="header" href="#function-extensionality" id="function-extensionality">Function Extensionality</a></h2>
<p>Similar to propositional extensionality, function extensionality
asserts that any two functions of type <code>(x : α) → β x</code> that agree on
all their inputs are equal.</p>
<pre><code class="language-lean">universe u v
#check (@funext :
           {α : Type u}
         → {β : α → Type u}
         → {f g : (x : α) → β x}
         → (∀ (x : α), f x = g x)
         → f = g)

#print funext
</code></pre>
<p>From a classical, set-theoretic perspective, this is exactly what it
means for two functions to be equal. This is known as an &quot;extensional&quot;
view of functions. From a constructive perspective, however, it is
sometimes more natural to think of functions as algorithms, or
computer programs, that are presented in some explicit way. It is
certainly the case that two computer programs can compute the same
answer for every input despite the fact that they are syntactically
quite different. In much the same way, you might want to maintain a
view of functions that does not force you to identify two functions
that have the same input / output behavior. This is known as an
&quot;intensional&quot; view of functions.</p>
<p>In fact, function extensionality follows from the existence of
quotients, which we describe in the next section. In the Lean standard
library, therefore, <code>funext</code> is thus
<a href="https://github.com/leanprover/lean4/blob/master/src/Init/Core.lean">proved from the quotient construction</a>.</p>
<p>Suppose that for <code>α : Type</code> we define the <code>Set α := α → Prop</code> to
denote the type of subsets of <code>α</code>, essentially identifying subsets
with predicates. By combining <code>funext</code> and <code>propext</code>, we obtain an
extensional theory of such sets:</p>
<pre><code class="language-lean">def Set (α : Type u) := α → Prop

namespace Set

def mem (x : α) (a : Set α) := a x

infix:50 (priority := high) &quot;∈&quot; =&gt; mem

theorem setext {a b : Set α} (h : ∀ x, x ∈ a ↔ x ∈ b) : a = b :=
  funext (fun x =&gt; propext (h x))

end Set
</code></pre>
<p>We can then proceed to define the empty set and set intersection, for
example, and prove set identities:</p>
<pre><code class="language-lean"><span class="boring">def Set (α : Type u) := α → Prop
</span><span class="boring">namespace Set
</span><span class="boring">def mem (x : α) (a : Set α) := a x
</span><span class="boring">infix:50 (priority := high) &quot;∈&quot; =&gt; mem
</span><span class="boring">theorem setext {a b : Set α} (h : ∀ x, x ∈ a ↔ x ∈ b) : a = b :=
</span><span class="boring">  funext (fun x =&gt; propext (h x))
</span>def empty : Set α := fun x =&gt; False

notation (priority := high) &quot;∅&quot; =&gt; empty

def inter (a b : Set α) : Set α :=
  fun x =&gt; x ∈ a ∧ x ∈ b

infix:70 &quot; ∩ &quot; =&gt; inter

theorem inter_self (a : Set α) : a ∩ a = a :=
  setext fun x =&gt; Iff.intro
    (fun ⟨h, _⟩ =&gt; h)
    (fun h =&gt; ⟨h, h⟩)

theorem inter_empty (a : Set α) : a ∩ ∅ = ∅ :=
  setext fun x =&gt; Iff.intro
    (fun ⟨_, h⟩ =&gt; h)
    (fun h =&gt; False.elim h)

theorem empty_inter (a : Set α) : ∅ ∩ a = ∅ :=
  setext fun x =&gt; Iff.intro
    (fun ⟨h, _⟩ =&gt; h)
    (fun h =&gt; False.elim h)

theorem inter.comm (a b : Set α) : a ∩ b = b ∩ a :=
  setext fun x =&gt; Iff.intro
    (fun ⟨h₁, h₂⟩ =&gt; ⟨h₂, h₁⟩)
    (fun ⟨h₁, h₂⟩ =&gt; ⟨h₂, h₁⟩)
<span class="boring">end Set
</span></code></pre>
<p>The following is an example of how function extensionality blocks
computation inside the Lean kernel.</p>
<pre><code class="language-lean">def f (x : Nat) := x
def g (x : Nat) := 0 + x

theorem f_eq_g : f = g :=
  funext fun x =&gt; (Nat.zero_add x).symm

def val : Nat :=
  Eq.recOn (motive := fun _ _ =&gt; Nat) f_eq_g 0

-- does not reduce to 0
#reduce val

-- evaluates to 0
#eval val
</code></pre>
<p>First, we show that the two functions <code>f</code> and <code>g</code> are equal using
function extensionality, and then we cast <code>0</code> of type <code>Nat</code> by
replacing <code>f</code> by <code>g</code> in the type. Of course, the cast is
vacuous, because <code>Nat</code> does not depend on <code>f</code>. But that is enough
to do the damage: under the computational rules of the system, we now
have a closed term of <code>Nat</code> that does not reduce to a numeral. In this
case, we may be tempted to reduce the expression to <code>0</code>. But in
nontrivial examples, eliminating cast changes the type of the term,
which might make an ambient expression type incorrect. The virtual
machine, however, has no trouble evaluating the expression to
<code>0</code>. Here is a similarly contrived example that shows how
<code>propext</code> can get in the way.</p>
<pre><code class="language-lean">theorem tteq : (True ∧ True) = True :=
  propext (Iff.intro (fun ⟨h, _⟩ =&gt; h) (fun h =&gt; ⟨h, h⟩))

def val : Nat :=
  Eq.recOn (motive := fun _ _ =&gt; Nat) tteq 0

-- does not reduce to 0
#reduce val

-- evaluates to 0
#eval val
</code></pre>
<p>Current research programs, including work on <em>observational type
theory</em> and <em>cubical type theory</em>, aim to extend type theory in ways
that permit reductions for casts involving function extensionality,
quotients, and more. But the solutions are not so clear cut, and the
rules of Lean's underlying calculus do not sanction such reductions.</p>
<p>In a sense, however, a cast does not change the meaning of an
expression. Rather, it is a mechanism to reason about the expression's
type. Given an appropriate semantics, it then makes sense to reduce
terms in ways that preserve their meaning, ignoring the intermediate
bookkeeping needed to make the reductions type correct. In that case,
adding new axioms in <code>Prop</code> does not matter; by proof irrelevance,
an expression in <code>Prop</code> carries no information, and can be safely
ignored by the reduction procedures.</p>
<h2><a class="header" href="#quotients" id="quotients">Quotients</a></h2>
<p>Let <code>α</code> be any type, and let <code>r</code> be an equivalence relation on
<code>α</code>. It is mathematically common to form the &quot;quotient&quot; <code>α / r</code>,
that is, the type of elements of <code>α</code> &quot;modulo&quot; <code>r</code>. Set
theoretically, one can view <code>α / r</code> as the set of equivalence
classes of <code>α</code> modulo <code>r</code>. If <code>f : α → β</code> is any function that
respects the equivalence relation in the sense that for every
<code>x y : α</code>, <code>r x y</code> implies <code>f x = f y</code>, then <code>f</code> &quot;lifts&quot; to a function
<code>f' : α / r → β</code> defined on each equivalence class <code>⟦x⟧</code> by
<code>f' ⟦x⟧ = f x</code>. Lean's standard library extends the Calculus of
Constructions with additional constants that perform exactly these
constructions, and installs this last equation as a definitional
reduction rule.</p>
<p>In its most basic form, the quotient construction does not even
require <code>r</code> to be an equivalence relation. The following constants
are built into Lean:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>universe u v

axiom Quot : {α : Sort u} → (α → α → Prop) → Sort u

axiom Quot.mk : {α : Sort u} → (r : α → α → Prop) → α → Quot r

axiom Quot.ind :
    ∀ {α : Sort u} {r : α → α → Prop} {β : Quot r → Prop},
      (∀ a, β (Quot.mk r a)) → (q : Quot r) → β q

axiom Quot.lift :
    {α : Sort u} → {r : α → α → Prop} → {β : Sort u} → (f : α → β)
    → (∀ a b, r a b → f a = f b) → Quot r → β
<span class="boring">end Hidden
</span></code></pre>
<p>The first one forms a type <code>Quot r</code> given a type <code>α</code> by any binary
relation <code>r</code> on <code>α</code>. The second maps <code>α</code> to <code>Quot α</code>, so that
if <code>r : α → α → Prop</code> and <code>a : α</code>, then <code>Quot.mk r a</code> is an
element of <code>Quot r</code>. The third principle, <code>Quot.ind</code>, says that
every element of <code>Quot.mk r a</code> is of this form.  As for
<code>Quot.lift</code>, given a function <code>f : α → β</code>, if <code>h</code> is a proof
that <code>f</code> respects the relation <code>r</code>, then <code>Quot.lift f h</code> is the
corresponding function on <code>Quot r</code>. The idea is that for each
element <code>a</code> in <code>α</code>, the function <code>Quot.lift f h</code> maps
<code>Quot.mk r a</code> (the <code>r</code>-class containing <code>a</code>) to <code>f a</code>, wherein <code>h</code>
shows that this function is well defined. In fact, the computation
principle is declared as a reduction rule, as the proof below makes
clear.</p>
<pre><code class="language-lean">def mod7Rel (x y : Nat) : Prop :=
  x % 7 = y % 7

-- the quotient type
#check (Quot mod7Rel : Type)

-- the class of a
#check (Quot.mk mod7Rel 4 : Quot mod7Rel)

def f (x : Nat) : Bool :=
  x % 7 = 0

theorem f_respects (a b : Nat) (h : mod7Rel a b) : f a = f b := by
  simp [mod7Rel, f] at *
  rw [h]

#check (Quot.lift f f_respects : Quot mod7Rel → Bool)

-- the computation principle
example (a : Nat) : Quot.lift f f_respects (Quot.mk mod7Rel a) = f a :=
  rfl
</code></pre>
<p>The four constants, <code>Quot</code>, <code>Quot.mk</code>, <code>Quot.ind</code>, and
<code>Quot.lift</code> in and of themselves are not very strong. You can check
that the <code>Quot.ind</code> is satisfied if we take <code>Quot r</code> to be simply
<code>α</code>, and take <code>Quot.lift</code> to be the identity function (ignoring
<code>h</code>). For that reason, these four constants are not viewed as
additional axioms.</p>
<!--
    variables α β : Type
    variable  r : α → α → Prop
    variable  a : α
    variable  f : α → β
    variable   h : ∀ a₁ a₂, r a₁ a₂ → f a₁ = f a₂
    theorem thm : quot.lift f h (quot.mk r a) = f a := rfl
    -- BEGIN
    #print axioms thm   -- no axioms
    -- END
-->
<p>They are, like inductively defined types and the associated
constructors and recursors, viewed as part of the logical framework.</p>
<p>What makes the <code>Quot</code> construction into a bona fide quotient is the
following additional axiom:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span><span class="boring">universe u v
</span>axiom Quot.sound :
      ∀ {α : Type u} {r : α → α → Prop} {a b : α},
        r a b → Quot.mk r a = Quot.mk r b
<span class="boring">end Hidden
</span></code></pre>
<p>This is the axiom that asserts that any two elements of <code>α</code> that are
related by <code>r</code> become identified in the quotient. If a theorem or
definition makes use of <code>Quot.sound</code>, it will show up in the
<code>#print axioms</code> command.</p>
<p>Of course, the quotient construction is most commonly used in
situations when <code>r</code> is an equivalence relation. Given <code>r</code> as
above, if we define <code>r'</code> according to the rule <code>r' a b</code> iff
<code>Quot.mk r a = Quot.mk r b</code>, then it's clear that <code>r'</code> is an
equivalence relation. Indeed, <code>r'</code> is the <em>kernel</em> of the function
<code>a ↦ quot.mk r a</code>.  The axiom <code>Quot.sound</code> says that <code>r a b</code>
implies <code>r' a b</code>. Using <code>Quot.lift</code> and <code>Quot.ind</code>, we can show
that <code>r'</code> is the smallest equivalence relation containing <code>r</code>, in
the sense that if <code>r''</code> is any equivalence relation containing
<code>r</code>, then <code>r' a b</code> implies <code>r'' a b</code>. In particular, if <code>r</code>
was an equivalence relation to start with, then for all <code>a</code> and
<code>b</code> we have <code>r a b</code> iff <code>r' a b</code>.</p>
<p>To support this common use case, the standard library defines the
notion of a <em>setoid</em>, which is simply a type with an associated
equivalence relation:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>class Setoid (α : Sort u) where
  r : α → α → Prop
  iseqv : Equivalence r

instance {α : Sort u} [Setoid α] : HasEquiv α :=
  ⟨Setoid.r⟩

namespace Setoid

variable {α : Sort u} [Setoid α]

theorem refl (a : α) : a ≈ a :=
  iseqv.refl a

theorem symm {a b : α} (hab : a ≈ b) : b ≈ a :=
  iseqv.symm hab

theorem trans {a b c : α} (hab : a ≈ b) (hbc : b ≈ c) : a ≈ c :=
  iseqv.trans hab hbc

end Setoid
<span class="boring">end Hidden
</span></code></pre>
<p>Given a type <code>α</code>, a relation <code>r</code> on <code>α</code>, and a proof <code>p</code>
that <code>r</code> is an equivalence relation, we can define <code>Setoid.mk r p</code>
as an instance of the setoid class.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>def Quotient {α : Sort u} (s : Setoid α) :=
  @Quot α Setoid.r
<span class="boring">end Hidden
</span></code></pre>
<p>The constants <code>Quotient.mk</code>, <code>Quotient.ind</code>, <code>Quotient.lift</code>,
and <code>Quotient.sound</code> are nothing more than the specializations of
the corresponding elements of <code>Quot</code>. The fact that type class
inference can find the setoid associated to a type <code>α</code> brings a
number of benefits. First, we can use the notation <code>a ≈ b</code> (entered
with <code>\approx</code>) for <code>Setoid.r a b</code>, where the instance of
<code>Setoid</code> is implicit in the notation <code>Setoid.r</code>. We can use the
generic theorems <code>Setoid.refl</code>, <code>Setoid.symm</code>, <code>Setoid.trans</code> to
reason about the relation. Specifically with quotients we can use the
generic notation <code>⟦a⟧</code> for <code>Quot.mk Setoid.r</code> where the instance
of <code>Setoid</code> is implicit in the notation <code>Setoid.r</code>, as well as the
theorem <code>Quotient.exact</code>:</p>
<pre><code class="language-lean"><span class="boring">universe u
</span>#check (@Quotient.exact :
         ∀ {α : Sort u} {s : Setoid α} {a b : α},
           Quotient.mk s a = Quotient.mk s b → a ≈ b)
</code></pre>
<p>Together with <code>Quotient.sound</code>, this implies that the elements of
the quotient correspond exactly to the equivalence classes of elements
in <code>α</code>.</p>
<p>Recall that in the standard library, <code>α × β</code> represents the
Cartesian product of the types <code>α</code> and <code>β</code>. To illustrate the use
of quotients, let us define the type of <em>unordered</em> pairs of elements
of a type <code>α</code> as a quotient of the type <code>α × α</code>. First, we define
the relevant equivalence relation:</p>
<pre><code class="language-lean">private def eqv (p₁ p₂ : α × α) : Prop :=
  (p₁.1 = p₂.1 ∧ p₁.2 = p₂.2) ∨ (p₁.1 = p₂.2 ∧ p₁.2 = p₂.1)

infix:50 &quot; ~ &quot; =&gt; eqv
</code></pre>
<p>The next step is to prove that <code>eqv</code> is in fact an equivalence
relation, which is to say, it is reflexive, symmetric and
transitive. We can prove these three facts in a convenient and
readable way by using dependent pattern matching to perform
case-analysis and break the hypotheses into pieces that are then
reassembled to produce the conclusion.</p>
<pre><code class="language-lean"><span class="boring">private def eqv (p₁ p₂ : α × α) : Prop :=
</span><span class="boring">  (p₁.1 = p₂.1 ∧ p₁.2 = p₂.2) ∨ (p₁.1 = p₂.2 ∧ p₁.2 = p₂.1)
</span><span class="boring">infix:50 &quot; ~ &quot; =&gt; eqv
</span>private theorem eqv.refl (p : α × α) : p ~ p :=
  Or.inl ⟨rfl, rfl⟩

private theorem eqv.symm : ∀ {p₁ p₂ : α × α}, p₁ ~ p₂ → p₂ ~ p₁
  | (a₁, a₂), (b₁, b₂), (Or.inl ⟨a₁b₁, a₂b₂⟩) =&gt;
    Or.inl (by simp_all)
  | (a₁, a₂), (b₁, b₂), (Or.inr ⟨a₁b₂, a₂b₁⟩) =&gt;
    Or.inr (by simp_all)

private theorem eqv.trans : ∀ {p₁ p₂ p₃ : α × α}, p₁ ~ p₂ → p₂ ~ p₃ → p₁ ~ p₃
  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inl ⟨a₁b₁, a₂b₂⟩, Or.inl ⟨b₁c₁, b₂c₂⟩ =&gt;
    Or.inl (by simp_all)
  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inl ⟨a₁b₁, a₂b₂⟩, Or.inr ⟨b₁c₂, b₂c₁⟩ =&gt;
    Or.inr (by simp_all)
  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inr ⟨a₁b₂, a₂b₁⟩, Or.inl ⟨b₁c₁, b₂c₂⟩ =&gt;
    Or.inr (by simp_all)
  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inr ⟨a₁b₂, a₂b₁⟩, Or.inr ⟨b₁c₂, b₂c₁⟩ =&gt;
    Or.inl (by simp_all)

private theorem is_equivalence : Equivalence (@eqv α) :=
  { refl := eqv.refl, symm := eqv.symm, trans := eqv.trans }
</code></pre>
<p>Now that we have proved that <code>eqv</code> is an equivalence relation, we
can construct a <code>Setoid (α × α)</code>, and use it to define the type
<code>UProd α</code> of unordered pairs.</p>
<pre><code class="language-lean"><span class="boring">private def eqv (p₁ p₂ : α × α) : Prop :=
</span><span class="boring">  (p₁.1 = p₂.1 ∧ p₁.2 = p₂.2) ∨ (p₁.1 = p₂.2 ∧ p₁.2 = p₂.1)
</span><span class="boring">infix:50 &quot; ~ &quot; =&gt; eqv
</span><span class="boring">private theorem eqv.refl (p : α × α) : p ~ p :=
</span><span class="boring">  Or.inl ⟨rfl, rfl⟩
</span><span class="boring">private theorem eqv.symm : ∀ {p₁ p₂ : α × α}, p₁ ~ p₂ → p₂ ~ p₁
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (Or.inl ⟨a₁b₁, a₂b₂⟩) =&gt;
</span><span class="boring">    Or.inl (by simp_all)
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (Or.inr ⟨a₁b₂, a₂b₁⟩) =&gt;
</span><span class="boring">    Or.inr (by simp_all)
</span><span class="boring">private theorem eqv.trans : ∀ {p₁ p₂ p₃ : α × α}, p₁ ~ p₂ → p₂ ~ p₃ → p₁ ~ p₃
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inl ⟨a₁b₁, a₂b₂⟩, Or.inl ⟨b₁c₁, b₂c₂⟩ =&gt;
</span><span class="boring">    Or.inl (by simp_all)
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inl ⟨a₁b₁, a₂b₂⟩, Or.inr ⟨b₁c₂, b₂c₁⟩ =&gt;
</span><span class="boring">    Or.inr (by simp_all)
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inr ⟨a₁b₂, a₂b₁⟩, Or.inl ⟨b₁c₁, b₂c₂⟩ =&gt;
</span><span class="boring">    Or.inr (by simp_all)
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inr ⟨a₁b₂, a₂b₁⟩, Or.inr ⟨b₁c₂, b₂c₁⟩ =&gt;
</span><span class="boring">    Or.inl (by simp_all)
</span><span class="boring">private theorem is_equivalence : Equivalence (@eqv α) :=
</span><span class="boring">  { refl := eqv.refl, symm := eqv.symm, trans := eqv.trans }
</span>instance uprodSetoid (α : Type u) : Setoid (α × α) where
  r     := eqv
  iseqv := is_equivalence

def UProd (α : Type u) : Type u :=
  Quotient (uprodSetoid α)

namespace UProd

def mk {α : Type} (a₁ a₂ : α) : UProd α :=
  Quotient.mk' (a₁, a₂)

notation &quot;{ &quot; a₁ &quot;, &quot; a₂ &quot; }&quot; =&gt; mk a₁ a₂

end UProd
</code></pre>
<p>Notice that we locally define the notation <code>{a₁, a₂}</code> for unordered
pairs as <code>Quotient.mk (a₁, a₂)</code>. This is useful for illustrative
purposes, but it is not a good idea in general, since the notation
will shadow other uses of curly brackets, such as for records and
sets.</p>
<p>We can easily prove that <code>{a₁, a₂} = {a₂, a₁}</code> using <code>Quot.sound</code>,
since we have <code>(a₁, a₂) ~ (a₂, a₁)</code>.</p>
<pre><code class="language-lean"><span class="boring">private def eqv (p₁ p₂ : α × α) : Prop :=
</span><span class="boring">  (p₁.1 = p₂.1 ∧ p₁.2 = p₂.2) ∨ (p₁.1 = p₂.2 ∧ p₁.2 = p₂.1)
</span><span class="boring">infix:50 &quot; ~ &quot; =&gt; eqv
</span><span class="boring">private theorem eqv.refl (p : α × α) : p ~ p :=
</span><span class="boring">  Or.inl ⟨rfl, rfl⟩
</span><span class="boring">private theorem eqv.symm : ∀ {p₁ p₂ : α × α}, p₁ ~ p₂ → p₂ ~ p₁
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (Or.inl ⟨a₁b₁, a₂b₂⟩) =&gt;
</span><span class="boring">    Or.inl (by simp_all)
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (Or.inr ⟨a₁b₂, a₂b₁⟩) =&gt;
</span><span class="boring">    Or.inr (by simp_all)
</span><span class="boring">private theorem eqv.trans : ∀ {p₁ p₂ p₃ : α × α}, p₁ ~ p₂ → p₂ ~ p₃ → p₁ ~ p₃
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inl ⟨a₁b₁, a₂b₂⟩, Or.inl ⟨b₁c₁, b₂c₂⟩ =&gt;
</span><span class="boring">    Or.inl (by simp_all)
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inl ⟨a₁b₁, a₂b₂⟩, Or.inr ⟨b₁c₂, b₂c₁⟩ =&gt;
</span><span class="boring">    Or.inr (by simp_all)
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inr ⟨a₁b₂, a₂b₁⟩, Or.inl ⟨b₁c₁, b₂c₂⟩ =&gt;
</span><span class="boring">    Or.inr (by simp_all)
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inr ⟨a₁b₂, a₂b₁⟩, Or.inr ⟨b₁c₂, b₂c₁⟩ =&gt;
</span><span class="boring">    Or.inl (by simp_all)
</span><span class="boring">private theorem is_equivalence : Equivalence (@eqv α) :=
</span><span class="boring">  { refl := eqv.refl, symm := eqv.symm, trans := eqv.trans }
</span><span class="boring">instance uprodSetoid (α : Type u) : Setoid (α × α) where
</span><span class="boring">  r     := eqv
</span><span class="boring">  iseqv := is_equivalence
</span><span class="boring">def UProd (α : Type u) : Type u :=
</span><span class="boring">  Quotient (uprodSetoid α)
</span><span class="boring">namespace UProd
</span><span class="boring">def mk {α : Type} (a₁ a₂ : α) : UProd α :=
</span><span class="boring">  Quotient.mk' (a₁, a₂)
</span><span class="boring">notation &quot;{ &quot; a₁ &quot;, &quot; a₂ &quot; }&quot; =&gt; mk a₁ a₂
</span>theorem mk_eq_mk (a₁ a₂ : α) : {a₁, a₂} = {a₂, a₁} :=
  Quot.sound (Or.inr ⟨rfl, rfl⟩)
<span class="boring">end UProd
</span></code></pre>
<p>To complete the example, given <code>a : α</code> and <code>u : uprod α</code>, we
define the proposition <code>a ∈ u</code> which should hold if <code>a</code> is one of
the elements of the unordered pair <code>u</code>. First, we define a similar
proposition <code>mem_fn a u</code> on (ordered) pairs; then we show that
<code>mem_fn</code> respects the equivalence relation <code>eqv</code> with the lemma
<code>mem_respects</code>. This is an idiom that is used extensively in the
Lean standard library.</p>
<pre><code class="language-lean"><span class="boring">private def eqv (p₁ p₂ : α × α) : Prop :=
</span><span class="boring">  (p₁.1 = p₂.1 ∧ p₁.2 = p₂.2) ∨ (p₁.1 = p₂.2 ∧ p₁.2 = p₂.1)
</span><span class="boring">infix:50 &quot; ~ &quot; =&gt; eqv
</span><span class="boring">private theorem eqv.refl (p : α × α) : p ~ p :=
</span><span class="boring">  Or.inl ⟨rfl, rfl⟩
</span><span class="boring">private theorem eqv.symm : ∀ {p₁ p₂ : α × α}, p₁ ~ p₂ → p₂ ~ p₁
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (Or.inl ⟨a₁b₁, a₂b₂⟩) =&gt;
</span><span class="boring">    Or.inl (by simp_all)
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (Or.inr ⟨a₁b₂, a₂b₁⟩) =&gt;
</span><span class="boring">    Or.inr (by simp_all)
</span><span class="boring">private theorem eqv.trans : ∀ {p₁ p₂ p₃ : α × α}, p₁ ~ p₂ → p₂ ~ p₃ → p₁ ~ p₃
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inl ⟨a₁b₁, a₂b₂⟩, Or.inl ⟨b₁c₁, b₂c₂⟩ =&gt;
</span><span class="boring">    Or.inl (by simp_all)
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inl ⟨a₁b₁, a₂b₂⟩, Or.inr ⟨b₁c₂, b₂c₁⟩ =&gt;
</span><span class="boring">    Or.inr (by simp_all)
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inr ⟨a₁b₂, a₂b₁⟩, Or.inl ⟨b₁c₁, b₂c₂⟩ =&gt;
</span><span class="boring">    Or.inr (by simp_all)
</span><span class="boring">  | (a₁, a₂), (b₁, b₂), (c₁, c₂), Or.inr ⟨a₁b₂, a₂b₁⟩, Or.inr ⟨b₁c₂, b₂c₁⟩ =&gt;
</span><span class="boring">    Or.inl (by simp_all)
</span><span class="boring">private theorem is_equivalence : Equivalence (@eqv α) :=
</span><span class="boring">  { refl := eqv.refl, symm := eqv.symm, trans := eqv.trans }
</span><span class="boring">instance uprodSetoid (α : Type u) : Setoid (α × α) where
</span><span class="boring">  r     := eqv
</span><span class="boring">  iseqv := is_equivalence
</span><span class="boring">def UProd (α : Type u) : Type u :=
</span><span class="boring">  Quotient (uprodSetoid α)
</span><span class="boring">namespace UProd
</span><span class="boring">def mk {α : Type} (a₁ a₂ : α) : UProd α :=
</span><span class="boring">  Quotient.mk' (a₁, a₂)
</span><span class="boring">notation &quot;{ &quot; a₁ &quot;, &quot; a₂ &quot; }&quot; =&gt; mk a₁ a₂
</span><span class="boring">theorem mk_eq_mk (a₁ a₂ : α) : {a₁, a₂} = {a₂, a₁} :=
</span><span class="boring">  Quot.sound (Or.inr ⟨rfl, rfl⟩)
</span>private def mem_fn (a : α) : α × α → Prop
  | (a₁, a₂) =&gt; a = a₁ ∨ a = a₂

-- auxiliary lemma for proving mem_respects
private theorem mem_swap {a : α} :
      ∀ {p : α × α}, mem_fn a p = mem_fn a (⟨p.2, p.1⟩)
  | (a₁, a₂) =&gt; by
    apply propext
    apply Iff.intro
    . intro
      | Or.inl h =&gt; exact Or.inr h
      | Or.inr h =&gt; exact Or.inl h
    . intro
      | Or.inl h =&gt; exact Or.inr h
      | Or.inr h =&gt; exact Or.inl h


private theorem mem_respects
      : {p₁ p₂ : α × α} → (a : α) → p₁ ~ p₂ → mem_fn a p₁ = mem_fn a p₂
  | (a₁, a₂), (b₁, b₂), a, Or.inl ⟨a₁b₁, a₂b₂⟩ =&gt; by simp_all
  | (a₁, a₂), (b₁, b₂), a, Or.inr ⟨a₁b₂, a₂b₁⟩ =&gt; by simp_all; apply mem_swap

def mem (a : α) (u : UProd α) : Prop :=
  Quot.liftOn u (fun p =&gt; mem_fn a p) (fun p₁ p₂ e =&gt; mem_respects a e)

infix:50 (priority := high) &quot; ∈ &quot; =&gt; mem

theorem mem_mk_left (a b : α) : a ∈ {a, b} :=
  Or.inl rfl

theorem mem_mk_right (a b : α) : b ∈ {a, b} :=
  Or.inr rfl

theorem mem_or_mem_of_mem_mk {a b c : α} : c ∈ {a, b} → c = a ∨ c = b :=
  fun h =&gt; h
<span class="boring">end UProd
</span></code></pre>
<p>For convenience, the standard library also defines <code>Quotient.lift₂</code>
for lifting binary functions, and <code>Quotient.ind₂</code> for induction on
two variables.</p>
<p>We close this section with some hints as to why the quotient
construction implies function extensionality. It is not hard to show
that extensional equality on the <code>(x : α) → β x</code> is an equivalence
relation, and so we can consider the type <code>extfun α β</code> of functions
&quot;up to equivalence.&quot; Of course, application respects that equivalence
in the sense that if <code>f₁</code> is equivalent to <code>f₂</code>, then <code>f₁ a</code> is
equal to <code>f₂ a</code>. Thus application gives rise to a function
<code>extfun_app : extfun α β → (x : α) → β x</code>. But for every <code>f</code>,
<code>extfun_app ⟦f⟧</code> is definitionally equal to <code>fun x =&gt; f x</code>, which is
in turn definitionally equal to <code>f</code>. So, when <code>f₁</code> and <code>f₂</code> are
extensionally equal, we have the following chain of equalities:</p>
<pre><code>    f₁ = extfun_app ⟦f₁⟧ = extfun_app ⟦f₂⟧ = f₂
</code></pre>
<p>As a result, <code>f₁</code> is equal to <code>f₂</code>.</p>
<h2><a class="header" href="#choice" id="choice">Choice</a></h2>
<p>To state the final axiom defined in the standard library, we need the
<code>Nonempty</code> type, which is defined as follows:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>class inductive Nonempty (α : Sort u) : Prop where
  | intro (val : α) : Nonempty α
<span class="boring">end Hidden
</span></code></pre>
<p>Because <code>Nonempty α</code> has type <code>Prop</code> and its constructor contains data, it can only eliminate to <code>Prop</code>.
In fact, <code>Nonempty α</code> is equivalent to <code>∃ x : α, True</code>:</p>
<pre><code class="language-lean">example (α : Type u) : Nonempty α ↔ ∃ x : α, True :=
  Iff.intro (fun ⟨a⟩ =&gt; ⟨a, trivial⟩) (fun ⟨a, h⟩ =&gt; ⟨a⟩)
</code></pre>
<p>Our axiom of choice is now expressed simply as follows:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span><span class="boring">universe u
</span>axiom choice {α : Sort u} : Nonempty α → α
<span class="boring">end Hidden
</span></code></pre>
<p>Given only the assertion <code>h</code> that <code>α</code> is nonempty, <code>choice h</code>
magically produces an element of <code>α</code>. Of course, this blocks any
meaningful computation: by the interpretation of <code>Prop</code>, <code>h</code>
contains no information at all as to how to find such an element.</p>
<p>This is found in the <code>Classical</code> namespace, so the full name of the
theorem is <code>Classical.choice</code>. The choice principle is equivalent to
the principle of <em>indefinite description</em>, which can be expressed with
subtypes as follows:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span><span class="boring">universe u
</span><span class="boring">axiom choice {α : Sort u} : Nonempty α → α
</span>noncomputable def indefiniteDescription {α : Sort u} (p : α → Prop)
                                        (h : ∃ x, p x) : {x // p x} :=
  choice &lt;| let ⟨x, px⟩ := h; ⟨⟨x, px⟩⟩
<span class="boring">end Hidden
</span></code></pre>
<p>Because it depends on <code>choice</code>, Lean cannot generate bytecode for
<code>indefiniteDescription</code>, and so requires us to mark the definition
as <code>noncomputable</code>. Also in the <code>Classical</code> namespace, the
function <code>choose</code> and the property <code>choose_spec</code> decompose the two
parts of the output of <code>indefiniteDescription</code>:</p>
<pre><code class="language-lean"><span class="boring">open Classical
</span><span class="boring">namespace Hidden
</span>noncomputable def choose {α : Sort u} {p : α → Prop} (h : ∃ x, p x) : α :=
  (indefiniteDescription p h).val

theorem choose_spec {α : Sort u} {p : α → Prop} (h : ∃ x, p x) : p (choose h) :=
  (indefiniteDescription p h).property
<span class="boring">end Hidden
</span></code></pre>
<p>The <code>choice</code> principle also erases the distinction between the
property of being <code>Nonempty</code> and the more constructive property of
being <code>Inhabited</code>:</p>
<pre><code class="language-lean"><span class="boring">open Classical
</span>theorem inhabited_of_nonempty : Nonempty α → Inhabited α :=
  fun h =&gt; choice (let ⟨a⟩ := h; ⟨⟨a⟩⟩)
</code></pre>
<p>In the next section, we will see that <code>propext</code>, <code>funext</code>, and
<code>choice</code>, taken together, imply the law of the excluded middle and
the decidability of all propositions. Using those, one can strengthen
the principle of indefinite description as follows:</p>
<pre><code class="language-lean"><span class="boring">open Classical
</span><span class="boring">universe u
</span>#check (@strongIndefiniteDescription :
         {α : Sort u} → (p : α → Prop)
         → Nonempty α → {x // (∃ (y : α), p y) → p x})
</code></pre>
<p>Assuming the ambient type <code>α</code> is nonempty,
<code>strongIndefiniteDescription p</code> produces an element of <code>α</code>
satisfying <code>p</code> if there is one. The data component of this
definition is conventionally known as <em>Hilbert's epsilon function</em>:</p>
<pre><code class="language-lean"><span class="boring">open Classical
</span><span class="boring">universe u
</span>#check (@epsilon :
         {α : Sort u} → [Nonempty α]
         → (α → Prop) → α)

#check (@epsilon_spec :
         ∀ {α : Sort u} {p : α → Prop} (hex : ∃ (y : α), p y),
           p (@epsilon _ (nonempty_of_exists hex) p))
</code></pre>
<h2><a class="header" href="#the-law-of-the-excluded-middle" id="the-law-of-the-excluded-middle">The Law of the Excluded Middle</a></h2>
<p>The law of the excluded middle is the following</p>
<pre><code class="language-lean">open Classical

#check (@em : ∀ (p : Prop), p ∨ ¬p)
</code></pre>
<p><a href="https://en.wikipedia.org/wiki/Diaconescu%27s_theorem">Diaconescu's theorem</a> states
that the axiom of choice is sufficient to derive the law of excluded
middle. More precisely, it shows that the law of the excluded middle
follows from <code>Classical.choice</code>, <code>propext</code>, and <code>funext</code>. We
sketch the proof that is found in the standard library.</p>
<p>First, we import the necessary axioms, and define two predicates <code>U</code> and <code>V</code>:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>open Classical
theorem em (p : Prop) : p ∨ ¬p :=
  let U (x : Prop) : Prop := x = True ∨ p
  let V (x : Prop) : Prop := x = False ∨ p

  have exU : ∃ x, U x := ⟨True, Or.inl rfl⟩
  have exV : ∃ x, V x := ⟨False, Or.inl rfl⟩
<span class="boring">  sorry
</span><span class="boring">end Hidden
</span></code></pre>
<p>If <code>p</code> is true, then every element of <code>Prop</code> is in both <code>U</code> and <code>V</code>.
If <code>p</code> is false, then <code>U</code> is the singleton <code>true</code>, and <code>V</code> is the singleton <code>false</code>.</p>
<p>Next, we use <code>some</code> to choose an element from each of <code>U</code> and <code>V</code>:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span><span class="boring">open Classical
</span><span class="boring">theorem em (p : Prop) : p ∨ ¬p :=
</span><span class="boring">  let U (x : Prop) : Prop := x = True ∨ p
</span><span class="boring">  let V (x : Prop) : Prop := x = False ∨ p
</span><span class="boring">  have exU : ∃ x, U x := ⟨True, Or.inl rfl⟩
</span><span class="boring">  have exV : ∃ x, V x := ⟨False, Or.inl rfl⟩
</span>  let u : Prop := choose exU
  let v : Prop := choose exV

  have u_def : U u := choose_spec exU
  have v_def : V v := choose_spec exV
<span class="boring">  sorry
</span><span class="boring">end Hidden
</span></code></pre>
<p>Each of <code>U</code> and <code>V</code> is a disjunction, so <code>u_def</code> and <code>v_def</code>
represent four cases. In one of these cases, <code>u = True</code> and
<code>v = False</code>, and in all the other cases, <code>p</code> is true. Thus we have:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span><span class="boring">open Classical
</span><span class="boring">theorem em (p : Prop) : p ∨ ¬p :=
</span><span class="boring">  let U (x : Prop) : Prop := x = True ∨ p
</span><span class="boring">  let V (x : Prop) : Prop := x = False ∨ p
</span><span class="boring">  have exU : ∃ x, U x := ⟨True, Or.inl rfl⟩
</span><span class="boring">  have exV : ∃ x, V x := ⟨False, Or.inl rfl⟩
</span><span class="boring">  let u : Prop := choose exU
</span><span class="boring">  let v : Prop := choose exV
</span><span class="boring">  have u_def : U u := choose_spec exU
</span><span class="boring">  have v_def : V v := choose_spec exV
</span>  have not_uv_or_p : u ≠ v ∨ p :=
    match u_def, v_def with
    | Or.inr h, _ =&gt; Or.inr h
    | _, Or.inr h =&gt; Or.inr h
    | Or.inl hut, Or.inl hvf =&gt;
      have hne : u ≠ v := by simp [hvf, hut, true_ne_false]
      Or.inl hne
<span class="boring">  sorry
</span><span class="boring">end Hidden
</span></code></pre>
<p>On the other hand, if <code>p</code> is true, then, by function extensionality
and propositional extensionality, <code>U</code> and <code>V</code> are equal. By the
definition of <code>u</code> and <code>v</code>, this implies that they are equal as well.</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span><span class="boring">open Classical
</span><span class="boring">theorem em (p : Prop) : p ∨ ¬p :=
</span><span class="boring">  let U (x : Prop) : Prop := x = True ∨ p
</span><span class="boring">  let V (x : Prop) : Prop := x = False ∨ p
</span><span class="boring">  have exU : ∃ x, U x := ⟨True, Or.inl rfl⟩
</span><span class="boring">  have exV : ∃ x, V x := ⟨False, Or.inl rfl⟩
</span><span class="boring">  let u : Prop := choose exU
</span><span class="boring">  let v : Prop := choose exV
</span><span class="boring">  have u_def : U u := choose_spec exU
</span><span class="boring">  have v_def : V v := choose_spec exV
</span><span class="boring">  have not_uv_or_p : u ≠ v ∨ p :=
</span><span class="boring">    match u_def, v_def with
</span><span class="boring">    | Or.inr h, _ =&gt; Or.inr h
</span><span class="boring">    | _, Or.inr h =&gt; Or.inr h
</span><span class="boring">    | Or.inl hut, Or.inl hvf =&gt;
</span><span class="boring">      have hne : u ≠ v := by simp [hvf, hut, true_ne_false]
</span><span class="boring">      Or.inl hne
</span>  have p_implies_uv : p → u = v :=
    fun hp =&gt;
    have hpred : U = V :=
      funext fun x =&gt;
        have hl : (x = True ∨ p) → (x = False ∨ p) :=
          fun _ =&gt; Or.inr hp
        have hr : (x = False ∨ p) → (x = True ∨ p) :=
          fun _ =&gt; Or.inr hp
        show (x = True ∨ p) = (x = False ∨ p) from
          propext (Iff.intro hl hr)
    have h₀ : ∀ exU exV, @choose _ U exU = @choose _ V exV := by
      rw [hpred]; intros; rfl
    show u = v from h₀ _ _
<span class="boring">  sorry
</span><span class="boring">end Hidden
</span></code></pre>
<p>Putting these last two facts together yields the desired conclusion:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span><span class="boring">open Classical
</span><span class="boring">theorem em (p : Prop) : p ∨ ¬p :=
</span><span class="boring">  let U (x : Prop) : Prop := x = True ∨ p
</span><span class="boring">  let V (x : Prop) : Prop := x = False ∨ p
</span><span class="boring">  have exU : ∃ x, U x := ⟨True, Or.inl rfl⟩
</span><span class="boring">  have exV : ∃ x, V x := ⟨False, Or.inl rfl⟩
</span><span class="boring">  let u : Prop := choose exU
</span><span class="boring">  let v : Prop := choose exV
</span><span class="boring">  have u_def : U u := choose_spec exU
</span><span class="boring">  have v_def : V v := choose_spec exV
</span><span class="boring">  have not_uv_or_p : u ≠ v ∨ p :=
</span><span class="boring">    match u_def, v_def with
</span><span class="boring">    | Or.inr h, _ =&gt; Or.inr h
</span><span class="boring">    | _, Or.inr h =&gt; Or.inr h
</span><span class="boring">    | Or.inl hut, Or.inl hvf =&gt;
</span><span class="boring">      have hne : u ≠ v := by simp [hvf, hut, true_ne_false]
</span><span class="boring">      Or.inl hne
</span><span class="boring">  have p_implies_uv : p → u = v :=
</span><span class="boring">    fun hp =&gt;
</span><span class="boring">    have hpred : U = V :=
</span><span class="boring">      funext fun x =&gt;
</span><span class="boring">        have hl : (x = True ∨ p) → (x = False ∨ p) :=
</span><span class="boring">          fun _ =&gt; Or.inr hp
</span><span class="boring">        have hr : (x = False ∨ p) → (x = True ∨ p) :=
</span><span class="boring">          fun _ =&gt; Or.inr hp
</span><span class="boring">        show (x = True ∨ p) = (x = False ∨ p) from
</span><span class="boring">          propext (Iff.intro hl hr)
</span><span class="boring">    have h₀ : ∀ exU exV, @choose _ U exU = @choose _ V exV := by
</span><span class="boring">      rw [hpred]; intros; rfl
</span><span class="boring">    show u = v from h₀ _ _
</span>  match not_uv_or_p with
  | Or.inl hne =&gt; Or.inr (mt p_implies_uv hne)
  | Or.inr h   =&gt; Or.inl h
<span class="boring">end Hidden
</span></code></pre>
<p>Consequences of excluded middle include double-negation elimination,
proof by cases, and proof by contradiction, all of which are described
in the <a href="./propositions_and_proofs.html#classical-logic">Section Classical Logic</a>.
The law of the excluded middle and propositional extensionality imply propositional completeness:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>open Classical
theorem propComplete (a : Prop) : a = True ∨ a = False :=
  match em a with
  | Or.inl ha =&gt; Or.inl (propext (Iff.intro (fun _ =&gt; ⟨⟩) (fun _ =&gt; ha)))
  | Or.inr hn =&gt; Or.inr (propext (Iff.intro (fun h =&gt; hn h) (fun h =&gt; False.elim h)))
<span class="boring">end Hidden
</span></code></pre>
<p>Together with choice, we also get the stronger principle that every
proposition is decidable. Recall that the class of <code>Decidable</code>
propositions is defined as follows:</p>
<pre><code class="language-lean"><span class="boring">namespace Hidden
</span>class inductive Decidable (p : Prop) where
  | isFalse (h : ¬p) : Decidable p
  | isTrue  (h : p)  : Decidable p
<span class="boring">end Hidden
</span></code></pre>
<p>In contrast to <code>p ∨ ¬ p</code>, which can only eliminate to <code>Prop</code>, the
type <code>Decidable p</code> is equivalent to the sum type <code>Sum p (¬ p)</code>, which
can eliminate to any type. It is this data that is needed to write an
if-then-else expression.</p>
<p>As an example of classical reasoning, we use <code>choose</code> to show that if
<code>f : α → β</code> is injective and <code>α</code> is inhabited, then <code>f</code> has a
left inverse. To define the left inverse <code>linv</code>, we use a dependent
if-then-else expression. Recall that <code>if h : c then t else e</code> is
notation for <code>dite c (fun h : c =&gt; t) (fun h : ¬ c =&gt; e)</code>. In the definition
of <code>linv</code>, choice is used twice: first, to show that
<code>(∃ a : A, f a = b)</code> is &quot;decidable,&quot; and then to choose an <code>a</code> such that
<code>f a = b</code>. Notice that <code>propDecidable</code> is a scoped instance and is activated
by the <code>open Classical</code> command. We use this instance to justify
the if-then-else expression. (See also the discussion in
<a href="./type_classes.html#decidable-propositions">Section Decidable Propositions</a>).</p>
<pre><code class="language-lean">open Classical

noncomputable def linv [Inhabited α] (f : α → β) : β → α :=
  fun b : β =&gt; if ex : (∃ a : α, f a = b) then choose ex else default

theorem linv_comp_self {f : α → β} [Inhabited α]
                       (inj : ∀ {a b}, f a = f b → a = b)
                       : linv f ∘ f = id :=
  funext fun a =&gt;
    have ex  : ∃ a₁ : α, f a₁ = f a := ⟨a, rfl⟩
    have feq : f (choose ex) = f a  := choose_spec ex
    calc linv f (f a)
      _ = choose ex := dif_pos ex
      _ = a         := inj feq
</code></pre>
<p>From a classical point of view, <code>linv</code> is a function. From a
constructive point of view, it is unacceptable; because there is no
way to implement such a function in general, the construction is not
informative.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
            window.clip_buttons = true;
            window.tryit_buttons = false;
            window.side_bar = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
